{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COMP47590 Assignment 2 LunarEirLander\n",
    "## Evaluate Image Based Classifier\n",
    "Load a pre-trained model for the LunarEirLander game and deploy it into the world"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, math\n",
    "import numpy as np\n",
    "\n",
    "import cv2\n",
    "\n",
    "# MOD Extra imports for image handling\n",
    "from PIL import Image\n",
    "import os\n",
    "import time\n",
    "import datetime\n",
    "from tensorflow import keras\n",
    "\n",
    "import Box2D\n",
    "from Box2D.b2 import (edgeShape, circleShape, fixtureDef, polygonShape, revoluteJointDef, contactListener)\n",
    "\n",
    "import gym\n",
    "from gym import spaces\n",
    "from gym.utils import seeding\n",
    "\n",
    "import LunarEirLander\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the LunarEirLander environment and deploy the agent into it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['+0.60', '-0.24', '+1.26', '+0.24', '-3.41', '-1.42', '+0.00', '+0.00']\n",
      "step 102 total_reward -571.62\n",
      "['+0.76', '-0.15', '+1.40', '-0.36', '-2.51', '-6.60', '+0.00', '+0.00']\n",
      "step 77 total_reward -507.47\n",
      "['-0.17', '+0.01', '+0.49', '+0.15', '-4.14', '+2.47', '+0.00', '+1.00']\n",
      "step 83 total_reward -546.50\n",
      "['+1.01', '+0.07', '+2.67', '-1.69', '-2.11', '-0.78', '+0.00', '+0.00']\n",
      "step 94 total_reward -560.32\n",
      "['-1.00', '+0.02', '-1.94', '+0.05', '+3.22', '+8.20', '+0.00', '+0.00']\n",
      "step 92 total_reward -587.03\n",
      "['+0.47', '+0.09', '+0.22', '-0.06', '-4.39', '-0.71', '+0.00', '+1.00']\n",
      "step 71 total_reward -530.53\n",
      "['-0.59', '+0.03', '-0.89', '-0.66', '+0.37', '-2.80', '+1.00', '+1.00']\n",
      "step 80 total_reward -93.37\n",
      "['+0.96', '-0.02', '+2.07', '-0.44', '-2.96', '-6.53', '+0.00', '+0.00']\n",
      "step 75 total_reward -549.36\n",
      "['-1.01', '+5.02', '-2.72', '+2.16', '+0.76', '+0.14', '+0.00', '+0.00']\n",
      "step 127 total_reward -872.49\n",
      "['+0.09', '+0.01', '-1.04', '-0.11', '+2.11', '-2.35', '+0.00', '+0.00']\n",
      "step 80 total_reward -346.48\n",
      "['+0.67', '-0.20', '+2.12', '-0.31', '-2.39', '-2.34', '+1.00', '+0.00']\n",
      "step 66 total_reward -440.35\n",
      "['-0.40', '-0.04', '-1.98', '-0.62', '+3.12', '+6.55', '+0.00', '+0.00']\n",
      "step 87 total_reward -518.74\n",
      "['+0.21', '+0.01', '-0.55', '+0.10', '-4.16', '+1.71', '+0.00', '+1.00']\n",
      "step 82 total_reward -452.30\n",
      "['+0.65', '+0.12', '+2.00', '-0.95', '-3.60', '-7.86', '+0.00', '+0.00']\n",
      "step 68 total_reward -606.80\n",
      "['+0.14', '-0.01', '+0.72', '-0.22', '-0.51', '+2.84', '+1.00', '+0.00']\n",
      "step 97 total_reward -105.62\n",
      "['-0.11', '+0.00', '+1.24', '-0.01', '-1.89', '+0.13', '+0.00', '+0.00']\n",
      "step 86 total_reward -275.92\n",
      "['+0.75', '+0.02', '+1.42', '+0.09', '-2.22', '-5.20', '+0.00', '+0.00']\n",
      "step 199 total_reward -510.85\n",
      "['+0.54', '-0.12', '+0.75', '-0.58', '+0.06', '-3.47', '+0.00', '+1.00']\n",
      "step 91 total_reward -69.40\n",
      "['+0.83', '-0.09', '+1.08', '-0.45', '-2.35', '-7.59', '+0.00', '+0.00']\n",
      "step 71 total_reward -483.15\n",
      "['+0.16', '+0.00', '+1.01', '+0.01', '-0.86', '-0.69', '+1.00', '+0.00']\n",
      "step 101 total_reward -177.00\n"
     ]
    }
   ],
   "source": [
    "# Load and initialise the control model\n",
    "ROWS = 32\n",
    "COLS = 32\n",
    "CHANNELS = 1\n",
    "\n",
    "# Load a pre-trained model\n",
    "model = keras.models.load_model(\"cnn_balanced.mod\")\n",
    "\n",
    "# Load the Lunar Lander environment and initialise it\n",
    "#env = LunarEirLander.LunarEirLander()\n",
    "\n",
    "\n",
    "# Run the game loop\n",
    "env = gym.make('LunarLander-v2')\n",
    "\n",
    "for i in range(20):\n",
    "    s = env.reset()\n",
    "    total_reward = 0\n",
    "    steps = 0\n",
    "    done = False\n",
    "    while not done:\n",
    "\n",
    "        # Access the rednered scrnen image\n",
    "        raw_image = env.render(mode='rgb_array')\n",
    "\n",
    "        # Prepare the image for presentation to the network - ensure this matches how the model was trained\n",
    "        processed_image = cv2.resize(raw_image, (ROWS, COLS), interpolation=cv2.INTER_CUBIC)\n",
    "        processed_image = cv2.cvtColor(processed_image, cv2.COLOR_RGB2GRAY)\n",
    "        processed_image = np.array(processed_image, dtype=np.float)\n",
    "        processed_image = processed_image.reshape((1, ROWS, COLS, CHANNELS))\n",
    "        processed_image = processed_image/255\n",
    "\n",
    "        # Get the model to make a prediction\n",
    "        a = np.argmax(model.predict(processed_image), axis=-1)\n",
    "        a = a[0]\n",
    "\n",
    "        # Step on the game\n",
    "        s, r, done, info = env.step(a)\n",
    "        env.render()\n",
    "        total_reward += r\n",
    "        steps += 1\n",
    "    env.close()\n",
    "    print([\"{:+0.2f}\".format(x) for x in s])\n",
    "    print(\"step {} total_reward {:+0.2f}\".format(steps, total_reward))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['-0.08', '-0.01', '-1.28', '+0.01', '+3.15', '+0.08', '+0.00', '+0.00']\n",
      "step 77 total_reward -459.81\n",
      "['-0.54', '-0.06', '-1.21', '-0.24', '+4.32', '+3.80', '+0.00', '+0.00']\n",
      "step 71 total_reward -605.27\n",
      "['-0.40', '+0.12', '-1.42', '-0.66', '+2.64', '+7.72', '+0.00', '+0.00']\n",
      "step 75 total_reward -476.34\n",
      "['-0.79', '+0.26', '-2.37', '-0.00', '+2.22', '+0.25', '+0.00', '+1.00']\n",
      "step 89 total_reward -511.52\n",
      "['-0.22', '-0.01', '-1.77', '-1.06', '+3.22', '+8.23', '+0.00', '+0.00']\n",
      "step 77 total_reward -491.83\n",
      "['-0.25', '+0.02', '-1.81', '-0.22', '+2.54', '+5.52', '+0.00', '+0.00']\n",
      "step 79 total_reward -438.93\n",
      "['-0.60', '+0.27', '-1.21', '-0.65', '+2.57', '+7.69', '+0.00', '+0.00']\n",
      "step 66 total_reward -486.11\n",
      "['-0.57', '-0.14', '-2.14', '-1.15', '+3.39', '+6.50', '+0.00', '+0.00']\n",
      "step 75 total_reward -592.06\n",
      "['-0.23', '+0.01', '-1.75', '-0.70', '+2.84', '+7.33', '+0.00', '+0.00']\n",
      "step 84 total_reward -474.72\n",
      "['+0.07', '-0.01', '-0.59', '+0.00', '+3.14', '+0.00', '+0.00', '+0.00']\n",
      "step 74 total_reward -451.38\n",
      "['-0.52', '-0.13', '-1.29', '-0.67', '+3.55', '-5.52', '+0.00', '+0.00']\n",
      "step 69 total_reward -592.02\n",
      "['-0.48', '-0.02', '-1.14', '-0.61', '+2.62', '+7.90', '+0.00', '+0.00']\n",
      "step 86 total_reward -487.06\n",
      "['-0.31', '-0.06', '-1.57', '-1.05', '+3.61', '-4.91', '+0.00', '+0.00']\n",
      "step 77 total_reward -567.80\n",
      "['-0.58', '-0.20', '+0.85', '-0.33', '+4.66', '-4.49', '+0.00', '+0.00']\n",
      "step 75 total_reward -655.08\n",
      "['-0.20', '+0.02', '-1.71', '-0.11', '+2.55', '+5.05', '+0.00', '+0.00']\n",
      "step 81 total_reward -406.89\n",
      "['-0.53', '-0.21', '-1.72', '-0.56', '+4.33', '+1.38', '+0.00', '+0.00']\n",
      "step 68 total_reward -611.02\n",
      "['-0.25', '+0.00', '-2.09', '-0.33', '+2.70', '+3.39', '+0.00', '+0.00']\n",
      "step 83 total_reward -455.10\n",
      "['-0.64', '-0.12', '-2.17', '-0.19', '+3.34', '+0.35', '+0.00', '+0.00']\n",
      "step 76 total_reward -607.88\n",
      "['-0.60', '+0.14', '-0.95', '-0.71', '+2.63', '+8.04', '+0.00', '+0.00']\n",
      "step 58 total_reward -444.00\n",
      "['-0.36', '-0.07', '-2.29', '-0.78', '+2.95', '+5.14', '+0.00', '+0.00']\n",
      "step 74 total_reward -510.65\n"
     ]
    }
   ],
   "source": [
    "# Load and initialise the control model\n",
    "ROWS = 32\n",
    "COLS = 32\n",
    "CHANNELS = 1\n",
    "\n",
    "# Load a pre-trained model\n",
    "model = keras.models.load_model(\"cnn_unbalanced.mod\")\n",
    "\n",
    "# Load the Lunar Lander environment and initialise it\n",
    "#env = LunarEirLander.LunarEirLander()\n",
    "\n",
    "\n",
    "# Run the game loop\n",
    "env = gym.make('LunarLander-v2')\n",
    "\n",
    "for i in range(20):\n",
    "    s = env.reset()\n",
    "    total_reward = 0\n",
    "    steps = 0\n",
    "    done = False\n",
    "    while not done:\n",
    "\n",
    "        # Access the rednered scrnen image\n",
    "        raw_image = env.render(mode='rgb_array')\n",
    "\n",
    "        # Prepare the image for presentation to the network - ensure this matches how the model was trained\n",
    "        processed_image = cv2.resize(raw_image, (ROWS, COLS), interpolation=cv2.INTER_CUBIC)\n",
    "        processed_image = cv2.cvtColor(processed_image, cv2.COLOR_RGB2GRAY)\n",
    "        processed_image = np.array(processed_image, dtype=np.float)\n",
    "        processed_image = processed_image.reshape((1, ROWS, COLS, CHANNELS))\n",
    "        processed_image = processed_image/255\n",
    "\n",
    "        # Get the model to make a prediction\n",
    "        a = np.argmax(model.predict(processed_image), axis=-1)\n",
    "        a = a[0]\n",
    "\n",
    "        # Step on the game\n",
    "        s, r, done, info = env.step(a)\n",
    "        env.render()\n",
    "        total_reward += r\n",
    "        steps += 1\n",
    "    env.close()\n",
    "    print([\"{:+0.2f}\".format(x) for x in s])\n",
    "    print(\"step {} total_reward {:+0.2f}\".format(steps, total_reward))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "env.reset()\n",
    "for _ in range(1000):\n",
    "    env.render()\n",
    "    env.step(env.action_space.sample()) # take a random action\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
