{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "\n",
    "import gym\n",
    "import pandas as pd\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, Flatten\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# We use keras-rl2 a reinforcement learning package that builds on top of keras and openAI gym (pip install keras-rl2) \n",
    "from rl.agents.dqn import DQNAgent\n",
    "from rl.policy import BoltzmannQPolicy, EpsGreedyQPolicy\n",
    "from rl.memory import SequentialMemory\n",
    "import LunarEirLander\n",
    "from rl.callbacks import FileLogger, ModelIntervalCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the environment and extract the number of actions.\n",
    "# env = gym.make(ENV_NAME)\n",
    "# np.random.seed(0)\n",
    "# env.seed(0)\n",
    "env = LunarEirLander.LunarEirLander()\n",
    "nb_actions = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten (Flatten)            (None, 8)                 0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 128)               1152      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 4)                 132       \n",
      "=================================================================\n",
      "Total params: 11,620\n",
      "Trainable params: 11,620\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Next, we build a very simple model.\n",
    "model = Sequential()\n",
    "model.add(Flatten(input_shape=(1,) + env.observation_space.shape)) # Neat approach to making model inputs match environment state\n",
    "model.add(Dense(128, activation = 'relu'))\n",
    "model.add(Dense(64, activation = 'relu'))\n",
    "model.add(Dense(32, activation = 'relu'))\n",
    "model.add(Dense(nb_actions, activation = 'linear'))\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = SequentialMemory(limit=50000, window_length=1)\n",
    "policy = EpsGreedyQPolicy()\n",
    "dqn = DQNAgent(model=model, nb_actions=nb_actions, memory=memory, nb_steps_warmup=30,\n",
    "               target_model_update=1e-2, policy=policy)\n",
    "dqn.compile(Adam(lr=1e-3), metrics=['mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 100000 steps ...\n",
      "Interval 1 (0 steps performed)\n",
      "500/500 [==============================] - 39s 78ms/step - reward: -0.6664\n",
      "2 episodes - episode_reward: -172.670 [-242.043, -103.297] - loss: 12.373 - mae: 13.684 - mean_q: -14.650\n",
      "\n",
      "Interval 2 (500 steps performed)\n",
      "500/500 [==============================] - 39s 78ms/step - reward: -0.1594\n",
      "Interval 3 (1000 steps performed)\n",
      "500/500 [==============================] - 41s 81ms/step - reward: -0.1494\n",
      "Interval 4 (1500 steps performed)\n",
      "500/500 [==============================] - 51s 102ms/step - reward: -0.2232\n",
      "Interval 5 (2000 steps performed)\n",
      "493/500 [============================>.] - ETA: 0s - reward: -0.1765done, took 227.714 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f86693c0610>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights_filename = f'dqn_lunar_weights.h5f'\n",
    "checkpoint_weights_filename = 'dqn_lunar_weights_{step}.h5f'\n",
    "log_filename = f'dqn_lunar_log.json'\n",
    "callbacks = [ModelIntervalCheckpoint(checkpoint_weights_filename, interval=250000)]\n",
    "callbacks += [FileLogger(log_filename, interval=100)]\n",
    "dqn.fit(env, callbacks=callbacks, nb_steps=100000, log_interval=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn.save_weights(weights_filename, overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing for 5 episodes ...\n",
      "Episode 1: reward: -324.210, steps: 110\n",
      "Episode 2: reward: -311.213, steps: 91\n",
      "Episode 3: reward: -126.502, steps: 63\n",
      "Episode 4: reward: -73.705, steps: 74\n",
      "Episode 5: reward: -215.888, steps: 143\n"
     ]
    }
   ],
   "source": [
    "dqn.test(env, nb_episodes=5, visualize=True)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 500000 steps ...\n",
      "     82/500000: episode: 1, duration: 0.197s, episode steps:  82, steps per second: 417, episode reward: -326.599, mean reward: -3.983 [-100.000,  1.730], mean action: 1.134 [0.000, 3.000],  loss: 7.963121, mae: 16.785300, mean_q: -19.568584\n",
      "    200/500000: episode: 2, duration: 0.406s, episode steps: 118, steps per second: 290, episode reward: -132.501, mean reward: -1.123 [-100.000,  2.283], mean action: 1.229 [0.000, 3.000],  loss: 11.054199, mae: 18.427864, mean_q: -21.652473\n",
      "    290/500000: episode: 3, duration: 0.287s, episode steps:  90, steps per second: 314, episode reward: -172.047, mean reward: -1.912 [-100.000,  5.475], mean action: 0.133 [0.000, 3.000],  loss: 9.099948, mae: 18.190281, mean_q: -21.644831\n",
      "    536/500000: episode: 4, duration: 0.821s, episode steps: 246, steps per second: 300, episode reward: -240.460, mean reward: -0.977 [-100.000, 23.062], mean action: 1.077 [0.000, 3.000],  loss: 11.023267, mae: 19.550644, mean_q: -23.711624\n",
      "    627/500000: episode: 5, duration: 0.300s, episode steps:  91, steps per second: 303, episode reward: -157.910, mean reward: -1.735 [-100.000,  8.786], mean action: 0.582 [0.000, 3.000],  loss: 6.127444, mae: 19.951771, mean_q: -24.555834\n",
      "    756/500000: episode: 6, duration: 0.425s, episode steps: 129, steps per second: 304, episode reward: -277.012, mean reward: -2.147 [-100.000, 11.934], mean action: 1.031 [0.000, 3.000],  loss: 6.914041, mae: 21.524691, mean_q: -26.661034\n",
      "    843/500000: episode: 7, duration: 0.280s, episode steps:  87, steps per second: 311, episode reward: -186.813, mean reward: -2.147 [-100.000, 12.434], mean action: 0.747 [0.000, 3.000],  loss: 9.232835, mae: 22.042553, mean_q: -26.993624\n",
      "    933/500000: episode: 8, duration: 0.289s, episode steps:  90, steps per second: 311, episode reward: -249.038, mean reward: -2.767 [-100.000, 77.450], mean action: 0.167 [0.000, 2.000],  loss: 8.086126, mae: 23.767313, mean_q: -29.408403\n",
      "   1119/500000: episode: 9, duration: 0.607s, episode steps: 186, steps per second: 306, episode reward: -348.154, mean reward: -1.872 [-100.000,  7.325], mean action: 1.102 [0.000, 3.000],  loss: 11.863024, mae: 24.536606, mean_q: -30.403034\n",
      "   1229/500000: episode: 10, duration: 0.364s, episode steps: 110, steps per second: 302, episode reward: -176.087, mean reward: -1.601 [-100.000,  5.549], mean action: 0.736 [0.000, 3.000],  loss: 7.755081, mae: 25.054293, mean_q: -30.976110\n",
      "   1478/500000: episode: 11, duration: 0.848s, episode steps: 249, steps per second: 294, episode reward: -21.874, mean reward: -0.088 [-100.000, 110.882], mean action: 1.241 [0.000, 3.000],  loss: 10.263538, mae: 25.976185, mean_q: -31.892923\n",
      "   2008/500000: episode: 12, duration: 2.004s, episode steps: 530, steps per second: 264, episode reward: -393.249, mean reward: -0.742 [-100.000, 41.113], mean action: 1.045 [0.000, 3.000],  loss: 13.448148, mae: 26.248859, mean_q: -32.362568\n",
      "   2121/500000: episode: 13, duration: 0.376s, episode steps: 113, steps per second: 300, episode reward: -287.321, mean reward: -2.543 [-100.000,  4.773], mean action: 0.823 [0.000, 3.000],  loss: 16.097712, mae: 26.382696, mean_q: -32.494587\n",
      "   2225/500000: episode: 14, duration: 0.343s, episode steps: 104, steps per second: 303, episode reward: -154.449, mean reward: -1.485 [-100.000, 10.728], mean action: 1.192 [0.000, 3.000],  loss: 13.139512, mae: 27.343060, mean_q: -33.796597\n",
      "   2375/500000: episode: 15, duration: 0.515s, episode steps: 150, steps per second: 291, episode reward: -307.664, mean reward: -2.051 [-100.000,  3.841], mean action: 1.400 [0.000, 3.000],  loss: 17.447950, mae: 27.598665, mean_q: -33.863304\n",
      "   2443/500000: episode: 16, duration: 0.231s, episode steps:  68, steps per second: 294, episode reward: -184.570, mean reward: -2.714 [-100.000,  6.315], mean action: 0.118 [0.000, 3.000],  loss: 21.004692, mae: 28.846622, mean_q: -35.409996\n",
      "   2503/500000: episode: 17, duration: 0.199s, episode steps:  60, steps per second: 301, episode reward: -162.549, mean reward: -2.709 [-100.000,  6.924], mean action: 0.383 [0.000, 3.000],  loss: 15.194431, mae: 28.095638, mean_q: -33.993958\n",
      "   2687/500000: episode: 18, duration: 0.613s, episode steps: 184, steps per second: 300, episode reward: -162.955, mean reward: -0.886 [-100.000,  5.166], mean action: 1.560 [0.000, 3.000],  loss: 16.161428, mae: 27.406696, mean_q: -33.658859\n",
      "   2764/500000: episode: 19, duration: 0.252s, episode steps:  77, steps per second: 306, episode reward: -173.601, mean reward: -2.255 [-100.000,  4.657], mean action: 1.364 [0.000, 3.000],  loss: 12.094186, mae: 28.794626, mean_q: -35.668453\n",
      "   2917/500000: episode: 20, duration: 0.540s, episode steps: 153, steps per second: 283, episode reward: -104.880, mean reward: -0.685 [-100.000, 82.172], mean action: 1.523 [0.000, 3.000],  loss: 12.370342, mae: 27.328442, mean_q: -33.896694\n",
      "   3098/500000: episode: 21, duration: 0.606s, episode steps: 181, steps per second: 299, episode reward: -263.132, mean reward: -1.454 [-100.000,  5.747], mean action: 1.481 [0.000, 3.000],  loss: 17.549286, mae: 28.595045, mean_q: -35.609676\n",
      "   3166/500000: episode: 22, duration: 0.222s, episode steps:  68, steps per second: 307, episode reward: -106.792, mean reward: -1.570 [-100.000, 33.635], mean action: 0.750 [0.000, 3.000],  loss: 14.351724, mae: 28.602774, mean_q: -35.626316\n",
      "   3314/500000: episode: 23, duration: 0.492s, episode steps: 148, steps per second: 301, episode reward: -218.627, mean reward: -1.477 [-100.000,  6.973], mean action: 1.723 [0.000, 3.000],  loss: 11.938474, mae: 28.730650, mean_q: -35.686142\n",
      "   3466/500000: episode: 24, duration: 0.499s, episode steps: 152, steps per second: 304, episode reward: -206.682, mean reward: -1.360 [-100.000,  6.189], mean action: 1.151 [0.000, 3.000],  loss: 13.628024, mae: 29.815538, mean_q: -37.186020\n",
      "   3609/500000: episode: 25, duration: 0.492s, episode steps: 143, steps per second: 290, episode reward: -200.944, mean reward: -1.405 [-100.000,  4.390], mean action: 1.315 [0.000, 3.000],  loss: 6.996233, mae: 29.075096, mean_q: -36.397877\n",
      "   3746/500000: episode: 26, duration: 0.476s, episode steps: 137, steps per second: 288, episode reward: -205.420, mean reward: -1.499 [-100.000,  7.269], mean action: 0.971 [0.000, 3.000],  loss: 12.366467, mae: 29.746479, mean_q: -37.070938\n",
      "   3897/500000: episode: 27, duration: 0.501s, episode steps: 151, steps per second: 301, episode reward: -210.809, mean reward: -1.396 [-100.000, 112.411], mean action: 1.662 [0.000, 3.000],  loss: 9.942432, mae: 29.663952, mean_q: -37.030937\n",
      "   4143/500000: episode: 28, duration: 0.855s, episode steps: 246, steps per second: 288, episode reward: -296.292, mean reward: -1.204 [-100.000,  9.128], mean action: 1.081 [0.000, 3.000],  loss: 15.175301, mae: 30.146811, mean_q: -37.529861\n",
      "   4364/500000: episode: 29, duration: 0.752s, episode steps: 221, steps per second: 294, episode reward: -423.840, mean reward: -1.918 [-100.000, 131.412], mean action: 1.023 [0.000, 3.000],  loss: 10.496904, mae: 30.345570, mean_q: -37.950584\n",
      "   4616/500000: episode: 30, duration: 0.867s, episode steps: 252, steps per second: 291, episode reward: -182.130, mean reward: -0.723 [-100.000,  6.272], mean action: 1.865 [0.000, 3.000],  loss: 16.189924, mae: 29.527409, mean_q: -36.793201\n",
      "   4772/500000: episode: 31, duration: 0.520s, episode steps: 156, steps per second: 300, episode reward: -235.515, mean reward: -1.510 [-100.000, 25.252], mean action: 1.000 [0.000, 3.000],  loss: 17.296076, mae: 29.358292, mean_q: -36.657345\n",
      "   5105/500000: episode: 32, duration: 1.172s, episode steps: 333, steps per second: 284, episode reward: -265.928, mean reward: -0.799 [-100.000,  7.581], mean action: 1.511 [0.000, 3.000],  loss: 16.686983, mae: 29.212011, mean_q: -36.413765\n",
      "   5203/500000: episode: 33, duration: 0.319s, episode steps:  98, steps per second: 307, episode reward: -282.242, mean reward: -2.880 [-100.000,  4.618], mean action: 0.959 [0.000, 3.000],  loss: 9.204337, mae: 29.081861, mean_q: -36.274704\n",
      "   5435/500000: episode: 34, duration: 0.792s, episode steps: 232, steps per second: 293, episode reward: -58.388, mean reward: -0.252 [-100.000, 12.861], mean action: 1.733 [0.000, 3.000],  loss: 12.654522, mae: 29.083906, mean_q: -36.379482\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   5539/500000: episode: 35, duration: 0.342s, episode steps: 104, steps per second: 304, episode reward: -182.582, mean reward: -1.756 [-100.000, 10.706], mean action: 0.933 [0.000, 3.000],  loss: 15.200828, mae: 29.991343, mean_q: -37.631325\n",
      "   5975/500000: episode: 36, duration: 1.559s, episode steps: 436, steps per second: 280, episode reward: 120.193, mean reward:  0.276 [-17.031, 100.000], mean action: 1.727 [0.000, 3.000],  loss: 15.517873, mae: 29.742462, mean_q: -37.085854\n",
      "   6454/500000: episode: 37, duration: 1.762s, episode steps: 479, steps per second: 272, episode reward: -87.701, mean reward: -0.183 [-100.000, 19.546], mean action: 1.530 [0.000, 3.000],  loss: 11.028628, mae: 28.885895, mean_q: -36.039391\n",
      "   7325/500000: episode: 38, duration: 3.381s, episode steps: 871, steps per second: 258, episode reward: -291.656, mean reward: -0.335 [-100.000, 14.591], mean action: 1.743 [0.000, 3.000],  loss: 16.469769, mae: 26.853895, mean_q: -32.953953\n",
      "   8183/500000: episode: 39, duration: 3.412s, episode steps: 858, steps per second: 251, episode reward: -266.891, mean reward: -0.311 [-100.000,  3.895], mean action: 1.900 [0.000, 3.000],  loss: 13.418805, mae: 22.788940, mean_q: -27.635660\n",
      "   9183/500000: episode: 40, duration: 4.148s, episode steps: 1000, steps per second: 241, episode reward: -137.610, mean reward: -0.138 [-4.747,  3.705], mean action: 1.878 [0.000, 3.000],  loss: 14.261219, mae: 18.151493, mean_q: -21.539013\n",
      "   9860/500000: episode: 41, duration: 2.609s, episode steps: 677, steps per second: 259, episode reward: -200.407, mean reward: -0.296 [-100.000,  6.244], mean action: 1.809 [0.000, 3.000],  loss: 12.639040, mae: 14.440413, mean_q: -16.557823\n",
      "  10495/500000: episode: 42, duration: 2.603s, episode steps: 635, steps per second: 244, episode reward: -222.204, mean reward: -0.350 [-100.000,  6.079], mean action: 1.907 [0.000, 3.000],  loss: 10.984765, mae: 12.504463, mean_q: -13.788247\n",
      "  10584/500000: episode: 43, duration: 0.303s, episode steps:  89, steps per second: 294, episode reward: -99.809, mean reward: -1.121 [-100.000, 14.714], mean action: 1.787 [0.000, 3.000],  loss: 11.830799, mae: 12.158874, mean_q: -13.281641\n",
      "  10807/500000: episode: 44, duration: 0.762s, episode steps: 223, steps per second: 293, episode reward: -272.506, mean reward: -1.222 [-100.000, 13.360], mean action: 1.861 [0.000, 3.000],  loss: 15.326461, mae: 11.994149, mean_q: -12.875682\n",
      "  11085/500000: episode: 45, duration: 1.054s, episode steps: 278, steps per second: 264, episode reward: -201.181, mean reward: -0.724 [-100.000,  3.132], mean action: 1.838 [0.000, 3.000],  loss: 15.420600, mae: 11.548886, mean_q: -12.253900\n",
      "  11194/500000: episode: 46, duration: 0.366s, episode steps: 109, steps per second: 298, episode reward: -113.290, mean reward: -1.039 [-100.000,  6.669], mean action: 1.706 [0.000, 3.000],  loss: 6.908068, mae: 11.052670, mean_q: -11.661876\n",
      "  11445/500000: episode: 47, duration: 0.862s, episode steps: 251, steps per second: 291, episode reward: -177.431, mean reward: -0.707 [-100.000,  3.266], mean action: 1.677 [0.000, 3.000],  loss: 9.408573, mae: 10.904980, mean_q: -11.466812\n",
      "  11975/500000: episode: 48, duration: 1.929s, episode steps: 530, steps per second: 275, episode reward: -79.628, mean reward: -0.150 [-100.000, 14.075], mean action: 1.738 [0.000, 3.000],  loss: 15.239145, mae: 11.064342, mean_q: -11.378607\n",
      "  12316/500000: episode: 49, duration: 1.215s, episode steps: 341, steps per second: 281, episode reward: -573.351, mean reward: -1.681 [-100.000,  2.648], mean action: 1.528 [0.000, 3.000],  loss: 11.018774, mae: 10.695832, mean_q: -10.590193\n",
      "  12417/500000: episode: 50, duration: 0.342s, episode steps: 101, steps per second: 295, episode reward: -62.239, mean reward: -0.616 [-100.000, 10.538], mean action: 1.802 [0.000, 3.000],  loss: 15.853340, mae: 10.010038, mean_q: -9.692876\n",
      "  12727/500000: episode: 51, duration: 1.092s, episode steps: 310, steps per second: 284, episode reward: -200.476, mean reward: -0.647 [-100.000, 12.202], mean action: 1.723 [0.000, 3.000],  loss: 11.114570, mae: 10.620956, mean_q: -10.359394\n",
      "  12819/500000: episode: 52, duration: 0.330s, episode steps:  92, steps per second: 278, episode reward: -71.871, mean reward: -0.781 [-100.000,  9.538], mean action: 1.663 [0.000, 3.000],  loss: 16.752890, mae: 11.059283, mean_q: -10.905911\n",
      "  12955/500000: episode: 53, duration: 0.467s, episode steps: 136, steps per second: 291, episode reward: -63.326, mean reward: -0.466 [-100.000, 27.280], mean action: 1.463 [0.000, 3.000],  loss: 10.600150, mae: 10.628671, mean_q: -10.320211\n",
      "  13283/500000: episode: 54, duration: 1.186s, episode steps: 328, steps per second: 277, episode reward: -275.678, mean reward: -0.840 [-100.000, 28.864], mean action: 1.671 [0.000, 3.000],  loss: 8.719075, mae: 10.740946, mean_q: -10.438327\n",
      "  13609/500000: episode: 55, duration: 1.185s, episode steps: 326, steps per second: 275, episode reward: -311.972, mean reward: -0.957 [-100.000, 14.265], mean action: 1.503 [0.000, 3.000],  loss: 13.341894, mae: 11.063298, mean_q: -10.722292\n",
      "  13693/500000: episode: 56, duration: 0.285s, episode steps:  84, steps per second: 295, episode reward: -150.917, mean reward: -1.797 [-100.000, 20.100], mean action: 1.679 [0.000, 3.000],  loss: 11.018280, mae: 11.240973, mean_q: -10.832601\n",
      "  13797/500000: episode: 57, duration: 0.348s, episode steps: 104, steps per second: 299, episode reward: -130.539, mean reward: -1.255 [-100.000,  9.072], mean action: 1.192 [0.000, 3.000],  loss: 17.563940, mae: 11.095612, mean_q: -10.447218\n",
      "  13875/500000: episode: 58, duration: 0.258s, episode steps:  78, steps per second: 303, episode reward: -133.704, mean reward: -1.714 [-100.000,  9.563], mean action: 1.038 [0.000, 3.000],  loss: 12.038926, mae: 11.288293, mean_q: -10.620207\n",
      "  13967/500000: episode: 59, duration: 0.310s, episode steps:  92, steps per second: 297, episode reward: -96.345, mean reward: -1.047 [-100.000, 14.748], mean action: 0.978 [0.000, 3.000],  loss: 10.390063, mae: 11.207517, mean_q: -10.871643\n",
      "  14092/500000: episode: 60, duration: 0.421s, episode steps: 125, steps per second: 297, episode reward: -109.364, mean reward: -0.875 [-100.000,  7.775], mean action: 1.648 [0.000, 3.000],  loss: 12.325104, mae: 11.677342, mean_q: -11.352605\n",
      "  14190/500000: episode: 61, duration: 0.332s, episode steps:  98, steps per second: 295, episode reward: -353.559, mean reward: -3.608 [-100.000,  6.751], mean action: 1.888 [0.000, 3.000],  loss: 9.483096, mae: 12.007771, mean_q: -11.814377\n",
      "  14271/500000: episode: 62, duration: 0.275s, episode steps:  81, steps per second: 294, episode reward: -134.480, mean reward: -1.660 [-100.000, 14.589], mean action: 1.753 [0.000, 3.000],  loss: 20.656813, mae: 11.896745, mean_q: -11.497335\n",
      "  14605/500000: episode: 63, duration: 1.180s, episode steps: 334, steps per second: 283, episode reward: -105.046, mean reward: -0.315 [-100.000, 15.852], mean action: 1.428 [0.000, 3.000],  loss: 14.397077, mae: 11.703495, mean_q: -11.183981\n",
      "  14699/500000: episode: 64, duration: 0.320s, episode steps:  94, steps per second: 294, episode reward: -114.766, mean reward: -1.221 [-100.000, 14.849], mean action: 1.011 [0.000, 3.000],  loss: 16.495865, mae: 12.008723, mean_q: -11.558271\n",
      "  15699/500000: episode: 65, duration: 4.098s, episode steps: 1000, steps per second: 244, episode reward: -193.754, mean reward: -0.194 [-6.625,  6.517], mean action: 1.655 [0.000, 3.000],  loss: 10.894320, mae: 11.331189, mean_q: -10.562107\n",
      "  16060/500000: episode: 66, duration: 1.252s, episode steps: 361, steps per second: 288, episode reward: -241.967, mean reward: -0.670 [-100.000, 10.292], mean action: 1.114 [0.000, 3.000],  loss: 9.852015, mae: 11.642550, mean_q: -10.923104\n",
      "  16197/500000: episode: 67, duration: 0.459s, episode steps: 137, steps per second: 299, episode reward: -429.140, mean reward: -3.132 [-100.000,  1.488], mean action: 1.606 [0.000, 3.000],  loss: 10.404615, mae: 11.989462, mean_q: -11.147625\n",
      "  16348/500000: episode: 68, duration: 0.551s, episode steps: 151, steps per second: 274, episode reward: -81.754, mean reward: -0.541 [-100.000, 13.500], mean action: 1.278 [0.000, 3.000],  loss: 8.223165, mae: 11.949726, mean_q: -11.150064\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  16560/500000: episode: 69, duration: 0.740s, episode steps: 212, steps per second: 286, episode reward: -430.727, mean reward: -2.032 [-100.000,  2.492], mean action: 1.396 [0.000, 3.000],  loss: 10.326934, mae: 11.911045, mean_q: -11.017550\n",
      "  16624/500000: episode: 70, duration: 0.247s, episode steps:  64, steps per second: 259, episode reward: -139.595, mean reward: -2.181 [-100.000,  6.011], mean action: 1.031 [0.000, 3.000],  loss: 12.721609, mae: 10.980802, mean_q: -9.661226\n",
      "  16794/500000: episode: 71, duration: 0.585s, episode steps: 170, steps per second: 290, episode reward: -404.244, mean reward: -2.378 [-100.000,  1.817], mean action: 1.724 [0.000, 3.000],  loss: 8.910123, mae: 10.784506, mean_q: -9.627635\n",
      "  16975/500000: episode: 72, duration: 0.616s, episode steps: 181, steps per second: 294, episode reward: -68.021, mean reward: -0.376 [-100.000,  9.383], mean action: 1.536 [0.000, 3.000],  loss: 12.049449, mae: 11.626975, mean_q: -10.690846\n",
      "  17215/500000: episode: 73, duration: 0.850s, episode steps: 240, steps per second: 282, episode reward: -410.441, mean reward: -1.710 [-100.000,  4.209], mean action: 1.617 [0.000, 3.000],  loss: 15.069948, mae: 12.209094, mean_q: -11.482507\n",
      "  17401/500000: episode: 74, duration: 0.676s, episode steps: 186, steps per second: 275, episode reward: -398.568, mean reward: -2.143 [-100.000,  2.252], mean action: 1.538 [0.000, 3.000],  loss: 8.209204, mae: 11.481483, mean_q: -10.328968\n",
      "  17530/500000: episode: 75, duration: 0.451s, episode steps: 129, steps per second: 286, episode reward: -247.899, mean reward: -1.922 [-100.000,  1.209], mean action: 1.589 [0.000, 3.000],  loss: 19.812656, mae: 12.029863, mean_q: -11.303168\n",
      "  17696/500000: episode: 76, duration: 0.599s, episode steps: 166, steps per second: 277, episode reward: -326.500, mean reward: -1.967 [-100.000,  3.814], mean action: 1.452 [0.000, 3.000],  loss: 19.534908, mae: 11.021277, mean_q: -10.000118\n",
      "  17833/500000: episode: 77, duration: 0.558s, episode steps: 137, steps per second: 246, episode reward: -254.451, mean reward: -1.857 [-100.000,  2.888], mean action: 1.620 [0.000, 3.000],  loss: 14.209516, mae: 11.413012, mean_q: -10.743612\n",
      "  17975/500000: episode: 78, duration: 0.515s, episode steps: 142, steps per second: 276, episode reward: -361.398, mean reward: -2.545 [-100.000,  2.172], mean action: 1.528 [0.000, 3.000],  loss: 14.052034, mae: 10.909275, mean_q: -9.881658\n",
      "  18063/500000: episode: 79, duration: 0.345s, episode steps:  88, steps per second: 255, episode reward: -100.999, mean reward: -1.148 [-100.000, 21.284], mean action: 1.239 [0.000, 3.000],  loss: 18.296232, mae: 11.080357, mean_q: -10.251330\n",
      "  18188/500000: episode: 80, duration: 0.472s, episode steps: 125, steps per second: 265, episode reward: -238.275, mean reward: -1.906 [-100.000,  2.221], mean action: 1.432 [0.000, 3.000],  loss: 11.960557, mae: 10.717502, mean_q: -9.682132\n",
      "  18586/500000: episode: 81, duration: 1.439s, episode steps: 398, steps per second: 277, episode reward: -119.506, mean reward: -0.300 [-100.000, 11.543], mean action: 1.176 [0.000, 3.000],  loss: 13.650034, mae: 11.423789, mean_q: -10.607937\n",
      "  18757/500000: episode: 82, duration: 0.589s, episode steps: 171, steps per second: 290, episode reward: -181.160, mean reward: -1.059 [-100.000,  4.265], mean action: 1.515 [0.000, 3.000],  loss: 16.690454, mae: 12.046965, mean_q: -11.262574\n",
      "  18869/500000: episode: 83, duration: 0.382s, episode steps: 112, steps per second: 293, episode reward: -177.184, mean reward: -1.582 [-100.000,  3.648], mean action: 1.027 [0.000, 3.000],  loss: 12.740361, mae: 11.450592, mean_q: -10.463986\n",
      "  18992/500000: episode: 84, duration: 0.446s, episode steps: 123, steps per second: 276, episode reward: -100.833, mean reward: -0.820 [-100.000,  9.455], mean action: 1.236 [0.000, 3.000],  loss: 20.749794, mae: 13.078385, mean_q: -12.567488\n",
      "  19156/500000: episode: 85, duration: 0.573s, episode steps: 164, steps per second: 286, episode reward: -70.412, mean reward: -0.429 [-100.000, 11.620], mean action: 1.604 [0.000, 3.000],  loss: 13.846930, mae: 12.760989, mean_q: -12.194570\n",
      "  19593/500000: episode: 86, duration: 1.620s, episode steps: 437, steps per second: 270, episode reward: -145.032, mean reward: -0.332 [-100.000, 21.745], mean action: 1.492 [0.000, 3.000],  loss: 13.718351, mae: 12.607518, mean_q: -12.028271\n",
      "  19783/500000: episode: 87, duration: 0.647s, episode steps: 190, steps per second: 293, episode reward: -210.247, mean reward: -1.107 [-100.000,  4.081], mean action: 1.379 [0.000, 3.000],  loss: 10.199955, mae: 13.463803, mean_q: -13.217298\n",
      "  20034/500000: episode: 88, duration: 0.908s, episode steps: 251, steps per second: 276, episode reward:  1.448, mean reward:  0.006 [-100.000, 17.019], mean action: 1.582 [0.000, 3.000],  loss: 13.318892, mae: 12.899037, mean_q: -12.327771\n",
      "  20144/500000: episode: 89, duration: 0.374s, episode steps: 110, steps per second: 294, episode reward: -185.997, mean reward: -1.691 [-100.000,  6.566], mean action: 1.555 [0.000, 3.000],  loss: 19.227200, mae: 12.910727, mean_q: -12.157585\n",
      "  20260/500000: episode: 90, duration: 0.399s, episode steps: 116, steps per second: 291, episode reward: -198.590, mean reward: -1.712 [-100.000,  1.282], mean action: 1.810 [0.000, 3.000],  loss: 13.473491, mae: 13.178355, mean_q: -12.272625\n",
      "  20340/500000: episode: 91, duration: 0.276s, episode steps:  80, steps per second: 290, episode reward: -349.721, mean reward: -4.372 [-100.000,  0.444], mean action: 1.900 [0.000, 3.000],  loss: 5.993606, mae: 13.078624, mean_q: -12.454000\n",
      "  20447/500000: episode: 92, duration: 0.372s, episode steps: 107, steps per second: 288, episode reward: -162.431, mean reward: -1.518 [-100.000, 31.184], mean action: 1.411 [0.000, 3.000],  loss: 14.566032, mae: 12.714700, mean_q: -11.915712\n",
      "  20815/500000: episode: 93, duration: 1.330s, episode steps: 368, steps per second: 277, episode reward: -151.739, mean reward: -0.412 [-100.000,  7.321], mean action: 1.410 [0.000, 3.000],  loss: 10.010877, mae: 12.996938, mean_q: -12.272095\n",
      "  20893/500000: episode: 94, duration: 0.264s, episode steps:  78, steps per second: 295, episode reward: -186.772, mean reward: -2.395 [-100.000,  7.667], mean action: 0.679 [0.000, 3.000],  loss: 20.764727, mae: 13.203462, mean_q: -12.330254\n",
      "  21066/500000: episode: 95, duration: 0.587s, episode steps: 173, steps per second: 295, episode reward: -194.201, mean reward: -1.123 [-100.000,  2.945], mean action: 0.977 [0.000, 3.000],  loss: 10.713524, mae: 13.300592, mean_q: -12.884566\n",
      "  21315/500000: episode: 96, duration: 0.877s, episode steps: 249, steps per second: 284, episode reward: -104.562, mean reward: -0.420 [-100.000, 17.708], mean action: 1.414 [0.000, 3.000],  loss: 9.815722, mae: 13.760293, mean_q: -13.353037\n",
      "  21609/500000: episode: 97, duration: 1.022s, episode steps: 294, steps per second: 288, episode reward: -118.833, mean reward: -0.404 [-100.000, 11.860], mean action: 1.551 [0.000, 3.000],  loss: 14.091586, mae: 13.212518, mean_q: -12.272326\n",
      "  21803/500000: episode: 98, duration: 0.665s, episode steps: 194, steps per second: 292, episode reward: -247.706, mean reward: -1.277 [-100.000,  2.302], mean action: 1.675 [0.000, 3.000],  loss: 10.218560, mae: 13.170045, mean_q: -12.330912\n",
      "  22054/500000: episode: 99, duration: 0.876s, episode steps: 251, steps per second: 287, episode reward: -112.640, mean reward: -0.449 [-100.000, 10.844], mean action: 1.594 [0.000, 3.000],  loss: 8.604932, mae: 13.558240, mean_q: -12.703189\n",
      "  22352/500000: episode: 100, duration: 1.060s, episode steps: 298, steps per second: 281, episode reward: -225.214, mean reward: -0.756 [-100.000, 17.732], mean action: 1.463 [0.000, 3.000],  loss: 10.194003, mae: 13.515525, mean_q: -12.514964\n",
      "  22447/500000: episode: 101, duration: 0.324s, episode steps:  95, steps per second: 293, episode reward: -164.602, mean reward: -1.733 [-100.000, 15.151], mean action: 1.305 [0.000, 3.000],  loss: 13.243715, mae: 13.208085, mean_q: -11.988016\n",
      "  22641/500000: episode: 102, duration: 0.674s, episode steps: 194, steps per second: 288, episode reward: -51.495, mean reward: -0.265 [-100.000, 12.590], mean action: 1.474 [0.000, 3.000],  loss: 10.124418, mae: 13.778297, mean_q: -12.713228\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  22916/500000: episode: 103, duration: 0.968s, episode steps: 275, steps per second: 284, episode reward: -57.672, mean reward: -0.210 [-100.000, 15.639], mean action: 1.382 [0.000, 3.000],  loss: 9.895812, mae: 14.095515, mean_q: -13.128907\n",
      "  23011/500000: episode: 104, duration: 0.343s, episode steps:  95, steps per second: 277, episode reward: -74.542, mean reward: -0.785 [-100.000, 43.530], mean action: 1.358 [0.000, 3.000],  loss: 14.064901, mae: 13.097715, mean_q: -11.642967\n",
      "  23257/500000: episode: 105, duration: 0.884s, episode steps: 246, steps per second: 278, episode reward: -172.336, mean reward: -0.701 [-100.000, 11.299], mean action: 1.634 [0.000, 3.000],  loss: 8.729342, mae: 13.018245, mean_q: -11.400496\n",
      "  23419/500000: episode: 106, duration: 0.559s, episode steps: 162, steps per second: 290, episode reward: -177.900, mean reward: -1.098 [-100.000,  5.345], mean action: 1.167 [0.000, 3.000],  loss: 10.295568, mae: 13.408601, mean_q: -11.941586\n",
      "  23641/500000: episode: 107, duration: 0.847s, episode steps: 222, steps per second: 262, episode reward: -19.347, mean reward: -0.087 [-100.000, 10.224], mean action: 1.320 [0.000, 3.000],  loss: 9.791440, mae: 13.971673, mean_q: -12.658500\n",
      "  23929/500000: episode: 108, duration: 1.226s, episode steps: 288, steps per second: 235, episode reward: -79.308, mean reward: -0.275 [-100.000,  8.725], mean action: 1.625 [0.000, 3.000],  loss: 10.253980, mae: 13.477474, mean_q: -11.998500\n",
      "  24093/500000: episode: 109, duration: 0.646s, episode steps: 164, steps per second: 254, episode reward: -172.663, mean reward: -1.053 [-100.000,  7.333], mean action: 1.476 [0.000, 3.000],  loss: 11.902474, mae: 13.447569, mean_q: -12.005400\n",
      "  24216/500000: episode: 110, duration: 0.454s, episode steps: 123, steps per second: 271, episode reward: -172.330, mean reward: -1.401 [-100.000, 12.383], mean action: 1.333 [0.000, 3.000],  loss: 12.067319, mae: 14.153120, mean_q: -12.851128\n",
      "  24323/500000: episode: 111, duration: 0.402s, episode steps: 107, steps per second: 266, episode reward: -94.782, mean reward: -0.886 [-100.000, 15.466], mean action: 1.131 [0.000, 3.000],  loss: 9.930722, mae: 14.134582, mean_q: -13.015834\n",
      "  24547/500000: episode: 112, duration: 1.099s, episode steps: 224, steps per second: 204, episode reward: -215.688, mean reward: -0.963 [-100.000,  9.011], mean action: 1.701 [0.000, 3.000],  loss: 8.489260, mae: 13.885605, mean_q: -12.669517\n",
      "  24858/500000: episode: 113, duration: 1.421s, episode steps: 311, steps per second: 219, episode reward: -22.247, mean reward: -0.072 [-100.000, 17.531], mean action: 1.299 [0.000, 3.000],  loss: 9.622433, mae: 14.150026, mean_q: -13.058174\n",
      "  25085/500000: episode: 114, duration: 0.867s, episode steps: 227, steps per second: 262, episode reward: -44.951, mean reward: -0.198 [-100.000, 22.899], mean action: 1.493 [0.000, 3.000],  loss: 9.510332, mae: 14.647299, mean_q: -13.827991\n",
      "  25304/500000: episode: 115, duration: 0.774s, episode steps: 219, steps per second: 283, episode reward: -286.368, mean reward: -1.308 [-100.000, 17.129], mean action: 1.521 [0.000, 3.000],  loss: 10.645103, mae: 14.854054, mean_q: -14.135960\n",
      "  25705/500000: episode: 116, duration: 1.432s, episode steps: 401, steps per second: 280, episode reward: -106.569, mean reward: -0.266 [-100.000,  6.215], mean action: 1.374 [0.000, 3.000],  loss: 9.330564, mae: 14.491782, mean_q: -13.550684\n",
      "  25971/500000: episode: 117, duration: 0.932s, episode steps: 266, steps per second: 285, episode reward: -137.657, mean reward: -0.518 [-100.000, 12.773], mean action: 1.511 [0.000, 3.000],  loss: 8.224081, mae: 14.019424, mean_q: -12.854105\n",
      "  26352/500000: episode: 118, duration: 1.431s, episode steps: 381, steps per second: 266, episode reward: -133.616, mean reward: -0.351 [-100.000, 15.017], mean action: 1.703 [0.000, 3.000],  loss: 9.562395, mae: 14.551996, mean_q: -13.342028\n",
      "  26530/500000: episode: 119, duration: 0.611s, episode steps: 178, steps per second: 292, episode reward: -130.294, mean reward: -0.732 [-100.000, 17.228], mean action: 1.337 [0.000, 3.000],  loss: 9.358141, mae: 14.423822, mean_q: -13.079843\n",
      "  26670/500000: episode: 120, duration: 0.480s, episode steps: 140, steps per second: 292, episode reward: -89.745, mean reward: -0.641 [-100.000, 14.016], mean action: 1.364 [0.000, 3.000],  loss: 8.100195, mae: 13.985950, mean_q: -12.311063\n",
      "  26879/500000: episode: 121, duration: 0.726s, episode steps: 209, steps per second: 288, episode reward: -51.897, mean reward: -0.248 [-100.000, 20.939], mean action: 1.373 [0.000, 3.000],  loss: 7.044168, mae: 13.889577, mean_q: -11.990407\n",
      "  27078/500000: episode: 122, duration: 0.695s, episode steps: 199, steps per second: 286, episode reward: -82.404, mean reward: -0.414 [-100.000, 12.476], mean action: 1.508 [0.000, 3.000],  loss: 6.325909, mae: 13.831724, mean_q: -11.688729\n",
      "  27232/500000: episode: 123, duration: 0.536s, episode steps: 154, steps per second: 287, episode reward:  6.255, mean reward:  0.041 [-100.000, 38.023], mean action: 1.279 [0.000, 3.000],  loss: 7.582281, mae: 13.627126, mean_q: -11.337413\n",
      "  27445/500000: episode: 124, duration: 0.790s, episode steps: 213, steps per second: 270, episode reward: -94.994, mean reward: -0.446 [-100.000, 12.010], mean action: 1.512 [0.000, 3.000],  loss: 7.598829, mae: 14.000534, mean_q: -11.806343\n",
      "  27672/500000: episode: 125, duration: 0.796s, episode steps: 227, steps per second: 285, episode reward: -100.803, mean reward: -0.444 [-100.000, 17.845], mean action: 1.419 [0.000, 3.000],  loss: 8.203136, mae: 13.861041, mean_q: -11.311130\n",
      "  27757/500000: episode: 126, duration: 0.295s, episode steps:  85, steps per second: 288, episode reward: -104.350, mean reward: -1.228 [-100.000,  8.547], mean action: 1.035 [0.000, 2.000],  loss: 8.144984, mae: 13.991972, mean_q: -11.566090\n",
      "  27988/500000: episode: 127, duration: 0.814s, episode steps: 231, steps per second: 284, episode reward: 53.411, mean reward:  0.231 [-100.000, 20.627], mean action: 1.632 [0.000, 3.000],  loss: 6.796402, mae: 14.080646, mean_q: -11.283076\n",
      "  28211/500000: episode: 128, duration: 0.779s, episode steps: 223, steps per second: 286, episode reward: -140.054, mean reward: -0.628 [-100.000, 20.145], mean action: 1.229 [0.000, 3.000],  loss: 9.623747, mae: 13.798099, mean_q: -10.480940\n",
      "  28412/500000: episode: 129, duration: 0.701s, episode steps: 201, steps per second: 287, episode reward: -295.467, mean reward: -1.470 [-100.000, 12.754], mean action: 1.328 [0.000, 3.000],  loss: 8.854036, mae: 14.333977, mean_q: -11.094501\n",
      "  28628/500000: episode: 130, duration: 0.760s, episode steps: 216, steps per second: 284, episode reward: -70.432, mean reward: -0.326 [-100.000, 19.516], mean action: 1.426 [0.000, 3.000],  loss: 8.183414, mae: 14.397779, mean_q: -10.763072\n",
      "  28900/500000: episode: 131, duration: 0.975s, episode steps: 272, steps per second: 279, episode reward: -62.921, mean reward: -0.231 [-100.000, 16.563], mean action: 1.478 [0.000, 3.000],  loss: 7.778986, mae: 14.380437, mean_q: -10.602594\n",
      "  29239/500000: episode: 132, duration: 1.249s, episode steps: 339, steps per second: 271, episode reward: -6.106, mean reward: -0.018 [-100.000, 42.473], mean action: 1.584 [0.000, 3.000],  loss: 8.639135, mae: 13.942564, mean_q: -9.498283\n",
      "  29474/500000: episode: 133, duration: 0.829s, episode steps: 235, steps per second: 283, episode reward: -128.879, mean reward: -0.548 [-100.000, 14.020], mean action: 1.472 [0.000, 3.000],  loss: 8.076371, mae: 13.872496, mean_q: -9.122598\n",
      "  29670/500000: episode: 134, duration: 0.687s, episode steps: 196, steps per second: 285, episode reward: -209.594, mean reward: -1.069 [-100.000,  8.615], mean action: 1.765 [0.000, 3.000],  loss: 8.623082, mae: 14.046833, mean_q: -8.649553\n",
      "  29805/500000: episode: 135, duration: 0.477s, episode steps: 135, steps per second: 283, episode reward: -24.693, mean reward: -0.183 [-100.000, 11.777], mean action: 1.585 [0.000, 3.000],  loss: 7.022376, mae: 13.763362, mean_q: -8.179331\n",
      "  30074/500000: episode: 136, duration: 0.982s, episode steps: 269, steps per second: 274, episode reward: -139.113, mean reward: -0.517 [-100.000,  9.761], mean action: 1.461 [0.000, 3.000],  loss: 9.012423, mae: 14.277120, mean_q: -8.592103\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  30330/500000: episode: 137, duration: 0.925s, episode steps: 256, steps per second: 277, episode reward: -42.758, mean reward: -0.167 [-100.000, 14.712], mean action: 1.641 [0.000, 3.000],  loss: 6.960256, mae: 14.905213, mean_q: -8.533231\n",
      "  30590/500000: episode: 138, duration: 0.921s, episode steps: 260, steps per second: 282, episode reward: -21.263, mean reward: -0.082 [-100.000, 18.814], mean action: 1.485 [0.000, 3.000],  loss: 7.752639, mae: 14.267570, mean_q: -7.340553\n",
      "  30884/500000: episode: 139, duration: 1.062s, episode steps: 294, steps per second: 277, episode reward: -139.958, mean reward: -0.476 [-100.000,  5.759], mean action: 1.330 [0.000, 3.000],  loss: 9.600763, mae: 14.808602, mean_q: -7.423028\n",
      "  31110/500000: episode: 140, duration: 0.813s, episode steps: 226, steps per second: 278, episode reward: -225.815, mean reward: -0.999 [-100.000, 20.851], mean action: 1.885 [0.000, 3.000],  loss: 8.119090, mae: 14.477385, mean_q: -6.445952\n",
      "  31412/500000: episode: 141, duration: 1.088s, episode steps: 302, steps per second: 277, episode reward: -16.108, mean reward: -0.053 [-100.000, 10.164], mean action: 1.470 [0.000, 3.000],  loss: 7.443657, mae: 14.547182, mean_q: -5.844519\n",
      "  31651/500000: episode: 142, duration: 0.850s, episode steps: 239, steps per second: 281, episode reward: -74.456, mean reward: -0.312 [-100.000, 11.173], mean action: 1.820 [0.000, 3.000],  loss: 11.811441, mae: 14.543729, mean_q: -5.434319\n",
      "  31871/500000: episode: 143, duration: 0.783s, episode steps: 220, steps per second: 281, episode reward: -2.394, mean reward: -0.011 [-100.000, 15.909], mean action: 1.614 [0.000, 3.000],  loss: 8.053797, mae: 14.736864, mean_q: -5.477721\n",
      "  32103/500000: episode: 144, duration: 0.850s, episode steps: 232, steps per second: 273, episode reward: -54.857, mean reward: -0.236 [-100.000, 10.086], mean action: 1.897 [0.000, 3.000],  loss: 9.739128, mae: 14.415744, mean_q: -4.649939\n",
      "  32753/500000: episode: 145, duration: 2.467s, episode steps: 650, steps per second: 263, episode reward: 234.217, mean reward:  0.360 [-17.436, 100.000], mean action: 1.966 [0.000, 3.000],  loss: 8.105941, mae: 14.675949, mean_q: -4.143833\n",
      "  32940/500000: episode: 146, duration: 0.654s, episode steps: 187, steps per second: 286, episode reward: -111.001, mean reward: -0.594 [-100.000, 20.548], mean action: 1.690 [0.000, 3.000],  loss: 9.749598, mae: 15.108104, mean_q: -3.939521\n",
      "  33095/500000: episode: 147, duration: 0.558s, episode steps: 155, steps per second: 278, episode reward: -125.094, mean reward: -0.807 [-100.000, 13.657], mean action: 1.755 [0.000, 3.000],  loss: 7.425936, mae: 15.104234, mean_q: -3.410952\n",
      "  33304/500000: episode: 148, duration: 0.752s, episode steps: 209, steps per second: 278, episode reward: -55.654, mean reward: -0.266 [-100.000,  9.252], mean action: 1.833 [0.000, 3.000],  loss: 9.186792, mae: 15.003102, mean_q: -3.060011\n",
      "  33574/500000: episode: 149, duration: 0.979s, episode steps: 270, steps per second: 276, episode reward: -154.104, mean reward: -0.571 [-100.000, 26.837], mean action: 1.681 [0.000, 3.000],  loss: 9.633847, mae: 15.141256, mean_q: -2.742077\n",
      "  33777/500000: episode: 150, duration: 0.723s, episode steps: 203, steps per second: 281, episode reward: -80.880, mean reward: -0.398 [-100.000, 26.444], mean action: 1.714 [0.000, 3.000],  loss: 9.841043, mae: 15.691226, mean_q: -3.346190\n",
      "  33948/500000: episode: 151, duration: 0.605s, episode steps: 171, steps per second: 282, episode reward: -56.264, mean reward: -0.329 [-100.000, 14.762], mean action: 1.655 [0.000, 3.000],  loss: 10.875274, mae: 15.427643, mean_q: -2.286110\n",
      "  34104/500000: episode: 152, duration: 0.547s, episode steps: 156, steps per second: 285, episode reward: -86.831, mean reward: -0.557 [-100.000, 11.813], mean action: 1.705 [0.000, 3.000],  loss: 8.103273, mae: 15.258672, mean_q: -2.015857\n",
      "  34370/500000: episode: 153, duration: 0.975s, episode steps: 266, steps per second: 273, episode reward: -44.373, mean reward: -0.167 [-100.000, 12.588], mean action: 1.357 [0.000, 3.000],  loss: 8.143013, mae: 15.407429, mean_q: -2.034520\n",
      "  34504/500000: episode: 154, duration: 0.470s, episode steps: 134, steps per second: 285, episode reward: -97.811, mean reward: -0.730 [-100.000, 12.335], mean action: 1.709 [0.000, 3.000],  loss: 13.611298, mae: 15.313382, mean_q: -2.012468\n",
      "  34783/500000: episode: 155, duration: 1.046s, episode steps: 279, steps per second: 267, episode reward: -103.538, mean reward: -0.371 [-100.000, 21.326], mean action: 1.581 [0.000, 3.000],  loss: 8.947250, mae: 15.804673, mean_q: -2.769325\n",
      "  35025/500000: episode: 156, duration: 0.874s, episode steps: 242, steps per second: 277, episode reward: -129.165, mean reward: -0.534 [-100.000, 12.663], mean action: 1.616 [0.000, 3.000],  loss: 7.699557, mae: 15.462489, mean_q: -1.953640\n",
      "  35281/500000: episode: 157, duration: 0.921s, episode steps: 256, steps per second: 278, episode reward: -116.639, mean reward: -0.456 [-100.000, 11.622], mean action: 1.566 [0.000, 3.000],  loss: 7.885189, mae: 15.377449, mean_q: -1.623143\n",
      "  35532/500000: episode: 158, duration: 0.939s, episode steps: 251, steps per second: 267, episode reward: -66.476, mean reward: -0.265 [-100.000, 23.194], mean action: 1.526 [0.000, 3.000],  loss: 6.906626, mae: 15.232445, mean_q: -1.184816\n",
      "  35680/500000: episode: 159, duration: 0.519s, episode steps: 148, steps per second: 285, episode reward: -60.349, mean reward: -0.408 [-100.000, 16.938], mean action: 1.966 [0.000, 3.000],  loss: 10.251295, mae: 15.400239, mean_q: -0.628402\n",
      "  35836/500000: episode: 160, duration: 0.553s, episode steps: 156, steps per second: 282, episode reward: -13.636, mean reward: -0.087 [-100.000, 15.521], mean action: 1.615 [0.000, 3.000],  loss: 8.715874, mae: 16.087494, mean_q: -1.103406\n",
      "  35982/500000: episode: 161, duration: 0.513s, episode steps: 146, steps per second: 285, episode reward: -22.423, mean reward: -0.154 [-100.000, 16.795], mean action: 1.966 [0.000, 3.000],  loss: 9.772314, mae: 15.820066, mean_q: -0.219897\n",
      "  36190/500000: episode: 162, duration: 0.790s, episode steps: 208, steps per second: 263, episode reward: -69.844, mean reward: -0.336 [-100.000, 18.878], mean action: 1.721 [0.000, 3.000],  loss: 8.917972, mae: 16.351498, mean_q: -0.339401\n",
      "  36344/500000: episode: 163, duration: 0.560s, episode steps: 154, steps per second: 275, episode reward: -76.546, mean reward: -0.497 [-100.000,  9.547], mean action: 1.961 [0.000, 3.000],  loss: 10.993118, mae: 15.988964, mean_q: 0.589058\n",
      "  36518/500000: episode: 164, duration: 0.619s, episode steps: 174, steps per second: 281, episode reward: -82.488, mean reward: -0.474 [-100.000,  9.673], mean action: 1.638 [0.000, 3.000],  loss: 11.275516, mae: 16.353016, mean_q: 0.340422\n",
      "  36669/500000: episode: 165, duration: 0.539s, episode steps: 151, steps per second: 280, episode reward: -106.737, mean reward: -0.707 [-100.000, 14.308], mean action: 1.987 [0.000, 3.000],  loss: 6.919807, mae: 16.234388, mean_q: 0.744580\n",
      "  36812/500000: episode: 166, duration: 0.505s, episode steps: 143, steps per second: 283, episode reward: -134.312, mean reward: -0.939 [-100.000,  1.980], mean action: 1.951 [0.000, 3.000],  loss: 10.075842, mae: 16.725975, mean_q: -0.011957\n",
      "  37252/500000: episode: 167, duration: 1.622s, episode steps: 440, steps per second: 271, episode reward: -133.848, mean reward: -0.304 [-100.000,  8.868], mean action: 1.709 [0.000, 3.000],  loss: 10.047833, mae: 16.820272, mean_q: 0.299810\n",
      "  37733/500000: episode: 168, duration: 1.870s, episode steps: 481, steps per second: 257, episode reward: -54.907, mean reward: -0.114 [-100.000, 19.697], mean action: 1.403 [0.000, 3.000],  loss: 10.823791, mae: 17.321180, mean_q: 0.698330\n",
      "  37925/500000: episode: 169, duration: 0.694s, episode steps: 192, steps per second: 276, episode reward: -47.176, mean reward: -0.246 [-100.000, 18.521], mean action: 1.760 [0.000, 3.000],  loss: 6.744571, mae: 17.196552, mean_q: 1.206294\n",
      "  38167/500000: episode: 170, duration: 0.878s, episode steps: 242, steps per second: 276, episode reward: -235.750, mean reward: -0.974 [-100.000, 26.731], mean action: 1.785 [0.000, 3.000],  loss: 7.967696, mae: 17.104206, mean_q: 1.050415\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  39040/500000: episode: 171, duration: 3.370s, episode steps: 873, steps per second: 259, episode reward: -198.080, mean reward: -0.227 [-100.000,  8.376], mean action: 1.829 [0.000, 3.000],  loss: 7.349049, mae: 16.740555, mean_q: 1.638568\n",
      "  39440/500000: episode: 172, duration: 1.492s, episode steps: 400, steps per second: 268, episode reward: -119.381, mean reward: -0.298 [-100.000, 12.259], mean action: 1.768 [0.000, 3.000],  loss: 7.468527, mae: 17.082056, mean_q: 1.728871\n",
      "  39607/500000: episode: 173, duration: 0.597s, episode steps: 167, steps per second: 280, episode reward: -29.629, mean reward: -0.177 [-100.000, 20.050], mean action: 2.006 [0.000, 3.000],  loss: 7.221805, mae: 17.167309, mean_q: 2.407713\n",
      "  39776/500000: episode: 174, duration: 0.604s, episode steps: 169, steps per second: 280, episode reward: -60.635, mean reward: -0.359 [-100.000, 12.983], mean action: 1.811 [0.000, 3.000],  loss: 10.034839, mae: 17.086485, mean_q: 2.144975\n",
      "  39932/500000: episode: 175, duration: 0.551s, episode steps: 156, steps per second: 283, episode reward: -130.662, mean reward: -0.838 [-100.000,  9.013], mean action: 1.974 [0.000, 3.000],  loss: 7.770670, mae: 16.820322, mean_q: 2.409415\n",
      "  40114/500000: episode: 176, duration: 0.649s, episode steps: 182, steps per second: 280, episode reward: -170.718, mean reward: -0.938 [-100.000,  4.062], mean action: 1.995 [0.000, 3.000],  loss: 8.395789, mae: 17.110842, mean_q: 2.498172\n",
      "  40257/500000: episode: 177, duration: 0.507s, episode steps: 143, steps per second: 282, episode reward: -27.921, mean reward: -0.195 [-100.000, 15.290], mean action: 1.937 [0.000, 3.000],  loss: 10.186829, mae: 17.439777, mean_q: 2.285114\n",
      "  40460/500000: episode: 178, duration: 0.734s, episode steps: 203, steps per second: 277, episode reward: -93.169, mean reward: -0.459 [-100.000, 24.480], mean action: 1.768 [0.000, 3.000],  loss: 8.792557, mae: 17.005180, mean_q: 3.047790\n",
      "  40599/500000: episode: 179, duration: 0.494s, episode steps: 139, steps per second: 282, episode reward: -70.659, mean reward: -0.508 [-100.000, 18.427], mean action: 1.942 [0.000, 3.000],  loss: 11.381634, mae: 17.393187, mean_q: 2.582339\n",
      "  40914/500000: episode: 180, duration: 1.155s, episode steps: 315, steps per second: 273, episode reward: -74.887, mean reward: -0.238 [-100.000, 10.282], mean action: 1.879 [0.000, 3.000],  loss: 12.757416, mae: 17.408182, mean_q: 3.345356\n",
      "  41914/500000: episode: 181, duration: 4.312s, episode steps: 1000, steps per second: 232, episode reward: -70.099, mean reward: -0.070 [-5.138,  5.255], mean action: 1.959 [0.000, 3.000],  loss: 9.068598, mae: 17.643084, mean_q: 3.549835\n",
      "  42322/500000: episode: 182, duration: 1.530s, episode steps: 408, steps per second: 267, episode reward: 130.343, mean reward:  0.319 [-8.962, 100.000], mean action: 2.265 [0.000, 3.000],  loss: 9.587906, mae: 17.602184, mean_q: 4.290337\n",
      "  42474/500000: episode: 183, duration: 0.543s, episode steps: 152, steps per second: 280, episode reward: -80.637, mean reward: -0.531 [-100.000, 17.202], mean action: 1.947 [0.000, 3.000],  loss: 5.563022, mae: 17.881823, mean_q: 4.070814\n",
      "  42724/500000: episode: 184, duration: 0.929s, episode steps: 250, steps per second: 269, episode reward: -230.896, mean reward: -0.924 [-100.000, 29.932], mean action: 1.740 [0.000, 3.000],  loss: 8.975689, mae: 17.555182, mean_q: 5.080867\n",
      "  42904/500000: episode: 185, duration: 0.648s, episode steps: 180, steps per second: 278, episode reward: -327.080, mean reward: -1.817 [-100.000,  8.531], mean action: 1.778 [0.000, 3.000],  loss: 13.316908, mae: 18.098194, mean_q: 4.462398\n",
      "  43220/500000: episode: 186, duration: 1.197s, episode steps: 316, steps per second: 264, episode reward: -37.088, mean reward: -0.117 [-100.000, 17.324], mean action: 1.943 [0.000, 3.000],  loss: 7.505857, mae: 17.909924, mean_q: 4.710261\n",
      "  43361/500000: episode: 187, duration: 0.507s, episode steps: 141, steps per second: 278, episode reward: -43.066, mean reward: -0.305 [-100.000, 28.540], mean action: 2.007 [0.000, 3.000],  loss: 8.364744, mae: 17.948454, mean_q: 5.022521\n",
      "  43541/500000: episode: 188, duration: 0.648s, episode steps: 180, steps per second: 278, episode reward: -141.048, mean reward: -0.784 [-100.000, 12.221], mean action: 1.744 [0.000, 3.000],  loss: 8.332057, mae: 18.405134, mean_q: 4.649596\n",
      "  44522/500000: episode: 189, duration: 4.320s, episode steps: 981, steps per second: 227, episode reward: -229.315, mean reward: -0.234 [-100.000, 12.684], mean action: 1.860 [0.000, 3.000],  loss: 9.790423, mae: 18.351610, mean_q: 5.236396\n",
      "  44663/500000: episode: 190, duration: 0.508s, episode steps: 141, steps per second: 278, episode reward: -61.947, mean reward: -0.439 [-100.000, 16.988], mean action: 1.872 [0.000, 3.000],  loss: 8.838888, mae: 18.892250, mean_q: 5.061062\n",
      "  44798/500000: episode: 191, duration: 0.484s, episode steps: 135, steps per second: 279, episode reward: -31.314, mean reward: -0.232 [-100.000, 12.928], mean action: 2.022 [0.000, 3.000],  loss: 7.488261, mae: 18.632252, mean_q: 6.309425\n",
      "  45002/500000: episode: 192, duration: 0.740s, episode steps: 204, steps per second: 276, episode reward: -114.376, mean reward: -0.561 [-100.000, 18.915], mean action: 1.730 [0.000, 3.000],  loss: 8.019725, mae: 19.011309, mean_q: 6.206999\n",
      "  45125/500000: episode: 193, duration: 0.444s, episode steps: 123, steps per second: 277, episode reward: -54.380, mean reward: -0.442 [-100.000, 12.296], mean action: 1.984 [0.000, 3.000],  loss: 13.083888, mae: 19.356607, mean_q: 6.022081\n",
      "  45259/500000: episode: 194, duration: 0.483s, episode steps: 134, steps per second: 278, episode reward: -103.285, mean reward: -0.771 [-100.000,  8.840], mean action: 2.030 [0.000, 3.000],  loss: 10.689343, mae: 19.191137, mean_q: 6.101518\n",
      "  45423/500000: episode: 195, duration: 0.598s, episode steps: 164, steps per second: 274, episode reward: -56.476, mean reward: -0.344 [-100.000, 21.937], mean action: 1.970 [0.000, 3.000],  loss: 15.536475, mae: 19.041155, mean_q: 6.366605\n",
      "  46423/500000: episode: 196, duration: 4.578s, episode steps: 1000, steps per second: 218, episode reward: -127.529, mean reward: -0.128 [-5.488,  5.330], mean action: 1.954 [0.000, 3.000],  loss: 10.981548, mae: 18.579605, mean_q: 5.930182\n",
      "  46598/500000: episode: 197, duration: 0.649s, episode steps: 175, steps per second: 270, episode reward: -26.240, mean reward: -0.150 [-100.000, 16.085], mean action: 1.771 [0.000, 3.000],  loss: 10.986587, mae: 18.046638, mean_q: 7.020889\n",
      "  46933/500000: episode: 198, duration: 1.249s, episode steps: 335, steps per second: 268, episode reward: -86.456, mean reward: -0.258 [-100.000, 17.404], mean action: 1.893 [0.000, 3.000],  loss: 8.635242, mae: 18.126991, mean_q: 6.404655\n",
      "  47052/500000: episode: 199, duration: 0.433s, episode steps: 119, steps per second: 275, episode reward: -65.827, mean reward: -0.553 [-100.000,  7.651], mean action: 2.059 [0.000, 3.000],  loss: 10.431879, mae: 18.223700, mean_q: 7.631872\n",
      "  47185/500000: episode: 200, duration: 0.482s, episode steps: 133, steps per second: 276, episode reward: -14.366, mean reward: -0.108 [-100.000, 20.508], mean action: 1.759 [0.000, 3.000],  loss: 9.646451, mae: 18.321611, mean_q: 7.248514\n",
      "  47486/500000: episode: 201, duration: 1.133s, episode steps: 301, steps per second: 266, episode reward: -128.942, mean reward: -0.428 [-100.000,  7.614], mean action: 1.987 [0.000, 3.000],  loss: 10.144479, mae: 17.974787, mean_q: 8.209761\n",
      "  47795/500000: episode: 202, duration: 1.159s, episode steps: 309, steps per second: 266, episode reward: -70.524, mean reward: -0.228 [-100.000, 15.915], mean action: 1.987 [0.000, 3.000],  loss: 7.962077, mae: 18.338661, mean_q: 7.464188\n",
      "  47932/500000: episode: 203, duration: 0.497s, episode steps: 137, steps per second: 276, episode reward: -72.885, mean reward: -0.532 [-100.000, 15.113], mean action: 2.029 [0.000, 3.000],  loss: 7.734240, mae: 18.280687, mean_q: 7.027401\n",
      "  48201/500000: episode: 204, duration: 0.987s, episode steps: 269, steps per second: 273, episode reward: -196.156, mean reward: -0.729 [-100.000, 80.964], mean action: 1.677 [0.000, 3.000],  loss: 11.707485, mae: 18.164251, mean_q: 7.610736\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  48393/500000: episode: 205, duration: 0.708s, episode steps: 192, steps per second: 271, episode reward: -134.017, mean reward: -0.698 [-100.000,  5.882], mean action: 1.880 [0.000, 3.000],  loss: 7.576789, mae: 17.853401, mean_q: 8.205998\n",
      "  48792/500000: episode: 206, duration: 1.607s, episode steps: 399, steps per second: 248, episode reward:  8.798, mean reward:  0.022 [-100.000, 26.705], mean action: 1.835 [0.000, 3.000],  loss: 10.151585, mae: 18.171574, mean_q: 7.864681\n",
      "  49098/500000: episode: 207, duration: 1.143s, episode steps: 306, steps per second: 268, episode reward: -21.751, mean reward: -0.071 [-100.000, 13.685], mean action: 1.977 [0.000, 3.000],  loss: 9.643100, mae: 18.207481, mean_q: 8.032268\n",
      "  49271/500000: episode: 208, duration: 0.634s, episode steps: 173, steps per second: 273, episode reward: -60.957, mean reward: -0.352 [-100.000, 12.144], mean action: 1.775 [0.000, 3.000],  loss: 11.869032, mae: 18.279453, mean_q: 8.511655\n",
      "  50271/500000: episode: 209, duration: 4.371s, episode steps: 1000, steps per second: 229, episode reward: -131.177, mean reward: -0.131 [-7.548, 12.407], mean action: 1.975 [0.000, 3.000],  loss: 8.505864, mae: 17.597618, mean_q: 9.577383\n",
      "  50472/500000: episode: 210, duration: 0.750s, episode steps: 201, steps per second: 268, episode reward: -48.354, mean reward: -0.241 [-100.000, 20.330], mean action: 2.030 [0.000, 3.000],  loss: 9.672680, mae: 17.771034, mean_q: 10.432658\n",
      "  50605/500000: episode: 211, duration: 0.483s, episode steps: 133, steps per second: 276, episode reward: -7.131, mean reward: -0.054 [-100.000, 18.321], mean action: 2.000 [0.000, 3.000],  loss: 6.769231, mae: 17.990686, mean_q: 10.509373\n",
      "  50893/500000: episode: 212, duration: 1.064s, episode steps: 288, steps per second: 271, episode reward:  1.698, mean reward:  0.006 [-100.000, 18.986], mean action: 1.972 [0.000, 3.000],  loss: 13.376212, mae: 18.234835, mean_q: 10.701265\n",
      "  51060/500000: episode: 213, duration: 0.609s, episode steps: 167, steps per second: 274, episode reward: -11.313, mean reward: -0.068 [-100.000, 13.958], mean action: 2.042 [0.000, 3.000],  loss: 8.038404, mae: 18.482643, mean_q: 10.569855\n",
      "  51187/500000: episode: 214, duration: 0.466s, episode steps: 127, steps per second: 273, episode reward: -70.905, mean reward: -0.558 [-100.000,  8.244], mean action: 2.024 [0.000, 3.000],  loss: 12.764908, mae: 18.562969, mean_q: 10.826319\n",
      "  51396/500000: episode: 215, duration: 0.762s, episode steps: 209, steps per second: 274, episode reward: -73.504, mean reward: -0.352 [-100.000, 12.989], mean action: 1.799 [0.000, 3.000],  loss: 10.519261, mae: 18.667095, mean_q: 11.534348\n",
      "  51599/500000: episode: 216, duration: 0.740s, episode steps: 203, steps per second: 274, episode reward: -251.549, mean reward: -1.239 [-100.000, 28.240], mean action: 1.695 [0.000, 3.000],  loss: 11.894176, mae: 18.602243, mean_q: 11.744547\n",
      "  52042/500000: episode: 217, duration: 1.708s, episode steps: 443, steps per second: 259, episode reward: -113.576, mean reward: -0.256 [-100.000,  4.292], mean action: 1.941 [0.000, 3.000],  loss: 12.852570, mae: 19.161737, mean_q: 11.596569\n",
      "  52640/500000: episode: 218, duration: 2.370s, episode steps: 598, steps per second: 252, episode reward: -150.024, mean reward: -0.251 [-100.000,  5.712], mean action: 1.941 [0.000, 3.000],  loss: 11.196265, mae: 19.036789, mean_q: 12.489287\n",
      "  53640/500000: episode: 219, duration: 4.748s, episode steps: 1000, steps per second: 211, episode reward: -109.829, mean reward: -0.110 [-5.115,  6.196], mean action: 1.930 [0.000, 3.000],  loss: 10.054450, mae: 19.801035, mean_q: 13.050659\n",
      "  54258/500000: episode: 220, duration: 2.620s, episode steps: 618, steps per second: 236, episode reward: 145.690, mean reward:  0.236 [-22.754, 100.000], mean action: 1.934 [0.000, 3.000],  loss: 9.778785, mae: 20.376917, mean_q: 14.990911\n",
      "  55258/500000: episode: 221, duration: 4.292s, episode steps: 1000, steps per second: 233, episode reward: -116.625, mean reward: -0.117 [-6.073,  5.542], mean action: 1.934 [0.000, 3.000],  loss: 8.810993, mae: 21.033428, mean_q: 15.726213\n",
      "  55418/500000: episode: 222, duration: 0.579s, episode steps: 160, steps per second: 276, episode reward: -46.825, mean reward: -0.293 [-100.000, 13.079], mean action: 1.837 [0.000, 3.000],  loss: 10.798935, mae: 21.487162, mean_q: 17.450420\n",
      "  56418/500000: episode: 223, duration: 4.079s, episode steps: 1000, steps per second: 245, episode reward: -64.283, mean reward: -0.064 [-5.366,  6.268], mean action: 1.816 [0.000, 3.000],  loss: 11.552084, mae: 22.350586, mean_q: 17.533613\n",
      "  57418/500000: episode: 224, duration: 5.021s, episode steps: 1000, steps per second: 199, episode reward: -35.561, mean reward: -0.036 [-5.716,  5.781], mean action: 1.679 [0.000, 3.000],  loss: 10.150072, mae: 23.336168, mean_q: 20.229862\n",
      "  57627/500000: episode: 225, duration: 0.768s, episode steps: 209, steps per second: 272, episode reward: 44.119, mean reward:  0.211 [-100.000, 18.666], mean action: 1.904 [0.000, 3.000],  loss: 9.908578, mae: 24.444874, mean_q: 22.491125\n",
      "  58627/500000: episode: 226, duration: 4.901s, episode steps: 1000, steps per second: 204, episode reward: -91.110, mean reward: -0.091 [-5.781,  6.251], mean action: 1.732 [0.000, 3.000],  loss: 9.769046, mae: 25.356617, mean_q: 23.694124\n",
      "  59627/500000: episode: 227, duration: 4.477s, episode steps: 1000, steps per second: 223, episode reward: -39.484, mean reward: -0.039 [-6.062,  5.416], mean action: 1.605 [0.000, 3.000],  loss: 11.116547, mae: 27.067345, mean_q: 26.051367\n",
      "  60627/500000: episode: 228, duration: 4.525s, episode steps: 1000, steps per second: 221, episode reward: -79.679, mean reward: -0.080 [-5.717,  5.392], mean action: 1.575 [0.000, 3.000],  loss: 10.531730, mae: 29.894922, mean_q: 30.492340\n",
      "  61627/500000: episode: 229, duration: 4.084s, episode steps: 1000, steps per second: 245, episode reward: -7.925, mean reward: -0.008 [-5.128,  5.977], mean action: 1.522 [0.000, 3.000],  loss: 12.666423, mae: 32.781757, mean_q: 34.710438\n",
      "  61866/500000: episode: 230, duration: 0.872s, episode steps: 239, steps per second: 274, episode reward: -2.096, mean reward: -0.009 [-100.000, 14.397], mean action: 1.682 [0.000, 3.000],  loss: 12.614876, mae: 34.509212, mean_q: 36.967915\n",
      "  62068/500000: episode: 231, duration: 0.744s, episode steps: 202, steps per second: 271, episode reward: -17.001, mean reward: -0.084 [-100.000, 87.394], mean action: 1.619 [0.000, 3.000],  loss: 12.333686, mae: 34.575356, mean_q: 38.640602\n",
      "  62228/500000: episode: 232, duration: 0.578s, episode steps: 160, steps per second: 277, episode reward: -68.638, mean reward: -0.429 [-100.000, 11.598], mean action: 1.494 [0.000, 3.000],  loss: 10.427564, mae: 34.886333, mean_q: 38.441532\n",
      "  62506/500000: episode: 233, duration: 1.017s, episode steps: 278, steps per second: 273, episode reward: -21.834, mean reward: -0.079 [-100.000, 14.345], mean action: 1.496 [0.000, 3.000],  loss: 6.894310, mae: 35.661495, mean_q: 39.748558\n",
      "  62664/500000: episode: 234, duration: 0.568s, episode steps: 158, steps per second: 278, episode reward: -50.859, mean reward: -0.322 [-100.000, 16.991], mean action: 1.551 [0.000, 3.000],  loss: 11.887217, mae: 36.504974, mean_q: 41.122803\n",
      "  63664/500000: episode: 235, duration: 4.637s, episode steps: 1000, steps per second: 216, episode reward: -59.077, mean reward: -0.059 [-4.703,  4.310], mean action: 1.510 [0.000, 3.000],  loss: 9.720596, mae: 37.451801, mean_q: 43.311096\n",
      "  64511/500000: episode: 236, duration: 3.800s, episode steps: 847, steps per second: 223, episode reward: -168.569, mean reward: -0.199 [-100.000, 12.537], mean action: 1.662 [0.000, 3.000],  loss: 11.865727, mae: 39.522003, mean_q: 46.905815\n",
      "  64692/500000: episode: 237, duration: 0.672s, episode steps: 181, steps per second: 269, episode reward: -80.089, mean reward: -0.442 [-100.000,  6.654], mean action: 1.718 [0.000, 3.000],  loss: 8.756616, mae: 40.450447, mean_q: 48.765713\n",
      "  64937/500000: episode: 238, duration: 0.913s, episode steps: 245, steps per second: 268, episode reward: -68.246, mean reward: -0.279 [-100.000, 15.608], mean action: 1.678 [0.000, 3.000],  loss: 11.887137, mae: 41.067562, mean_q: 49.272339\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  65386/500000: episode: 239, duration: 1.794s, episode steps: 449, steps per second: 250, episode reward: -78.982, mean reward: -0.176 [-100.000, 18.354], mean action: 1.450 [0.000, 3.000],  loss: 10.409628, mae: 41.187744, mean_q: 49.963337\n",
      "  65593/500000: episode: 240, duration: 0.775s, episode steps: 207, steps per second: 267, episode reward: -26.991, mean reward: -0.130 [-100.000, 15.983], mean action: 1.580 [0.000, 3.000],  loss: 6.938948, mae: 41.804695, mean_q: 51.444000\n",
      "  66593/500000: episode: 241, duration: 4.594s, episode steps: 1000, steps per second: 218, episode reward: -81.497, mean reward: -0.081 [-4.931,  4.876], mean action: 1.543 [0.000, 3.000],  loss: 9.306623, mae: 42.604172, mean_q: 52.752853\n",
      "  66796/500000: episode: 242, duration: 0.759s, episode steps: 203, steps per second: 267, episode reward: -21.071, mean reward: -0.104 [-100.000, 16.646], mean action: 1.547 [0.000, 3.000],  loss: 9.415795, mae: 43.372421, mean_q: 54.361992\n",
      "  67302/500000: episode: 243, duration: 2.074s, episode steps: 506, steps per second: 244, episode reward: -345.994, mean reward: -0.684 [-100.000, 36.147], mean action: 1.739 [0.000, 3.000],  loss: 13.177092, mae: 44.317108, mean_q: 54.756321\n",
      "  68302/500000: episode: 244, duration: 4.319s, episode steps: 1000, steps per second: 232, episode reward: -38.730, mean reward: -0.039 [-5.670,  4.778], mean action: 1.544 [0.000, 3.000],  loss: 10.216887, mae: 44.985718, mean_q: 56.211296\n",
      "  69302/500000: episode: 245, duration: 4.525s, episode steps: 1000, steps per second: 221, episode reward: -16.765, mean reward: -0.017 [-4.946,  4.477], mean action: 1.559 [0.000, 3.000],  loss: 12.511878, mae: 46.793003, mean_q: 58.877930\n",
      "  69668/500000: episode: 246, duration: 1.424s, episode steps: 366, steps per second: 257, episode reward: -45.375, mean reward: -0.124 [-100.000, 16.335], mean action: 1.754 [0.000, 3.000],  loss: 7.968174, mae: 47.639874, mean_q: 60.130203\n",
      "  69811/500000: episode: 247, duration: 0.531s, episode steps: 143, steps per second: 269, episode reward: -406.742, mean reward: -2.844 [-100.000,  1.956], mean action: 1.958 [0.000, 3.000],  loss: 8.569396, mae: 47.936272, mean_q: 61.702721\n",
      "  70149/500000: episode: 248, duration: 1.309s, episode steps: 338, steps per second: 258, episode reward: -50.188, mean reward: -0.148 [-100.000, 16.264], mean action: 1.746 [0.000, 3.000],  loss: 14.290273, mae: 48.388893, mean_q: 61.055420\n",
      "  71149/500000: episode: 249, duration: 4.604s, episode steps: 1000, steps per second: 217, episode reward: -90.452, mean reward: -0.090 [-3.598,  4.116], mean action: 1.643 [0.000, 3.000],  loss: 12.041572, mae: 48.776085, mean_q: 61.886353\n",
      "  72149/500000: episode: 250, duration: 4.228s, episode steps: 1000, steps per second: 237, episode reward: -115.850, mean reward: -0.116 [-4.814,  4.692], mean action: 1.598 [0.000, 3.000],  loss: 10.277875, mae: 49.302967, mean_q: 62.641796\n",
      "  73149/500000: episode: 251, duration: 4.127s, episode steps: 1000, steps per second: 242, episode reward: -34.114, mean reward: -0.034 [-5.325,  4.253], mean action: 1.667 [0.000, 3.000],  loss: 12.209752, mae: 49.346260, mean_q: 62.392921\n",
      "  74149/500000: episode: 252, duration: 4.455s, episode steps: 1000, steps per second: 224, episode reward: -75.407, mean reward: -0.075 [-4.299,  4.499], mean action: 1.631 [0.000, 3.000],  loss: 10.725692, mae: 49.043987, mean_q: 61.972042\n",
      "  74350/500000: episode: 253, duration: 0.737s, episode steps: 201, steps per second: 273, episode reward: -78.038, mean reward: -0.388 [-100.000, 15.968], mean action: 1.836 [0.000, 3.000],  loss: 7.331557, mae: 48.792015, mean_q: 61.347355\n",
      "  74638/500000: episode: 254, duration: 1.069s, episode steps: 288, steps per second: 269, episode reward: -183.609, mean reward: -0.638 [-100.000, 62.247], mean action: 1.427 [0.000, 3.000],  loss: 7.891720, mae: 49.143330, mean_q: 62.029984\n",
      "  74819/500000: episode: 255, duration: 0.663s, episode steps: 181, steps per second: 273, episode reward: -58.106, mean reward: -0.321 [-100.000, 10.307], mean action: 1.768 [0.000, 3.000],  loss: 6.159908, mae: 48.634319, mean_q: 61.054085\n",
      "  75819/500000: episode: 256, duration: 4.568s, episode steps: 1000, steps per second: 219, episode reward: -153.080, mean reward: -0.153 [-6.356,  5.025], mean action: 1.808 [0.000, 3.000],  loss: 13.116369, mae: 48.397449, mean_q: 61.031757\n",
      "  76760/500000: episode: 257, duration: 4.542s, episode steps: 941, steps per second: 207, episode reward: -244.365, mean reward: -0.260 [-100.000,  8.364], mean action: 1.680 [0.000, 3.000],  loss: 10.356339, mae: 47.651833, mean_q: 60.406998\n",
      "  77760/500000: episode: 258, duration: 4.732s, episode steps: 1000, steps per second: 211, episode reward: -84.751, mean reward: -0.085 [-4.177,  4.511], mean action: 1.790 [0.000, 3.000],  loss: 11.829141, mae: 47.187347, mean_q: 59.498371\n",
      "  78760/500000: episode: 259, duration: 4.498s, episode steps: 1000, steps per second: 222, episode reward: -92.032, mean reward: -0.092 [-5.055,  4.622], mean action: 1.729 [0.000, 3.000],  loss: 11.039247, mae: 46.650070, mean_q: 59.215538\n",
      "  79760/500000: episode: 260, duration: 4.252s, episode steps: 1000, steps per second: 235, episode reward: -71.464, mean reward: -0.071 [-4.926,  4.819], mean action: 1.769 [0.000, 3.000],  loss: 7.105776, mae: 46.012592, mean_q: 58.603062\n",
      "  80760/500000: episode: 261, duration: 4.721s, episode steps: 1000, steps per second: 212, episode reward: -73.726, mean reward: -0.074 [-4.084,  4.231], mean action: 1.810 [0.000, 3.000],  loss: 7.967759, mae: 45.616844, mean_q: 57.971970\n",
      "  81760/500000: episode: 262, duration: 4.579s, episode steps: 1000, steps per second: 218, episode reward: -154.357, mean reward: -0.154 [-3.933,  4.066], mean action: 1.774 [0.000, 3.000],  loss: 7.801902, mae: 45.146626, mean_q: 57.693157\n",
      "  82760/500000: episode: 263, duration: 4.339s, episode steps: 1000, steps per second: 230, episode reward: -72.437, mean reward: -0.072 [-3.998,  4.214], mean action: 1.749 [0.000, 3.000],  loss: 11.415572, mae: 44.540829, mean_q: 56.327545\n",
      "  83760/500000: episode: 264, duration: 4.524s, episode steps: 1000, steps per second: 221, episode reward: -70.964, mean reward: -0.071 [-4.134,  4.243], mean action: 1.742 [0.000, 3.000],  loss: 6.714340, mae: 44.439327, mean_q: 56.464073\n",
      "  84760/500000: episode: 265, duration: 4.308s, episode steps: 1000, steps per second: 232, episode reward: -54.238, mean reward: -0.054 [-4.406,  5.345], mean action: 1.856 [0.000, 3.000],  loss: 7.321414, mae: 43.938534, mean_q: 56.438438\n",
      "  85760/500000: episode: 266, duration: 4.170s, episode steps: 1000, steps per second: 240, episode reward: -79.984, mean reward: -0.080 [-4.667,  4.713], mean action: 1.794 [0.000, 3.000],  loss: 8.684094, mae: 43.756233, mean_q: 55.573696\n",
      "  86760/500000: episode: 267, duration: 4.158s, episode steps: 1000, steps per second: 240, episode reward: -65.929, mean reward: -0.066 [-4.934,  5.590], mean action: 1.828 [0.000, 3.000],  loss: 8.740330, mae: 43.504154, mean_q: 55.304314\n",
      "  87760/500000: episode: 268, duration: 4.025s, episode steps: 1000, steps per second: 248, episode reward: -79.248, mean reward: -0.079 [-3.613,  4.296], mean action: 1.789 [0.000, 3.000],  loss: 5.973763, mae: 43.411133, mean_q: 55.089531\n",
      "  88760/500000: episode: 269, duration: 4.404s, episode steps: 1000, steps per second: 227, episode reward: -48.968, mean reward: -0.049 [-4.228,  4.075], mean action: 1.877 [0.000, 3.000],  loss: 5.926006, mae: 43.399475, mean_q: 54.947800\n",
      "  88876/500000: episode: 270, duration: 0.425s, episode steps: 116, steps per second: 273, episode reward: -233.343, mean reward: -2.012 [-100.000,  7.573], mean action: 1.810 [0.000, 3.000],  loss: 11.586907, mae: 43.222172, mean_q: 55.161476\n",
      "  89876/500000: episode: 271, duration: 4.668s, episode steps: 1000, steps per second: 214, episode reward: -87.523, mean reward: -0.088 [-4.590,  5.275], mean action: 1.889 [0.000, 3.000],  loss: 9.280788, mae: 42.820274, mean_q: 54.567799\n",
      "  90876/500000: episode: 272, duration: 4.753s, episode steps: 1000, steps per second: 210, episode reward: -29.588, mean reward: -0.030 [-4.845,  4.842], mean action: 1.918 [0.000, 3.000],  loss: 9.350681, mae: 42.027397, mean_q: 53.689964\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  91876/500000: episode: 273, duration: 4.218s, episode steps: 1000, steps per second: 237, episode reward: -26.949, mean reward: -0.027 [-5.615,  4.525], mean action: 1.856 [0.000, 3.000],  loss: 5.140697, mae: 41.572868, mean_q: 53.361107\n",
      "  92876/500000: episode: 274, duration: 5.612s, episode steps: 1000, steps per second: 178, episode reward: -32.652, mean reward: -0.033 [-5.269,  4.543], mean action: 1.769 [0.000, 3.000],  loss: 8.134213, mae: 41.382038, mean_q: 53.039093\n",
      "  93876/500000: episode: 275, duration: 5.127s, episode steps: 1000, steps per second: 195, episode reward: -17.882, mean reward: -0.018 [-5.206,  4.196], mean action: 1.853 [0.000, 3.000],  loss: 6.930369, mae: 40.796459, mean_q: 52.962818\n",
      "  94876/500000: episode: 276, duration: 4.815s, episode steps: 1000, steps per second: 208, episode reward: -30.994, mean reward: -0.031 [-4.583,  4.799], mean action: 1.816 [0.000, 3.000],  loss: 7.090418, mae: 40.140377, mean_q: 52.275181\n",
      "  95876/500000: episode: 277, duration: 6.297s, episode steps: 1000, steps per second: 159, episode reward: -30.676, mean reward: -0.031 [-5.347,  5.283], mean action: 1.783 [0.000, 3.000],  loss: 7.408745, mae: 39.414436, mean_q: 51.534641\n",
      "  96876/500000: episode: 278, duration: 5.877s, episode steps: 1000, steps per second: 170, episode reward: -39.193, mean reward: -0.039 [-13.121, 12.607], mean action: 1.807 [0.000, 3.000],  loss: 7.029664, mae: 38.455612, mean_q: 50.308006\n",
      "  97876/500000: episode: 279, duration: 7.381s, episode steps: 1000, steps per second: 135, episode reward: -18.841, mean reward: -0.019 [-10.415, 13.489], mean action: 1.842 [0.000, 3.000],  loss: 7.639502, mae: 37.501652, mean_q: 49.252071\n",
      "  98876/500000: episode: 280, duration: 4.660s, episode steps: 1000, steps per second: 215, episode reward: -74.380, mean reward: -0.074 [-4.971,  4.536], mean action: 1.684 [0.000, 3.000],  loss: 5.468163, mae: 36.712238, mean_q: 48.208866\n",
      "  99876/500000: episode: 281, duration: 4.830s, episode steps: 1000, steps per second: 207, episode reward: -70.208, mean reward: -0.070 [-4.429,  4.431], mean action: 1.691 [0.000, 3.000],  loss: 5.261521, mae: 35.341587, mean_q: 46.522205\n",
      " 100876/500000: episode: 282, duration: 4.282s, episode steps: 1000, steps per second: 234, episode reward: -14.774, mean reward: -0.015 [-3.777,  4.307], mean action: 1.643 [0.000, 3.000],  loss: 3.692584, mae: 34.323452, mean_q: 45.296215\n",
      " 101876/500000: episode: 283, duration: 5.346s, episode steps: 1000, steps per second: 187, episode reward: -25.756, mean reward: -0.026 [-4.280,  4.417], mean action: 1.646 [0.000, 3.000],  loss: 3.609569, mae: 33.382805, mean_q: 44.315262\n",
      " 102782/500000: episode: 284, duration: 3.942s, episode steps: 906, steps per second: 230, episode reward: -186.102, mean reward: -0.205 [-100.000, 10.350], mean action: 1.770 [0.000, 3.000],  loss: 4.347829, mae: 32.273083, mean_q: 42.797981\n",
      " 103096/500000: episode: 285, duration: 1.199s, episode steps: 314, steps per second: 262, episode reward: -127.053, mean reward: -0.405 [-100.000,  6.912], mean action: 1.637 [0.000, 3.000],  loss: 3.490774, mae: 32.055656, mean_q: 42.302059\n",
      " 103538/500000: episode: 286, duration: 1.716s, episode steps: 442, steps per second: 258, episode reward: -144.317, mean reward: -0.327 [-100.000,  7.241], mean action: 1.588 [0.000, 3.000],  loss: 3.179941, mae: 31.617682, mean_q: 41.951401\n",
      " 104538/500000: episode: 287, duration: 4.363s, episode steps: 1000, steps per second: 229, episode reward: -142.051, mean reward: -0.142 [-4.044,  4.732], mean action: 1.615 [0.000, 3.000],  loss: 2.674932, mae: 30.689383, mean_q: 40.589443\n",
      " 105274/500000: episode: 288, duration: 3.017s, episode steps: 736, steps per second: 244, episode reward: -149.746, mean reward: -0.203 [-100.000, 12.365], mean action: 1.654 [0.000, 3.000],  loss: 2.483632, mae: 29.742310, mean_q: 39.025089\n",
      " 105469/500000: episode: 289, duration: 0.705s, episode steps: 195, steps per second: 277, episode reward: -124.474, mean reward: -0.638 [-100.000,  3.212], mean action: 1.497 [0.000, 3.000],  loss: 4.094599, mae: 29.310003, mean_q: 38.282360\n",
      " 106469/500000: episode: 290, duration: 4.637s, episode steps: 1000, steps per second: 216, episode reward: -92.096, mean reward: -0.092 [-5.248,  6.657], mean action: 1.786 [0.000, 3.000],  loss: 2.899042, mae: 28.695860, mean_q: 37.370766\n",
      " 107469/500000: episode: 291, duration: 4.832s, episode steps: 1000, steps per second: 207, episode reward: -70.732, mean reward: -0.071 [-4.681,  4.541], mean action: 1.655 [0.000, 3.000],  loss: 3.834753, mae: 27.617638, mean_q: 35.757767\n",
      " 108469/500000: episode: 292, duration: 4.964s, episode steps: 1000, steps per second: 201, episode reward: -57.512, mean reward: -0.058 [-4.256,  5.424], mean action: 1.782 [0.000, 3.000],  loss: 4.312762, mae: 27.157017, mean_q: 34.904663\n",
      " 109469/500000: episode: 293, duration: 4.266s, episode steps: 1000, steps per second: 234, episode reward: -79.997, mean reward: -0.080 [-4.774,  5.062], mean action: 1.678 [0.000, 3.000],  loss: 4.276574, mae: 25.838495, mean_q: 33.281342\n",
      " 110469/500000: episode: 294, duration: 5.094s, episode steps: 1000, steps per second: 196, episode reward: -76.832, mean reward: -0.077 [-4.688,  4.627], mean action: 1.785 [0.000, 3.000],  loss: 3.411672, mae: 25.198740, mean_q: 32.212242\n",
      " 111469/500000: episode: 295, duration: 5.303s, episode steps: 1000, steps per second: 189, episode reward: -52.573, mean reward: -0.053 [-4.716,  5.264], mean action: 1.656 [0.000, 3.000],  loss: 2.663249, mae: 24.801546, mean_q: 31.791573\n",
      " 112469/500000: episode: 296, duration: 4.233s, episode steps: 1000, steps per second: 236, episode reward: -65.813, mean reward: -0.066 [-4.064,  4.430], mean action: 1.783 [0.000, 3.000],  loss: 2.652849, mae: 24.276199, mean_q: 30.855722\n",
      " 113469/500000: episode: 297, duration: 4.564s, episode steps: 1000, steps per second: 219, episode reward: -41.604, mean reward: -0.042 [-5.149,  5.786], mean action: 1.778 [0.000, 3.000],  loss: 2.534956, mae: 23.832329, mean_q: 30.093433\n",
      " 114469/500000: episode: 298, duration: 4.488s, episode steps: 1000, steps per second: 223, episode reward: -44.755, mean reward: -0.045 [-4.904,  4.393], mean action: 1.631 [0.000, 3.000],  loss: 4.300772, mae: 23.712988, mean_q: 30.152420\n",
      " 115469/500000: episode: 299, duration: 4.805s, episode steps: 1000, steps per second: 208, episode reward: -120.546, mean reward: -0.121 [-5.641,  4.080], mean action: 1.778 [0.000, 3.000],  loss: 5.615688, mae: 23.076544, mean_q: 29.375227\n",
      " 116469/500000: episode: 300, duration: 5.069s, episode steps: 1000, steps per second: 197, episode reward: -88.371, mean reward: -0.088 [-5.030,  4.177], mean action: 1.809 [0.000, 3.000],  loss: 2.564121, mae: 22.908703, mean_q: 29.681108\n",
      " 117469/500000: episode: 301, duration: 4.439s, episode steps: 1000, steps per second: 225, episode reward: -90.709, mean reward: -0.091 [-4.882,  4.250], mean action: 1.823 [0.000, 3.000],  loss: 2.406576, mae: 22.614840, mean_q: 29.413015\n",
      " 118469/500000: episode: 302, duration: 4.680s, episode steps: 1000, steps per second: 214, episode reward: -54.338, mean reward: -0.054 [-4.606,  4.525], mean action: 1.846 [0.000, 3.000],  loss: 2.056764, mae: 22.130423, mean_q: 28.919363\n",
      " 119469/500000: episode: 303, duration: 5.107s, episode steps: 1000, steps per second: 196, episode reward: -49.937, mean reward: -0.050 [-5.522,  4.302], mean action: 1.741 [0.000, 3.000],  loss: 3.643611, mae: 21.693722, mean_q: 28.413834\n",
      " 120469/500000: episode: 304, duration: 5.148s, episode steps: 1000, steps per second: 194, episode reward: -88.806, mean reward: -0.089 [-4.829,  4.034], mean action: 1.754 [0.000, 3.000],  loss: 2.311525, mae: 20.728998, mean_q: 27.274397\n",
      " 121469/500000: episode: 305, duration: 4.486s, episode steps: 1000, steps per second: 223, episode reward: -25.957, mean reward: -0.026 [-4.576,  4.397], mean action: 1.684 [0.000, 3.000],  loss: 1.342880, mae: 19.987093, mean_q: 26.237942\n",
      " 122469/500000: episode: 306, duration: 5.029s, episode steps: 1000, steps per second: 199, episode reward: -38.123, mean reward: -0.038 [-4.007,  4.358], mean action: 1.722 [0.000, 3.000],  loss: 1.387584, mae: 19.408566, mean_q: 25.265730\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 123469/500000: episode: 307, duration: 5.529s, episode steps: 1000, steps per second: 181, episode reward: -27.991, mean reward: -0.028 [-4.974,  4.893], mean action: 1.686 [0.000, 3.000],  loss: 2.113146, mae: 18.927168, mean_q: 24.485199\n",
      " 124469/500000: episode: 308, duration: 4.744s, episode steps: 1000, steps per second: 211, episode reward: -56.515, mean reward: -0.057 [-4.907,  3.985], mean action: 1.701 [0.000, 3.000],  loss: 1.546476, mae: 18.377102, mean_q: 23.680477\n",
      " 125469/500000: episode: 309, duration: 5.562s, episode steps: 1000, steps per second: 180, episode reward: -82.659, mean reward: -0.083 [-3.979,  3.781], mean action: 1.730 [0.000, 3.000],  loss: 1.598101, mae: 17.484318, mean_q: 22.918779\n",
      " 125736/500000: episode: 310, duration: 1.015s, episode steps: 267, steps per second: 263, episode reward: -163.828, mean reward: -0.614 [-100.000,  3.714], mean action: 1.854 [0.000, 3.000],  loss: 0.945899, mae: 17.095306, mean_q: 22.377094\n",
      " 126736/500000: episode: 311, duration: 4.618s, episode steps: 1000, steps per second: 217, episode reward: -30.591, mean reward: -0.031 [-4.309,  4.578], mean action: 1.431 [0.000, 3.000],  loss: 0.961015, mae: 16.832645, mean_q: 22.067644\n",
      " 127736/500000: episode: 312, duration: 4.958s, episode steps: 1000, steps per second: 202, episode reward: -14.975, mean reward: -0.015 [-4.657,  4.900], mean action: 1.560 [0.000, 3.000],  loss: 1.467137, mae: 16.141134, mean_q: 21.069321\n",
      " 128251/500000: episode: 313, duration: 2.003s, episode steps: 515, steps per second: 257, episode reward: -222.377, mean reward: -0.432 [-100.000, 29.232], mean action: 1.744 [0.000, 3.000],  loss: 2.055052, mae: 15.758595, mean_q: 20.655258\n",
      " 129251/500000: episode: 314, duration: 4.970s, episode steps: 1000, steps per second: 201, episode reward: -29.354, mean reward: -0.029 [-4.258,  4.901], mean action: 1.553 [0.000, 3.000],  loss: 1.364511, mae: 15.369536, mean_q: 20.071911\n",
      " 130251/500000: episode: 315, duration: 4.252s, episode steps: 1000, steps per second: 235, episode reward: 18.884, mean reward:  0.019 [-4.371,  5.522], mean action: 1.429 [0.000, 3.000],  loss: 1.252572, mae: 15.102194, mean_q: 19.783722\n",
      " 131251/500000: episode: 316, duration: 5.127s, episode steps: 1000, steps per second: 195, episode reward:  4.809, mean reward:  0.005 [-4.382,  4.436], mean action: 1.493 [0.000, 3.000],  loss: 1.363524, mae: 14.505004, mean_q: 19.051275\n",
      " 132251/500000: episode: 317, duration: 4.818s, episode steps: 1000, steps per second: 208, episode reward: -17.825, mean reward: -0.018 [-5.111,  4.423], mean action: 1.604 [0.000, 3.000],  loss: 1.167229, mae: 14.362664, mean_q: 18.762848\n",
      " 133251/500000: episode: 318, duration: 5.023s, episode steps: 1000, steps per second: 199, episode reward: -78.753, mean reward: -0.079 [-4.615,  4.403], mean action: 1.585 [0.000, 3.000],  loss: 1.016509, mae: 14.205308, mean_q: 18.688919\n",
      " 134251/500000: episode: 319, duration: 4.226s, episode steps: 1000, steps per second: 237, episode reward: -17.015, mean reward: -0.017 [-4.534,  5.213], mean action: 1.481 [0.000, 3.000],  loss: 0.930464, mae: 13.986010, mean_q: 18.295280\n",
      " 135251/500000: episode: 320, duration: 4.206s, episode steps: 1000, steps per second: 238, episode reward: -9.004, mean reward: -0.009 [-4.128,  4.190], mean action: 1.554 [0.000, 3.000],  loss: 0.900645, mae: 13.619441, mean_q: 17.819683\n",
      " 136251/500000: episode: 321, duration: 4.255s, episode steps: 1000, steps per second: 235, episode reward: -44.188, mean reward: -0.044 [-4.515,  4.971], mean action: 1.556 [0.000, 3.000],  loss: 1.021636, mae: 13.664990, mean_q: 17.882978\n",
      " 137251/500000: episode: 322, duration: 5.217s, episode steps: 1000, steps per second: 192, episode reward:  2.360, mean reward:  0.002 [-4.852,  4.169], mean action: 1.618 [0.000, 3.000],  loss: 1.368217, mae: 13.778114, mean_q: 17.921215\n",
      " 138251/500000: episode: 323, duration: 4.431s, episode steps: 1000, steps per second: 226, episode reward: -82.450, mean reward: -0.082 [-5.198,  4.327], mean action: 1.763 [0.000, 3.000],  loss: 1.231913, mae: 13.742892, mean_q: 17.796633\n",
      " 139251/500000: episode: 324, duration: 4.118s, episode steps: 1000, steps per second: 243, episode reward: -27.459, mean reward: -0.027 [-6.061,  5.228], mean action: 1.630 [0.000, 3.000],  loss: 1.611194, mae: 13.603859, mean_q: 17.762127\n",
      " 140251/500000: episode: 325, duration: 4.838s, episode steps: 1000, steps per second: 207, episode reward: -18.098, mean reward: -0.018 [-4.105,  4.161], mean action: 1.474 [0.000, 3.000],  loss: 0.829325, mae: 13.564680, mean_q: 17.847971\n",
      " 141251/500000: episode: 326, duration: 4.872s, episode steps: 1000, steps per second: 205, episode reward: -33.230, mean reward: -0.033 [-4.390,  4.114], mean action: 1.451 [0.000, 3.000],  loss: 0.791516, mae: 13.455946, mean_q: 17.732948\n",
      " 142251/500000: episode: 327, duration: 4.671s, episode steps: 1000, steps per second: 214, episode reward: -51.906, mean reward: -0.052 [-3.741,  4.933], mean action: 1.419 [0.000, 3.000],  loss: 0.724748, mae: 13.184643, mean_q: 17.277500\n",
      " 143251/500000: episode: 328, duration: 4.095s, episode steps: 1000, steps per second: 244, episode reward: -37.658, mean reward: -0.038 [-4.106,  4.686], mean action: 1.659 [0.000, 3.000],  loss: 0.917406, mae: 12.968187, mean_q: 16.902054\n",
      " 144251/500000: episode: 329, duration: 4.999s, episode steps: 1000, steps per second: 200, episode reward: -10.296, mean reward: -0.010 [-4.234,  4.523], mean action: 1.541 [0.000, 3.000],  loss: 1.182551, mae: 13.179188, mean_q: 17.139380\n",
      " 145251/500000: episode: 330, duration: 5.074s, episode steps: 1000, steps per second: 197, episode reward: -40.711, mean reward: -0.041 [-4.819,  5.124], mean action: 1.688 [0.000, 3.000],  loss: 1.015312, mae: 13.053895, mean_q: 16.993116\n",
      " 146251/500000: episode: 331, duration: 4.823s, episode steps: 1000, steps per second: 207, episode reward: -47.783, mean reward: -0.048 [-3.905,  4.305], mean action: 1.585 [0.000, 3.000],  loss: 0.732172, mae: 13.003782, mean_q: 16.879652\n",
      " 147251/500000: episode: 332, duration: 4.553s, episode steps: 1000, steps per second: 220, episode reward: -48.627, mean reward: -0.049 [-4.379,  4.217], mean action: 1.531 [0.000, 3.000],  loss: 0.883776, mae: 13.137967, mean_q: 16.945309\n",
      " 148038/500000: episode: 333, duration: 3.255s, episode steps: 787, steps per second: 242, episode reward: 66.984, mean reward:  0.085 [-5.133, 100.000], mean action: 1.618 [0.000, 3.000],  loss: 0.514788, mae: 13.163304, mean_q: 16.915075\n",
      " 148455/500000: episode: 334, duration: 1.661s, episode steps: 417, steps per second: 251, episode reward: -60.509, mean reward: -0.145 [-100.000,  9.405], mean action: 1.777 [0.000, 3.000],  loss: 1.942003, mae: 13.233619, mean_q: 16.927078\n",
      " 149455/500000: episode: 335, duration: 4.227s, episode steps: 1000, steps per second: 237, episode reward: -37.362, mean reward: -0.037 [-3.894,  4.290], mean action: 1.607 [0.000, 3.000],  loss: 0.830655, mae: 13.161685, mean_q: 16.801924\n",
      " 150455/500000: episode: 336, duration: 4.230s, episode steps: 1000, steps per second: 236, episode reward: -28.143, mean reward: -0.028 [-4.347,  5.482], mean action: 1.777 [0.000, 3.000],  loss: 0.607219, mae: 13.310221, mean_q: 16.973890\n",
      " 151455/500000: episode: 337, duration: 4.762s, episode steps: 1000, steps per second: 210, episode reward: -16.785, mean reward: -0.017 [-4.939,  4.449], mean action: 1.625 [0.000, 3.000],  loss: 1.465879, mae: 13.256814, mean_q: 16.859846\n",
      " 152455/500000: episode: 338, duration: 4.340s, episode steps: 1000, steps per second: 230, episode reward: -22.579, mean reward: -0.023 [-4.585,  4.417], mean action: 1.683 [0.000, 3.000],  loss: 1.142846, mae: 13.454750, mean_q: 17.066313\n",
      " 153455/500000: episode: 339, duration: 4.113s, episode steps: 1000, steps per second: 243, episode reward: -36.277, mean reward: -0.036 [-4.466,  4.334], mean action: 1.698 [0.000, 3.000],  loss: 2.343291, mae: 13.385299, mean_q: 16.952160\n",
      " 154455/500000: episode: 340, duration: 4.765s, episode steps: 1000, steps per second: 210, episode reward: -15.133, mean reward: -0.015 [-4.616,  3.885], mean action: 1.588 [0.000, 3.000],  loss: 1.111856, mae: 13.282156, mean_q: 16.907892\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 155014/500000: episode: 341, duration: 2.269s, episode steps: 559, steps per second: 246, episode reward: -103.900, mean reward: -0.186 [-100.000, 11.545], mean action: 1.623 [0.000, 3.000],  loss: 0.775510, mae: 13.090877, mean_q: 16.489843\n",
      " 156014/500000: episode: 342, duration: 4.126s, episode steps: 1000, steps per second: 242, episode reward: -11.454, mean reward: -0.011 [-3.421,  4.696], mean action: 1.795 [0.000, 3.000],  loss: 1.764187, mae: 13.130936, mean_q: 16.414310\n",
      " 156635/500000: episode: 343, duration: 2.460s, episode steps: 621, steps per second: 252, episode reward: -123.417, mean reward: -0.199 [-100.000, 21.067], mean action: 1.676 [0.000, 3.000],  loss: 0.661014, mae: 12.952446, mean_q: 16.114143\n",
      " 157252/500000: episode: 344, duration: 2.440s, episode steps: 617, steps per second: 253, episode reward: -187.548, mean reward: -0.304 [-100.000, 19.875], mean action: 1.767 [0.000, 3.000],  loss: 1.797441, mae: 12.800582, mean_q: 15.733581\n",
      " 158252/500000: episode: 345, duration: 4.226s, episode steps: 1000, steps per second: 237, episode reward: -20.165, mean reward: -0.020 [-4.608,  4.904], mean action: 1.566 [0.000, 3.000],  loss: 1.318080, mae: 13.007490, mean_q: 15.822993\n",
      " 159252/500000: episode: 346, duration: 4.405s, episode steps: 1000, steps per second: 227, episode reward: -13.063, mean reward: -0.013 [-4.194,  4.455], mean action: 1.499 [0.000, 3.000],  loss: 1.581343, mae: 12.678180, mean_q: 15.340326\n",
      " 160252/500000: episode: 347, duration: 4.715s, episode steps: 1000, steps per second: 212, episode reward: -71.261, mean reward: -0.071 [-6.520,  4.531], mean action: 1.634 [0.000, 3.000],  loss: 1.053267, mae: 12.531075, mean_q: 15.112863\n",
      " 161252/500000: episode: 348, duration: 4.955s, episode steps: 1000, steps per second: 202, episode reward: -25.561, mean reward: -0.026 [-5.562,  4.627], mean action: 1.578 [0.000, 3.000],  loss: 1.240280, mae: 12.098442, mean_q: 14.618791\n",
      " 162252/500000: episode: 349, duration: 4.700s, episode steps: 1000, steps per second: 213, episode reward: -51.854, mean reward: -0.052 [-4.403,  5.026], mean action: 1.560 [0.000, 3.000],  loss: 1.379491, mae: 11.975563, mean_q: 14.535225\n",
      " 163252/500000: episode: 350, duration: 4.826s, episode steps: 1000, steps per second: 207, episode reward: -3.995, mean reward: -0.004 [-4.341,  5.471], mean action: 1.593 [0.000, 3.000],  loss: 1.154410, mae: 11.729937, mean_q: 14.083189\n",
      " 164252/500000: episode: 351, duration: 4.966s, episode steps: 1000, steps per second: 201, episode reward: -44.576, mean reward: -0.045 [-4.771,  4.653], mean action: 1.622 [0.000, 3.000],  loss: 1.029440, mae: 11.663606, mean_q: 14.037371\n",
      " 164603/500000: episode: 352, duration: 1.361s, episode steps: 351, steps per second: 258, episode reward: -211.709, mean reward: -0.603 [-100.000, 16.641], mean action: 1.963 [0.000, 3.000],  loss: 1.280653, mae: 11.747285, mean_q: 14.093165\n",
      " 164877/500000: episode: 353, duration: 1.037s, episode steps: 274, steps per second: 264, episode reward: -121.562, mean reward: -0.444 [-100.000, 18.247], mean action: 1.894 [0.000, 3.000],  loss: 0.866251, mae: 11.790196, mean_q: 14.284328\n",
      " 165565/500000: episode: 354, duration: 2.649s, episode steps: 688, steps per second: 260, episode reward: -74.543, mean reward: -0.108 [-100.000, 12.592], mean action: 1.721 [0.000, 3.000],  loss: 0.997589, mae: 11.988589, mean_q: 14.436573\n",
      " 166565/500000: episode: 355, duration: 4.053s, episode steps: 1000, steps per second: 247, episode reward: -4.783, mean reward: -0.005 [-11.996, 12.775], mean action: 1.644 [0.000, 3.000],  loss: 1.608012, mae: 11.729086, mean_q: 13.960996\n",
      " 167522/500000: episode: 356, duration: 4.774s, episode steps: 957, steps per second: 200, episode reward: -474.419, mean reward: -0.496 [-100.000, 17.225], mean action: 1.787 [0.000, 3.000],  loss: 2.050299, mae: 11.684328, mean_q: 13.740933\n",
      " 167891/500000: episode: 357, duration: 1.477s, episode steps: 369, steps per second: 250, episode reward: -157.467, mean reward: -0.427 [-100.000, 16.854], mean action: 1.900 [0.000, 3.000],  loss: 2.002518, mae: 11.834651, mean_q: 13.573207\n",
      " 168215/500000: episode: 358, duration: 1.294s, episode steps: 324, steps per second: 250, episode reward: -208.003, mean reward: -0.642 [-100.000, 25.014], mean action: 1.910 [0.000, 3.000],  loss: 3.281832, mae: 12.032781, mean_q: 14.013480\n",
      " 168759/500000: episode: 359, duration: 2.273s, episode steps: 544, steps per second: 239, episode reward: -207.424, mean reward: -0.381 [-100.000, 13.991], mean action: 1.842 [0.000, 3.000],  loss: 3.171024, mae: 11.982337, mean_q: 14.188102\n",
      " 169759/500000: episode: 360, duration: 4.864s, episode steps: 1000, steps per second: 206, episode reward: -52.011, mean reward: -0.052 [-4.818,  5.489], mean action: 1.641 [0.000, 3.000],  loss: 2.484925, mae: 12.074843, mean_q: 14.305490\n",
      " 170759/500000: episode: 361, duration: 4.232s, episode steps: 1000, steps per second: 236, episode reward: -67.619, mean reward: -0.068 [-4.902,  6.027], mean action: 1.775 [0.000, 3.000],  loss: 1.874780, mae: 12.036412, mean_q: 14.435978\n",
      " 171759/500000: episode: 362, duration: 4.578s, episode steps: 1000, steps per second: 218, episode reward: -95.526, mean reward: -0.096 [-5.207,  4.663], mean action: 1.716 [0.000, 3.000],  loss: 2.383563, mae: 11.827512, mean_q: 14.333675\n",
      " 172759/500000: episode: 363, duration: 5.280s, episode steps: 1000, steps per second: 189, episode reward: -64.678, mean reward: -0.065 [-6.002,  4.036], mean action: 1.702 [0.000, 3.000],  loss: 1.644607, mae: 12.031398, mean_q: 14.875873\n",
      " 173759/500000: episode: 364, duration: 4.574s, episode steps: 1000, steps per second: 219, episode reward: -31.573, mean reward: -0.032 [-3.916,  4.448], mean action: 1.631 [0.000, 3.000],  loss: 1.981597, mae: 11.932713, mean_q: 14.496479\n",
      " 174733/500000: episode: 365, duration: 4.890s, episode steps: 974, steps per second: 199, episode reward: -126.333, mean reward: -0.130 [-100.000, 14.016], mean action: 1.621 [0.000, 3.000],  loss: 2.109111, mae: 12.032567, mean_q: 14.438282\n",
      " 175733/500000: episode: 366, duration: 4.257s, episode steps: 1000, steps per second: 235, episode reward:  5.787, mean reward:  0.006 [-2.923,  5.996], mean action: 1.553 [0.000, 3.000],  loss: 2.098077, mae: 12.253460, mean_q: 14.707834\n",
      " 176733/500000: episode: 367, duration: 4.067s, episode steps: 1000, steps per second: 246, episode reward: -5.683, mean reward: -0.006 [-4.417,  4.541], mean action: 1.633 [0.000, 3.000],  loss: 1.479713, mae: 12.201822, mean_q: 14.559183\n",
      " 177733/500000: episode: 368, duration: 4.083s, episode steps: 1000, steps per second: 245, episode reward: -53.894, mean reward: -0.054 [-21.538, 22.162], mean action: 1.639 [0.000, 3.000],  loss: 1.999791, mae: 12.164382, mean_q: 14.507915\n",
      " 178733/500000: episode: 369, duration: 4.365s, episode steps: 1000, steps per second: 229, episode reward: -22.652, mean reward: -0.023 [-4.432,  5.461], mean action: 1.530 [0.000, 3.000],  loss: 2.313052, mae: 12.125076, mean_q: 14.521640\n",
      " 179733/500000: episode: 370, duration: 4.877s, episode steps: 1000, steps per second: 205, episode reward: -18.046, mean reward: -0.018 [-4.184,  4.182], mean action: 1.560 [0.000, 3.000],  loss: 2.034064, mae: 12.305428, mean_q: 14.825521\n",
      " 180733/500000: episode: 371, duration: 5.199s, episode steps: 1000, steps per second: 192, episode reward: -62.733, mean reward: -0.063 [-4.162,  5.293], mean action: 1.651 [0.000, 3.000],  loss: 1.842474, mae: 12.211332, mean_q: 14.811277\n",
      " 181733/500000: episode: 372, duration: 4.880s, episode steps: 1000, steps per second: 205, episode reward: -52.153, mean reward: -0.052 [-4.827,  4.595], mean action: 1.708 [0.000, 3.000],  loss: 2.251232, mae: 12.330132, mean_q: 15.041789\n",
      " 182733/500000: episode: 373, duration: 5.271s, episode steps: 1000, steps per second: 190, episode reward: -67.137, mean reward: -0.067 [-4.396,  4.621], mean action: 1.604 [0.000, 3.000],  loss: 1.819175, mae: 12.452253, mean_q: 15.103457\n",
      " 183733/500000: episode: 374, duration: 4.803s, episode steps: 1000, steps per second: 208, episode reward: -29.400, mean reward: -0.029 [-4.750,  4.347], mean action: 1.494 [0.000, 3.000],  loss: 1.493341, mae: 12.564581, mean_q: 14.988150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 184733/500000: episode: 375, duration: 4.496s, episode steps: 1000, steps per second: 222, episode reward: -54.383, mean reward: -0.054 [-4.686,  5.390], mean action: 1.567 [0.000, 3.000],  loss: 1.672996, mae: 12.608762, mean_q: 14.808332\n",
      " 185433/500000: episode: 376, duration: 3.280s, episode steps: 700, steps per second: 213, episode reward: -131.118, mean reward: -0.187 [-100.000, 12.088], mean action: 1.643 [0.000, 3.000],  loss: 2.291059, mae: 12.603600, mean_q: 14.849350\n",
      " 186433/500000: episode: 377, duration: 4.929s, episode steps: 1000, steps per second: 203, episode reward: -25.034, mean reward: -0.025 [-4.462,  4.778], mean action: 1.482 [0.000, 3.000],  loss: 1.968692, mae: 12.676223, mean_q: 15.118955\n",
      " 187433/500000: episode: 378, duration: 4.894s, episode steps: 1000, steps per second: 204, episode reward: -66.126, mean reward: -0.066 [-4.394,  5.250], mean action: 1.478 [0.000, 3.000],  loss: 1.278437, mae: 12.684498, mean_q: 15.065818\n",
      " 187588/500000: episode: 379, duration: 0.588s, episode steps: 155, steps per second: 264, episode reward: -241.020, mean reward: -1.555 [-100.000,  4.026], mean action: 1.697 [0.000, 3.000],  loss: 2.914724, mae: 12.812850, mean_q: 14.925385\n",
      " 188588/500000: episode: 380, duration: 4.589s, episode steps: 1000, steps per second: 218, episode reward: -53.146, mean reward: -0.053 [-5.594,  4.783], mean action: 1.557 [0.000, 3.000],  loss: 1.891772, mae: 12.698492, mean_q: 14.930822\n",
      " 189317/500000: episode: 381, duration: 3.331s, episode steps: 729, steps per second: 219, episode reward: -130.785, mean reward: -0.179 [-100.000, 11.288], mean action: 1.635 [0.000, 3.000],  loss: 3.018483, mae: 12.706753, mean_q: 14.652265\n",
      " 190317/500000: episode: 382, duration: 4.675s, episode steps: 1000, steps per second: 214, episode reward: -86.854, mean reward: -0.087 [-4.647,  4.656], mean action: 1.575 [0.000, 3.000],  loss: 2.245795, mae: 12.666187, mean_q: 14.621770\n",
      " 190466/500000: episode: 383, duration: 0.547s, episode steps: 149, steps per second: 273, episode reward: -378.868, mean reward: -2.543 [-100.000, 87.526], mean action: 1.899 [0.000, 3.000],  loss: 1.262079, mae: 12.974671, mean_q: 14.648051\n",
      " 191466/500000: episode: 384, duration: 4.226s, episode steps: 1000, steps per second: 237, episode reward: -32.585, mean reward: -0.033 [-17.811, 13.452], mean action: 1.548 [0.000, 3.000],  loss: 2.406891, mae: 12.855277, mean_q: 14.401004\n",
      " 192466/500000: episode: 385, duration: 4.712s, episode steps: 1000, steps per second: 212, episode reward: -30.516, mean reward: -0.031 [-6.577,  4.865], mean action: 1.703 [0.000, 3.000],  loss: 2.462267, mae: 13.321676, mean_q: 14.841812\n",
      " 193171/500000: episode: 386, duration: 3.155s, episode steps: 705, steps per second: 223, episode reward: -162.248, mean reward: -0.230 [-100.000, 12.628], mean action: 1.679 [0.000, 3.000],  loss: 2.176620, mae: 13.707052, mean_q: 15.403036\n",
      " 193439/500000: episode: 387, duration: 1.037s, episode steps: 268, steps per second: 259, episode reward: -276.187, mean reward: -1.031 [-100.000,  6.193], mean action: 1.866 [0.000, 3.000],  loss: 3.117088, mae: 13.942643, mean_q: 15.018495\n",
      " 193627/500000: episode: 388, duration: 0.686s, episode steps: 188, steps per second: 274, episode reward: -531.639, mean reward: -2.828 [-100.000,  5.936], mean action: 1.676 [0.000, 3.000],  loss: 2.331280, mae: 14.468688, mean_q: 16.027086\n",
      " 193830/500000: episode: 389, duration: 0.790s, episode steps: 203, steps per second: 257, episode reward: -69.149, mean reward: -0.341 [-100.000, 19.516], mean action: 2.000 [0.000, 3.000],  loss: 3.981024, mae: 14.848734, mean_q: 16.349825\n",
      " 193960/500000: episode: 390, duration: 0.478s, episode steps: 130, steps per second: 272, episode reward: -370.214, mean reward: -2.848 [-100.000,  4.274], mean action: 1.915 [0.000, 3.000],  loss: 3.771581, mae: 14.672945, mean_q: 15.617685\n",
      " 194182/500000: episode: 391, duration: 0.833s, episode steps: 222, steps per second: 266, episode reward: -295.335, mean reward: -1.330 [-100.000, 32.168], mean action: 1.869 [0.000, 3.000],  loss: 3.669468, mae: 15.273497, mean_q: 16.169765\n",
      " 194318/500000: episode: 392, duration: 0.527s, episode steps: 136, steps per second: 258, episode reward: -192.620, mean reward: -1.416 [-100.000,  3.713], mean action: 1.779 [0.000, 3.000],  loss: 31.825151, mae: 16.013313, mean_q: 17.676069\n",
      " 194711/500000: episode: 393, duration: 1.577s, episode steps: 393, steps per second: 249, episode reward: -54.248, mean reward: -0.138 [-100.000, 19.500], mean action: 1.840 [0.000, 3.000],  loss: 8.937061, mae: 16.267962, mean_q: 19.198404\n",
      " 195711/500000: episode: 394, duration: 4.526s, episode steps: 1000, steps per second: 221, episode reward: -61.939, mean reward: -0.062 [-4.919,  4.760], mean action: 1.532 [0.000, 3.000],  loss: 4.629042, mae: 16.135626, mean_q: 18.949778\n",
      " 196387/500000: episode: 395, duration: 2.850s, episode steps: 676, steps per second: 237, episode reward: -87.774, mean reward: -0.130 [-100.000, 10.442], mean action: 1.590 [0.000, 3.000],  loss: 6.545822, mae: 16.353004, mean_q: 18.673723\n",
      " 197387/500000: episode: 396, duration: 4.439s, episode steps: 1000, steps per second: 225, episode reward: -75.878, mean reward: -0.076 [-20.232, 22.517], mean action: 1.566 [0.000, 3.000],  loss: 5.358248, mae: 16.324476, mean_q: 19.065119\n",
      " 198174/500000: episode: 397, duration: 3.396s, episode steps: 787, steps per second: 232, episode reward: -180.446, mean reward: -0.229 [-100.000, 15.196], mean action: 1.675 [0.000, 3.000],  loss: 6.991849, mae: 16.524626, mean_q: 19.458031\n",
      " 199174/500000: episode: 398, duration: 4.709s, episode steps: 1000, steps per second: 212, episode reward: -33.536, mean reward: -0.034 [-4.751,  5.528], mean action: 1.637 [0.000, 3.000],  loss: 4.132618, mae: 16.710865, mean_q: 19.977890\n",
      " 200174/500000: episode: 399, duration: 4.723s, episode steps: 1000, steps per second: 212, episode reward: -53.872, mean reward: -0.054 [-5.580,  4.820], mean action: 1.629 [0.000, 3.000],  loss: 4.042010, mae: 16.531166, mean_q: 19.965019\n",
      " 201174/500000: episode: 400, duration: 5.057s, episode steps: 1000, steps per second: 198, episode reward: -33.292, mean reward: -0.033 [-5.388,  6.649], mean action: 1.654 [0.000, 3.000],  loss: 3.102325, mae: 16.990803, mean_q: 20.290546\n",
      " 202174/500000: episode: 401, duration: 5.062s, episode steps: 1000, steps per second: 198, episode reward: -38.436, mean reward: -0.038 [-4.952,  5.211], mean action: 1.615 [0.000, 3.000],  loss: 3.455524, mae: 17.163429, mean_q: 20.952477\n",
      " 203174/500000: episode: 402, duration: 4.267s, episode steps: 1000, steps per second: 234, episode reward: -10.538, mean reward: -0.011 [-11.666, 13.151], mean action: 1.787 [0.000, 3.000],  loss: 3.909375, mae: 17.295645, mean_q: 21.220758\n",
      " 204174/500000: episode: 403, duration: 4.970s, episode steps: 1000, steps per second: 201, episode reward: -38.331, mean reward: -0.038 [-5.687,  5.011], mean action: 1.743 [0.000, 3.000],  loss: 3.862752, mae: 17.477051, mean_q: 21.643209\n",
      " 205174/500000: episode: 404, duration: 4.329s, episode steps: 1000, steps per second: 231, episode reward: -29.583, mean reward: -0.030 [-5.300,  7.066], mean action: 1.772 [0.000, 3.000],  loss: 3.456790, mae: 17.588217, mean_q: 22.061287\n",
      " 206174/500000: episode: 405, duration: 4.623s, episode steps: 1000, steps per second: 216, episode reward: -63.756, mean reward: -0.064 [-15.028, 10.200], mean action: 1.569 [0.000, 3.000],  loss: 2.825151, mae: 17.531935, mean_q: 22.172598\n",
      " 207174/500000: episode: 406, duration: 4.425s, episode steps: 1000, steps per second: 226, episode reward:  5.393, mean reward:  0.005 [-4.133,  6.479], mean action: 1.529 [0.000, 3.000],  loss: 3.233241, mae: 17.760113, mean_q: 22.530796\n",
      " 208174/500000: episode: 407, duration: 4.088s, episode steps: 1000, steps per second: 245, episode reward: -27.641, mean reward: -0.028 [-4.514,  4.792], mean action: 1.585 [0.000, 3.000],  loss: 3.407046, mae: 17.376955, mean_q: 22.225943\n",
      " 209174/500000: episode: 408, duration: 4.045s, episode steps: 1000, steps per second: 247, episode reward: -18.861, mean reward: -0.019 [-4.702,  4.747], mean action: 1.469 [0.000, 3.000],  loss: 2.880952, mae: 17.366556, mean_q: 22.106585\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 210174/500000: episode: 409, duration: 4.026s, episode steps: 1000, steps per second: 248, episode reward: -1.295, mean reward: -0.001 [-5.287,  5.045], mean action: 1.535 [0.000, 3.000],  loss: 3.076172, mae: 17.293058, mean_q: 21.945524\n",
      " 211174/500000: episode: 410, duration: 4.239s, episode steps: 1000, steps per second: 236, episode reward: -43.082, mean reward: -0.043 [-4.731,  4.522], mean action: 1.537 [0.000, 3.000],  loss: 5.620227, mae: 17.235342, mean_q: 21.706322\n",
      " 212174/500000: episode: 411, duration: 4.192s, episode steps: 1000, steps per second: 239, episode reward: -49.194, mean reward: -0.049 [-18.405, 11.404], mean action: 1.398 [0.000, 3.000],  loss: 2.793849, mae: 17.049244, mean_q: 21.670004\n",
      " 212902/500000: episode: 412, duration: 3.124s, episode steps: 728, steps per second: 233, episode reward: 127.934, mean reward:  0.176 [-4.540, 100.000], mean action: 1.275 [0.000, 3.000],  loss: 3.264437, mae: 17.057383, mean_q: 21.365499\n",
      " 213486/500000: episode: 413, duration: 2.406s, episode steps: 584, steps per second: 243, episode reward: 185.270, mean reward:  0.317 [-15.241, 100.000], mean action: 1.221 [0.000, 3.000],  loss: 4.906242, mae: 17.181822, mean_q: 21.361801\n",
      " 214208/500000: episode: 414, duration: 3.286s, episode steps: 722, steps per second: 220, episode reward: 144.773, mean reward:  0.201 [-19.077, 100.000], mean action: 1.073 [0.000, 3.000],  loss: 4.851546, mae: 17.415993, mean_q: 21.555063\n",
      " 215208/500000: episode: 415, duration: 4.622s, episode steps: 1000, steps per second: 216, episode reward: 53.008, mean reward:  0.053 [-20.036, 19.834], mean action: 1.064 [0.000, 3.000],  loss: 3.503655, mae: 17.248800, mean_q: 21.761127\n",
      " 216208/500000: episode: 416, duration: 5.013s, episode steps: 1000, steps per second: 199, episode reward: -49.968, mean reward: -0.050 [-4.569,  5.234], mean action: 1.517 [0.000, 3.000],  loss: 4.386085, mae: 16.841545, mean_q: 21.370073\n",
      " 217208/500000: episode: 417, duration: 4.836s, episode steps: 1000, steps per second: 207, episode reward: -14.603, mean reward: -0.015 [-12.933, 14.112], mean action: 1.518 [0.000, 3.000],  loss: 2.922628, mae: 16.831669, mean_q: 21.385614\n",
      " 218208/500000: episode: 418, duration: 4.777s, episode steps: 1000, steps per second: 209, episode reward: -10.956, mean reward: -0.011 [-19.092, 18.578], mean action: 1.326 [0.000, 3.000],  loss: 3.977583, mae: 16.785517, mean_q: 21.487907\n",
      " 219208/500000: episode: 419, duration: 6.253s, episode steps: 1000, steps per second: 160, episode reward:  4.816, mean reward:  0.005 [-4.657,  5.722], mean action: 1.588 [0.000, 3.000],  loss: 2.827738, mae: 16.190378, mean_q: 20.863348\n",
      " 220208/500000: episode: 420, duration: 6.926s, episode steps: 1000, steps per second: 144, episode reward:  2.995, mean reward:  0.003 [-19.393, 16.695], mean action: 1.485 [0.000, 3.000],  loss: 3.333814, mae: 15.790201, mean_q: 20.396271\n",
      " 221208/500000: episode: 421, duration: 6.348s, episode steps: 1000, steps per second: 158, episode reward: -33.760, mean reward: -0.034 [-11.628, 10.511], mean action: 1.521 [0.000, 3.000],  loss: 2.867905, mae: 15.149696, mean_q: 19.422243\n",
      " 221929/500000: episode: 422, duration: 4.612s, episode steps: 721, steps per second: 156, episode reward: 145.607, mean reward:  0.202 [-3.795, 100.000], mean action: 1.451 [0.000, 3.000],  loss: 3.396957, mae: 15.034728, mean_q: 19.010813\n",
      " 222929/500000: episode: 423, duration: 6.132s, episode steps: 1000, steps per second: 163, episode reward: -57.899, mean reward: -0.058 [-4.477,  4.964], mean action: 1.463 [0.000, 3.000],  loss: 2.940763, mae: 14.380095, mean_q: 17.840847\n",
      " 223176/500000: episode: 424, duration: 1.313s, episode steps: 247, steps per second: 188, episode reward: -70.572, mean reward: -0.286 [-100.000,  4.108], mean action: 1.713 [0.000, 3.000],  loss: 2.425021, mae: 14.154378, mean_q: 17.306471\n",
      " 224176/500000: episode: 425, duration: 7.136s, episode steps: 1000, steps per second: 140, episode reward: -50.766, mean reward: -0.051 [-11.559, 16.927], mean action: 1.515 [0.000, 3.000],  loss: 5.384486, mae: 14.395674, mean_q: 17.778027\n",
      " 224685/500000: episode: 426, duration: 3.060s, episode steps: 509, steps per second: 166, episode reward: -137.435, mean reward: -0.270 [-100.000,  6.119], mean action: 1.627 [0.000, 3.000],  loss: 14.850368, mae: 14.861303, mean_q: 17.992317\n",
      " 225685/500000: episode: 427, duration: 6.746s, episode steps: 1000, steps per second: 148, episode reward: -56.126, mean reward: -0.056 [-3.907,  4.656], mean action: 1.527 [0.000, 3.000],  loss: 5.313658, mae: 14.337318, mean_q: 18.186907\n",
      " 226685/500000: episode: 428, duration: 6.286s, episode steps: 1000, steps per second: 159, episode reward: -27.849, mean reward: -0.028 [-4.374,  5.031], mean action: 1.522 [0.000, 3.000],  loss: 3.197361, mae: 14.334242, mean_q: 18.121048\n",
      " 227685/500000: episode: 429, duration: 5.717s, episode steps: 1000, steps per second: 175, episode reward: -35.917, mean reward: -0.036 [-4.425,  4.726], mean action: 1.508 [0.000, 3.000],  loss: 4.646201, mae: 14.254346, mean_q: 17.699789\n",
      " 228685/500000: episode: 430, duration: 5.275s, episode steps: 1000, steps per second: 190, episode reward: -69.858, mean reward: -0.070 [-13.228, 16.410], mean action: 1.760 [0.000, 3.000],  loss: 6.266815, mae: 14.514872, mean_q: 18.033676\n",
      " 229685/500000: episode: 431, duration: 4.624s, episode steps: 1000, steps per second: 216, episode reward: -69.800, mean reward: -0.070 [-3.456,  4.416], mean action: 1.540 [0.000, 3.000],  loss: 7.866243, mae: 14.577385, mean_q: 18.131592\n",
      " 230685/500000: episode: 432, duration: 4.348s, episode steps: 1000, steps per second: 230, episode reward: -54.894, mean reward: -0.055 [-4.749,  4.889], mean action: 1.548 [0.000, 3.000],  loss: 4.116269, mae: 14.881639, mean_q: 19.243076\n",
      " 231685/500000: episode: 433, duration: 4.530s, episode steps: 1000, steps per second: 221, episode reward: -3.477, mean reward: -0.003 [-5.657,  7.048], mean action: 1.575 [0.000, 3.000],  loss: 5.411644, mae: 15.002975, mean_q: 19.538321\n",
      " 232685/500000: episode: 434, duration: 4.568s, episode steps: 1000, steps per second: 219, episode reward: -17.343, mean reward: -0.017 [-4.561,  4.738], mean action: 1.514 [0.000, 3.000],  loss: 2.897780, mae: 15.130142, mean_q: 19.768599\n",
      " 233685/500000: episode: 435, duration: 4.527s, episode steps: 1000, steps per second: 221, episode reward: -40.408, mean reward: -0.040 [-4.181,  6.102], mean action: 1.628 [0.000, 3.000],  loss: 2.674438, mae: 15.129424, mean_q: 19.912949\n",
      " 234685/500000: episode: 436, duration: 4.356s, episode steps: 1000, steps per second: 230, episode reward: -48.693, mean reward: -0.049 [-4.904,  4.588], mean action: 1.571 [0.000, 3.000],  loss: 2.599735, mae: 14.880164, mean_q: 19.483856\n",
      " 235685/500000: episode: 437, duration: 4.990s, episode steps: 1000, steps per second: 200, episode reward: -23.660, mean reward: -0.024 [-19.939, 13.933], mean action: 1.715 [0.000, 3.000],  loss: 3.813958, mae: 14.892412, mean_q: 19.423687\n",
      " 236053/500000: episode: 438, duration: 1.422s, episode steps: 368, steps per second: 259, episode reward: 152.156, mean reward:  0.413 [-10.057, 100.000], mean action: 1.005 [0.000, 3.000],  loss: 3.888836, mae: 14.762372, mean_q: 19.011311\n",
      " 237053/500000: episode: 439, duration: 4.745s, episode steps: 1000, steps per second: 211, episode reward: -31.323, mean reward: -0.031 [-4.371,  5.559], mean action: 1.615 [0.000, 3.000],  loss: 3.146652, mae: 14.989457, mean_q: 19.440876\n",
      " 238053/500000: episode: 440, duration: 4.079s, episode steps: 1000, steps per second: 245, episode reward: -75.825, mean reward: -0.076 [-4.638,  4.334], mean action: 1.628 [0.000, 3.000],  loss: 3.909301, mae: 15.197849, mean_q: 19.524384\n",
      " 239053/500000: episode: 441, duration: 4.361s, episode steps: 1000, steps per second: 229, episode reward: -31.892, mean reward: -0.032 [-18.655, 28.228], mean action: 1.625 [0.000, 3.000],  loss: 5.248475, mae: 15.145077, mean_q: 19.617128\n",
      " 240053/500000: episode: 442, duration: 4.406s, episode steps: 1000, steps per second: 227, episode reward:  2.219, mean reward:  0.002 [-5.047,  4.903], mean action: 1.649 [0.000, 3.000],  loss: 2.624224, mae: 15.496718, mean_q: 20.021797\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 240768/500000: episode: 443, duration: 3.218s, episode steps: 715, steps per second: 222, episode reward: 160.686, mean reward:  0.225 [-13.089, 100.000], mean action: 1.358 [0.000, 3.000],  loss: 3.826142, mae: 15.589561, mean_q: 20.106886\n",
      " 241768/500000: episode: 444, duration: 5.612s, episode steps: 1000, steps per second: 178, episode reward: -12.065, mean reward: -0.012 [-4.803,  5.155], mean action: 1.585 [0.000, 3.000],  loss: 2.544282, mae: 15.563592, mean_q: 20.224792\n",
      " 242768/500000: episode: 445, duration: 4.841s, episode steps: 1000, steps per second: 207, episode reward: -1.097, mean reward: -0.001 [-4.717,  5.453], mean action: 1.642 [0.000, 3.000],  loss: 3.046918, mae: 15.183323, mean_q: 19.837259\n",
      " 243768/500000: episode: 446, duration: 4.518s, episode steps: 1000, steps per second: 221, episode reward: -79.542, mean reward: -0.080 [-5.132,  4.837], mean action: 1.693 [0.000, 3.000],  loss: 4.003587, mae: 14.866347, mean_q: 19.398449\n",
      " 244768/500000: episode: 447, duration: 4.332s, episode steps: 1000, steps per second: 231, episode reward: -58.193, mean reward: -0.058 [-4.582,  5.037], mean action: 1.480 [0.000, 3.000],  loss: 2.101102, mae: 13.968946, mean_q: 18.421825\n",
      " 245768/500000: episode: 448, duration: 4.284s, episode steps: 1000, steps per second: 233, episode reward: -26.391, mean reward: -0.026 [-4.602,  5.596], mean action: 1.546 [0.000, 3.000],  loss: 2.404869, mae: 13.352058, mean_q: 17.596333\n",
      " 246768/500000: episode: 449, duration: 4.945s, episode steps: 1000, steps per second: 202, episode reward: -61.688, mean reward: -0.062 [-4.384,  4.512], mean action: 1.593 [0.000, 3.000],  loss: 1.643654, mae: 13.179067, mean_q: 17.507711\n",
      " 247768/500000: episode: 450, duration: 4.680s, episode steps: 1000, steps per second: 214, episode reward: -20.415, mean reward: -0.020 [-4.540,  5.555], mean action: 1.603 [0.000, 3.000],  loss: 1.604990, mae: 13.098609, mean_q: 17.517883\n",
      " 248768/500000: episode: 451, duration: 4.813s, episode steps: 1000, steps per second: 208, episode reward: -45.874, mean reward: -0.046 [-4.510,  4.368], mean action: 1.686 [0.000, 3.000],  loss: 1.715882, mae: 12.540183, mean_q: 16.770283\n",
      " 249768/500000: episode: 452, duration: 5.004s, episode steps: 1000, steps per second: 200, episode reward:  7.446, mean reward:  0.007 [-17.424, 23.645], mean action: 1.489 [0.000, 3.000],  loss: 1.843714, mae: 12.268230, mean_q: 16.418640\n",
      " 250768/500000: episode: 453, duration: 4.608s, episode steps: 1000, steps per second: 217, episode reward: -61.474, mean reward: -0.061 [-4.431,  4.835], mean action: 1.639 [0.000, 3.000],  loss: 1.997100, mae: 11.996850, mean_q: 16.078770\n",
      " 251768/500000: episode: 454, duration: 5.031s, episode steps: 1000, steps per second: 199, episode reward: -31.850, mean reward: -0.032 [-5.321,  4.418], mean action: 1.714 [0.000, 3.000],  loss: 2.172986, mae: 11.775909, mean_q: 15.820301\n",
      " 252768/500000: episode: 455, duration: 4.504s, episode steps: 1000, steps per second: 222, episode reward: -18.228, mean reward: -0.018 [-5.223,  6.177], mean action: 1.612 [0.000, 3.000],  loss: 1.518363, mae: 11.425879, mean_q: 15.358135\n",
      " 253768/500000: episode: 456, duration: 4.833s, episode steps: 1000, steps per second: 207, episode reward: -23.057, mean reward: -0.023 [-4.773,  6.398], mean action: 1.704 [0.000, 3.000],  loss: 1.174823, mae: 11.324648, mean_q: 15.234296\n",
      " 254768/500000: episode: 457, duration: 4.590s, episode steps: 1000, steps per second: 218, episode reward: -73.654, mean reward: -0.074 [-4.363,  4.704], mean action: 1.785 [0.000, 3.000],  loss: 1.711694, mae: 11.025240, mean_q: 14.810317\n",
      " 255768/500000: episode: 458, duration: 4.286s, episode steps: 1000, steps per second: 233, episode reward: 53.633, mean reward:  0.054 [-19.500, 13.144], mean action: 1.371 [0.000, 3.000],  loss: 1.689267, mae: 11.159394, mean_q: 15.015444\n",
      " 256768/500000: episode: 459, duration: 4.158s, episode steps: 1000, steps per second: 240, episode reward: -100.477, mean reward: -0.100 [-14.558, 16.231], mean action: 1.832 [0.000, 3.000],  loss: 1.383566, mae: 11.298838, mean_q: 15.252639\n",
      " 257768/500000: episode: 460, duration: 4.575s, episode steps: 1000, steps per second: 219, episode reward: -2.661, mean reward: -0.003 [-3.731,  4.645], mean action: 1.644 [0.000, 3.000],  loss: 1.377622, mae: 11.578736, mean_q: 15.682559\n",
      " 258768/500000: episode: 461, duration: 5.060s, episode steps: 1000, steps per second: 198, episode reward: -55.530, mean reward: -0.056 [-4.827,  4.440], mean action: 1.749 [0.000, 3.000],  loss: 1.700596, mae: 11.737562, mean_q: 15.927690\n",
      " 259768/500000: episode: 462, duration: 5.100s, episode steps: 1000, steps per second: 196, episode reward: -52.879, mean reward: -0.053 [-4.400,  5.012], mean action: 1.780 [0.000, 3.000],  loss: 1.381031, mae: 11.799781, mean_q: 16.010777\n",
      " 260768/500000: episode: 463, duration: 4.444s, episode steps: 1000, steps per second: 225, episode reward: -46.581, mean reward: -0.047 [-5.540,  4.569], mean action: 1.740 [0.000, 3.000],  loss: 2.331889, mae: 12.079609, mean_q: 16.346727\n",
      " 261768/500000: episode: 464, duration: 5.050s, episode steps: 1000, steps per second: 198, episode reward:  9.983, mean reward:  0.010 [-3.396,  4.370], mean action: 1.647 [0.000, 3.000],  loss: 1.813365, mae: 12.333844, mean_q: 16.708883\n",
      " 262768/500000: episode: 465, duration: 4.597s, episode steps: 1000, steps per second: 218, episode reward: -8.392, mean reward: -0.008 [-4.397,  5.714], mean action: 1.725 [0.000, 3.000],  loss: 1.574586, mae: 12.353870, mean_q: 16.721806\n",
      " 263768/500000: episode: 466, duration: 4.353s, episode steps: 1000, steps per second: 230, episode reward: 12.102, mean reward:  0.012 [-3.669,  5.199], mean action: 1.716 [0.000, 3.000],  loss: 1.348177, mae: 12.352727, mean_q: 16.690823\n",
      " 264768/500000: episode: 467, duration: 4.506s, episode steps: 1000, steps per second: 222, episode reward: 22.939, mean reward:  0.023 [-3.634,  5.384], mean action: 1.716 [0.000, 3.000],  loss: 1.349307, mae: 12.160594, mean_q: 16.389681\n",
      " 265765/500000: episode: 468, duration: 4.931s, episode steps: 997, steps per second: 202, episode reward: 147.916, mean reward:  0.148 [-18.084, 100.000], mean action: 1.557 [0.000, 3.000],  loss: 1.101548, mae: 11.915536, mean_q: 16.001688\n",
      " 266555/500000: episode: 469, duration: 3.275s, episode steps: 790, steps per second: 241, episode reward: -87.117, mean reward: -0.110 [-100.000,  9.145], mean action: 1.728 [0.000, 3.000],  loss: 1.423961, mae: 12.091584, mean_q: 16.195427\n",
      " 267506/500000: episode: 470, duration: 4.566s, episode steps: 951, steps per second: 208, episode reward: 181.184, mean reward:  0.191 [-17.613, 100.000], mean action: 1.393 [0.000, 3.000],  loss: 1.083872, mae: 12.149043, mean_q: 16.211678\n",
      " 268481/500000: episode: 471, duration: 4.248s, episode steps: 975, steps per second: 230, episode reward: 168.303, mean reward:  0.173 [-8.897, 100.000], mean action: 1.578 [0.000, 3.000],  loss: 1.644051, mae: 12.475084, mean_q: 16.613932\n",
      " 269251/500000: episode: 472, duration: 3.432s, episode steps: 770, steps per second: 224, episode reward: 197.542, mean reward:  0.257 [-11.341, 100.000], mean action: 1.452 [0.000, 3.000],  loss: 1.545734, mae: 12.437797, mean_q: 16.541464\n",
      " 270205/500000: episode: 473, duration: 4.439s, episode steps: 954, steps per second: 215, episode reward: 172.205, mean reward:  0.181 [-19.385, 100.000], mean action: 1.570 [0.000, 3.000],  loss: 1.703510, mae: 12.460557, mean_q: 16.538050\n",
      " 271205/500000: episode: 474, duration: 4.187s, episode steps: 1000, steps per second: 239, episode reward: 34.718, mean reward:  0.035 [-17.399, 23.670], mean action: 1.803 [0.000, 3.000],  loss: 1.964038, mae: 12.628655, mean_q: 16.704512\n",
      " 271631/500000: episode: 475, duration: 1.625s, episode steps: 426, steps per second: 262, episode reward: -88.405, mean reward: -0.208 [-100.000,  9.851], mean action: 1.629 [0.000, 3.000],  loss: 1.942725, mae: 12.826322, mean_q: 16.995064\n",
      " 272631/500000: episode: 476, duration: 4.370s, episode steps: 1000, steps per second: 229, episode reward: -12.833, mean reward: -0.013 [-10.602, 16.909], mean action: 1.673 [0.000, 3.000],  loss: 1.960585, mae: 12.661805, mean_q: 16.784172\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 273608/500000: episode: 477, duration: 4.694s, episode steps: 977, steps per second: 208, episode reward: 156.377, mean reward:  0.160 [-18.938, 100.000], mean action: 1.254 [0.000, 3.000],  loss: 2.811951, mae: 12.712617, mean_q: 16.865797\n",
      " 274523/500000: episode: 478, duration: 4.279s, episode steps: 915, steps per second: 214, episode reward: 129.449, mean reward:  0.141 [-20.444, 100.000], mean action: 1.655 [0.000, 3.000],  loss: 2.111888, mae: 12.921087, mean_q: 17.166407\n",
      " 274773/500000: episode: 479, duration: 0.949s, episode steps: 250, steps per second: 263, episode reward: -181.510, mean reward: -0.726 [-100.000, 17.591], mean action: 1.736 [0.000, 3.000],  loss: 3.457007, mae: 13.151826, mean_q: 17.517763\n",
      " 275528/500000: episode: 480, duration: 3.213s, episode steps: 755, steps per second: 235, episode reward: 189.821, mean reward:  0.251 [-19.327, 100.000], mean action: 1.162 [0.000, 3.000],  loss: 1.675916, mae: 13.279563, mean_q: 17.765587\n",
      " 276201/500000: episode: 481, duration: 3.474s, episode steps: 673, steps per second: 194, episode reward: 189.870, mean reward:  0.282 [-10.321, 100.000], mean action: 1.377 [0.000, 3.000],  loss: 2.299861, mae: 13.503639, mean_q: 18.071741\n",
      " 276288/500000: episode: 482, duration: 0.373s, episode steps:  87, steps per second: 233, episode reward: -277.302, mean reward: -3.187 [-100.000,  9.337], mean action: 1.931 [0.000, 3.000],  loss: 1.683948, mae: 13.569812, mean_q: 18.141079\n",
      " 276381/500000: episode: 483, duration: 0.407s, episode steps:  93, steps per second: 229, episode reward: -299.159, mean reward: -3.217 [-100.000,  2.812], mean action: 1.785 [0.000, 3.000],  loss: 4.222819, mae: 13.390327, mean_q: 17.919529\n",
      " 276750/500000: episode: 484, duration: 1.534s, episode steps: 369, steps per second: 241, episode reward: -49.422, mean reward: -0.134 [-100.000, 18.106], mean action: 1.705 [0.000, 3.000],  loss: 1.791232, mae: 14.034995, mean_q: 18.814207\n",
      " 276890/500000: episode: 485, duration: 0.615s, episode steps: 140, steps per second: 228, episode reward: -96.281, mean reward: -0.688 [-100.000, 12.604], mean action: 2.064 [0.000, 3.000],  loss: 5.467949, mae: 13.971427, mean_q: 18.773523\n",
      " 277189/500000: episode: 486, duration: 1.151s, episode steps: 299, steps per second: 260, episode reward: -92.441, mean reward: -0.309 [-100.000, 11.432], mean action: 1.823 [0.000, 3.000],  loss: 5.251537, mae: 13.860420, mean_q: 18.623569\n",
      " 277714/500000: episode: 487, duration: 2.378s, episode steps: 525, steps per second: 221, episode reward: -33.369, mean reward: -0.064 [-100.000, 14.207], mean action: 1.663 [0.000, 3.000],  loss: 5.337609, mae: 14.072203, mean_q: 18.940542\n",
      " 277942/500000: episode: 488, duration: 0.916s, episode steps: 228, steps per second: 249, episode reward: -373.617, mean reward: -1.639 [-100.000, 18.534], mean action: 1.623 [0.000, 3.000],  loss: 7.710381, mae: 14.051325, mean_q: 18.970387\n",
      " 278225/500000: episode: 489, duration: 1.362s, episode steps: 283, steps per second: 208, episode reward: -82.511, mean reward: -0.292 [-100.000, 12.131], mean action: 1.714 [0.000, 3.000],  loss: 3.148506, mae: 14.182482, mean_q: 19.096966\n",
      " 278532/500000: episode: 490, duration: 1.223s, episode steps: 307, steps per second: 251, episode reward: -101.701, mean reward: -0.331 [-100.000, 22.086], mean action: 1.456 [0.000, 3.000],  loss: 3.617260, mae: 14.066858, mean_q: 18.834669\n",
      " 278691/500000: episode: 491, duration: 0.583s, episode steps: 159, steps per second: 273, episode reward: -295.634, mean reward: -1.859 [-100.000, 40.277], mean action: 1.610 [0.000, 3.000],  loss: 4.469263, mae: 14.639937, mean_q: 19.575140\n",
      " 278784/500000: episode: 492, duration: 0.341s, episode steps:  93, steps per second: 272, episode reward: -177.459, mean reward: -1.908 [-100.000, 12.005], mean action: 1.903 [0.000, 3.000],  loss: 2.837829, mae: 14.551776, mean_q: 19.240536\n",
      " 279204/500000: episode: 493, duration: 1.660s, episode steps: 420, steps per second: 253, episode reward: -91.743, mean reward: -0.218 [-100.000, 12.237], mean action: 1.698 [0.000, 3.000],  loss: 3.730797, mae: 14.615125, mean_q: 19.488207\n",
      " 279308/500000: episode: 494, duration: 0.386s, episode steps: 104, steps per second: 269, episode reward: -277.204, mean reward: -2.665 [-100.000, 41.088], mean action: 1.798 [1.000, 3.000],  loss: 2.987098, mae: 14.551006, mean_q: 19.662205\n",
      " 279394/500000: episode: 495, duration: 0.315s, episode steps:  86, steps per second: 273, episode reward: -246.780, mean reward: -2.870 [-100.000, 13.386], mean action: 1.930 [0.000, 3.000],  loss: 7.869719, mae: 14.434991, mean_q: 19.282249\n",
      " 279477/500000: episode: 496, duration: 0.308s, episode steps:  83, steps per second: 269, episode reward: -196.149, mean reward: -2.363 [-100.000, 16.301], mean action: 1.867 [0.000, 3.000],  loss: 11.173453, mae: 15.208523, mean_q: 20.074291\n",
      " 279584/500000: episode: 497, duration: 0.400s, episode steps: 107, steps per second: 267, episode reward: -228.755, mean reward: -2.138 [-100.000,  4.011], mean action: 2.178 [0.000, 3.000],  loss: 4.097598, mae: 14.282683, mean_q: 19.070789\n",
      " 279757/500000: episode: 498, duration: 0.647s, episode steps: 173, steps per second: 267, episode reward: -310.718, mean reward: -1.796 [-100.000,  4.676], mean action: 1.896 [0.000, 3.000],  loss: 8.067704, mae: 14.823709, mean_q: 19.317944\n",
      " 279883/500000: episode: 499, duration: 0.466s, episode steps: 126, steps per second: 270, episode reward: -262.137, mean reward: -2.080 [-100.000, 23.759], mean action: 1.540 [0.000, 3.000],  loss: 7.634946, mae: 14.923401, mean_q: 19.766270\n",
      " 280058/500000: episode: 500, duration: 0.641s, episode steps: 175, steps per second: 273, episode reward: -34.911, mean reward: -0.199 [-100.000, 23.514], mean action: 1.726 [0.000, 3.000],  loss: 5.030667, mae: 15.049715, mean_q: 19.568855\n",
      " 280221/500000: episode: 501, duration: 0.606s, episode steps: 163, steps per second: 269, episode reward: -86.050, mean reward: -0.528 [-100.000, 13.485], mean action: 1.687 [0.000, 3.000],  loss: 9.210959, mae: 15.339287, mean_q: 20.456993\n",
      " 280445/500000: episode: 502, duration: 0.948s, episode steps: 224, steps per second: 236, episode reward: -215.841, mean reward: -0.964 [-100.000, 25.272], mean action: 1.621 [0.000, 3.000],  loss: 4.755396, mae: 15.411990, mean_q: 20.489939\n",
      " 280643/500000: episode: 503, duration: 0.749s, episode steps: 198, steps per second: 264, episode reward: -65.396, mean reward: -0.330 [-100.000,  9.740], mean action: 1.803 [0.000, 3.000],  loss: 4.846550, mae: 15.541568, mean_q: 20.677723\n",
      " 281072/500000: episode: 504, duration: 1.653s, episode steps: 429, steps per second: 260, episode reward: -20.800, mean reward: -0.048 [-100.000, 17.128], mean action: 1.709 [0.000, 3.000],  loss: 3.994970, mae: 15.905864, mean_q: 20.976116\n",
      " 281209/500000: episode: 505, duration: 0.505s, episode steps: 137, steps per second: 271, episode reward: -419.226, mean reward: -3.060 [-100.000,  9.563], mean action: 1.832 [0.000, 3.000],  loss: 18.673960, mae: 16.281666, mean_q: 21.587759\n",
      " 281723/500000: episode: 506, duration: 2.064s, episode steps: 514, steps per second: 249, episode reward: -133.828, mean reward: -0.260 [-100.000, 12.958], mean action: 1.607 [0.000, 3.000],  loss: 4.719509, mae: 15.952506, mean_q: 21.201540\n",
      " 282028/500000: episode: 507, duration: 1.174s, episode steps: 305, steps per second: 260, episode reward: -340.898, mean reward: -1.118 [-100.000,  7.143], mean action: 1.820 [0.000, 3.000],  loss: 5.503648, mae: 15.905821, mean_q: 20.822062\n",
      " 282212/500000: episode: 508, duration: 0.680s, episode steps: 184, steps per second: 271, episode reward: -174.620, mean reward: -0.949 [-100.000, 20.107], mean action: 1.511 [0.000, 3.000],  loss: 7.236319, mae: 15.815114, mean_q: 20.784426\n",
      " 282485/500000: episode: 509, duration: 1.036s, episode steps: 273, steps per second: 264, episode reward: -218.342, mean reward: -0.800 [-100.000,  4.006], mean action: 1.806 [0.000, 3.000],  loss: 5.698116, mae: 16.590101, mean_q: 21.771919\n",
      " 282724/500000: episode: 510, duration: 0.890s, episode steps: 239, steps per second: 269, episode reward: -58.772, mean reward: -0.246 [-100.000, 17.816], mean action: 1.799 [0.000, 3.000],  loss: 5.401743, mae: 15.996346, mean_q: 21.087341\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 282855/500000: episode: 511, duration: 0.483s, episode steps: 131, steps per second: 271, episode reward: -177.726, mean reward: -1.357 [-100.000,  7.803], mean action: 1.832 [0.000, 3.000],  loss: 5.597253, mae: 16.233988, mean_q: 21.342339\n",
      " 283052/500000: episode: 512, duration: 0.728s, episode steps: 197, steps per second: 270, episode reward: -276.771, mean reward: -1.405 [-100.000, 59.469], mean action: 1.736 [0.000, 3.000],  loss: 9.571940, mae: 16.821354, mean_q: 22.100100\n",
      " 283289/500000: episode: 513, duration: 0.889s, episode steps: 237, steps per second: 266, episode reward: 33.391, mean reward:  0.141 [-100.000, 11.244], mean action: 1.759 [0.000, 3.000],  loss: 10.365641, mae: 16.894941, mean_q: 21.961985\n",
      " 283947/500000: episode: 514, duration: 2.682s, episode steps: 658, steps per second: 245, episode reward: 246.632, mean reward:  0.375 [-19.396, 100.000], mean action: 1.263 [0.000, 3.000],  loss: 7.717343, mae: 16.870836, mean_q: 21.944614\n",
      " 284226/500000: episode: 515, duration: 1.053s, episode steps: 279, steps per second: 265, episode reward: -234.926, mean reward: -0.842 [-100.000, 10.032], mean action: 2.100 [0.000, 3.000],  loss: 6.357311, mae: 17.353159, mean_q: 22.310928\n",
      " 284491/500000: episode: 516, duration: 1.002s, episode steps: 265, steps per second: 264, episode reward: -149.712, mean reward: -0.565 [-100.000, 10.228], mean action: 1.845 [0.000, 3.000],  loss: 5.631737, mae: 17.378384, mean_q: 22.466579\n",
      " 285192/500000: episode: 517, duration: 2.814s, episode steps: 701, steps per second: 249, episode reward: -140.403, mean reward: -0.200 [-100.000, 21.323], mean action: 1.695 [0.000, 3.000],  loss: 6.616452, mae: 17.726643, mean_q: 22.730396\n",
      " 285635/500000: episode: 518, duration: 1.726s, episode steps: 443, steps per second: 257, episode reward: -46.201, mean reward: -0.104 [-100.000, 14.594], mean action: 1.781 [0.000, 3.000],  loss: 6.189680, mae: 17.822798, mean_q: 22.758793\n",
      " 286635/500000: episode: 519, duration: 4.744s, episode steps: 1000, steps per second: 211, episode reward: -19.865, mean reward: -0.020 [-4.483,  5.300], mean action: 1.696 [0.000, 3.000],  loss: 4.998336, mae: 17.794504, mean_q: 22.260195\n",
      " 286852/500000: episode: 520, duration: 0.802s, episode steps: 217, steps per second: 270, episode reward: -67.783, mean reward: -0.312 [-100.000,  6.887], mean action: 1.783 [0.000, 3.000],  loss: 6.714843, mae: 17.818733, mean_q: 22.024906\n",
      " 287852/500000: episode: 521, duration: 4.359s, episode steps: 1000, steps per second: 229, episode reward: 44.345, mean reward:  0.044 [-19.330, 12.507], mean action: 1.542 [0.000, 3.000],  loss: 6.220025, mae: 17.612036, mean_q: 21.914179\n",
      " 288311/500000: episode: 522, duration: 1.798s, episode steps: 459, steps per second: 255, episode reward: -106.091, mean reward: -0.231 [-100.000, 23.139], mean action: 1.734 [0.000, 3.000],  loss: 9.239546, mae: 17.602798, mean_q: 21.775570\n",
      " 288922/500000: episode: 523, duration: 2.548s, episode steps: 611, steps per second: 240, episode reward: -109.506, mean reward: -0.179 [-100.000, 10.543], mean action: 1.849 [0.000, 3.000],  loss: 6.181660, mae: 17.689610, mean_q: 22.156906\n",
      " 289437/500000: episode: 524, duration: 2.121s, episode steps: 515, steps per second: 243, episode reward: -77.049, mean reward: -0.150 [-100.000, 11.442], mean action: 1.689 [0.000, 3.000],  loss: 6.035252, mae: 17.822903, mean_q: 21.700909\n",
      " 290437/500000: episode: 525, duration: 4.754s, episode steps: 1000, steps per second: 210, episode reward: 43.166, mean reward:  0.043 [-12.135, 13.625], mean action: 1.412 [0.000, 3.000],  loss: 6.901351, mae: 17.907188, mean_q: 21.675938\n",
      " 291437/500000: episode: 526, duration: 4.529s, episode steps: 1000, steps per second: 221, episode reward: -37.530, mean reward: -0.038 [-4.305,  4.596], mean action: 1.655 [0.000, 3.000],  loss: 7.517423, mae: 18.142487, mean_q: 21.998737\n",
      " 292437/500000: episode: 527, duration: 4.278s, episode steps: 1000, steps per second: 234, episode reward: -76.416, mean reward: -0.076 [-3.878,  4.335], mean action: 1.730 [0.000, 3.000],  loss: 5.312503, mae: 17.857971, mean_q: 21.563023\n",
      " 293437/500000: episode: 528, duration: 4.288s, episode steps: 1000, steps per second: 233, episode reward: 11.194, mean reward:  0.011 [-4.634,  5.315], mean action: 1.713 [0.000, 3.000],  loss: 5.800583, mae: 17.790041, mean_q: 21.419699\n",
      " 294437/500000: episode: 529, duration: 4.756s, episode steps: 1000, steps per second: 210, episode reward: -26.079, mean reward: -0.026 [-3.998,  5.090], mean action: 1.680 [0.000, 3.000],  loss: 5.912685, mae: 17.669598, mean_q: 21.405806\n",
      " 295437/500000: episode: 530, duration: 4.285s, episode steps: 1000, steps per second: 233, episode reward: 61.622, mean reward:  0.062 [-9.519, 10.683], mean action: 1.703 [0.000, 3.000],  loss: 5.396348, mae: 17.505754, mean_q: 21.020840\n",
      " 296437/500000: episode: 531, duration: 4.761s, episode steps: 1000, steps per second: 210, episode reward: 29.615, mean reward:  0.030 [-4.010, 10.888], mean action: 1.641 [0.000, 3.000],  loss: 5.987973, mae: 17.674835, mean_q: 21.398159\n",
      " 296583/500000: episode: 532, duration: 0.543s, episode steps: 146, steps per second: 269, episode reward: -70.914, mean reward: -0.486 [-100.000,  3.766], mean action: 1.514 [0.000, 3.000],  loss: 5.084435, mae: 17.795015, mean_q: 21.410799\n",
      " 297583/500000: episode: 533, duration: 4.670s, episode steps: 1000, steps per second: 214, episode reward: -31.448, mean reward: -0.031 [-3.555,  4.303], mean action: 1.693 [0.000, 3.000],  loss: 4.749628, mae: 17.663462, mean_q: 21.460230\n",
      " 298583/500000: episode: 534, duration: 4.644s, episode steps: 1000, steps per second: 215, episode reward: -20.112, mean reward: -0.020 [-3.244,  4.521], mean action: 1.678 [0.000, 3.000],  loss: 4.668598, mae: 17.616579, mean_q: 21.262089\n",
      " 299583/500000: episode: 535, duration: 4.761s, episode steps: 1000, steps per second: 210, episode reward: -45.550, mean reward: -0.046 [-3.703,  4.792], mean action: 1.543 [0.000, 3.000],  loss: 4.875250, mae: 17.572136, mean_q: 21.355854\n",
      " 300583/500000: episode: 536, duration: 4.548s, episode steps: 1000, steps per second: 220, episode reward: -62.745, mean reward: -0.063 [-3.815,  3.913], mean action: 1.552 [0.000, 3.000],  loss: 6.193242, mae: 17.684336, mean_q: 21.240965\n",
      " 301583/500000: episode: 537, duration: 4.471s, episode steps: 1000, steps per second: 224, episode reward: -6.392, mean reward: -0.006 [-4.229,  4.468], mean action: 1.533 [0.000, 3.000],  loss: 5.494690, mae: 17.595377, mean_q: 21.015078\n",
      " 302583/500000: episode: 538, duration: 4.748s, episode steps: 1000, steps per second: 211, episode reward: -1.714, mean reward: -0.002 [-7.292, 11.494], mean action: 1.693 [0.000, 3.000],  loss: 6.075408, mae: 17.420195, mean_q: 20.761475\n",
      " 303386/500000: episode: 539, duration: 3.415s, episode steps: 803, steps per second: 235, episode reward: -100.983, mean reward: -0.126 [-100.000, 11.036], mean action: 1.842 [0.000, 3.000],  loss: 6.391063, mae: 18.012941, mean_q: 21.478537\n",
      " 304386/500000: episode: 540, duration: 4.479s, episode steps: 1000, steps per second: 223, episode reward: 43.227, mean reward:  0.043 [-17.429, 23.676], mean action: 1.521 [0.000, 3.000],  loss: 6.383802, mae: 18.033878, mean_q: 21.329851\n",
      " 305128/500000: episode: 541, duration: 3.093s, episode steps: 742, steps per second: 240, episode reward: -129.056, mean reward: -0.174 [-100.000, 10.641], mean action: 1.571 [0.000, 3.000],  loss: 6.486675, mae: 18.122688, mean_q: 20.968746\n",
      " 306128/500000: episode: 542, duration: 4.371s, episode steps: 1000, steps per second: 229, episode reward: 19.340, mean reward:  0.019 [-8.960, 13.104], mean action: 1.526 [0.000, 3.000],  loss: 5.907485, mae: 18.267569, mean_q: 21.763029\n",
      " 307107/500000: episode: 543, duration: 4.119s, episode steps: 979, steps per second: 238, episode reward: 153.536, mean reward:  0.157 [-16.982, 100.000], mean action: 1.527 [0.000, 3.000],  loss: 5.342960, mae: 18.155174, mean_q: 22.001322\n",
      " 308065/500000: episode: 544, duration: 3.960s, episode steps: 958, steps per second: 242, episode reward: 152.890, mean reward:  0.160 [-20.360, 100.000], mean action: 1.443 [0.000, 3.000],  loss: 5.311880, mae: 18.363501, mean_q: 21.781788\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 309065/500000: episode: 545, duration: 4.652s, episode steps: 1000, steps per second: 215, episode reward: 68.111, mean reward:  0.068 [-10.893, 10.979], mean action: 1.463 [0.000, 3.000],  loss: 5.609982, mae: 18.259171, mean_q: 21.719511\n",
      " 309894/500000: episode: 546, duration: 3.511s, episode steps: 829, steps per second: 236, episode reward: 133.850, mean reward:  0.161 [-17.481, 100.000], mean action: 1.613 [0.000, 3.000],  loss: 5.646359, mae: 18.622087, mean_q: 22.495911\n",
      " 310611/500000: episode: 547, duration: 3.056s, episode steps: 717, steps per second: 235, episode reward: -112.419, mean reward: -0.157 [-100.000, 13.860], mean action: 1.679 [0.000, 3.000],  loss: 6.925633, mae: 18.787813, mean_q: 22.635271\n",
      " 311611/500000: episode: 548, duration: 4.500s, episode steps: 1000, steps per second: 222, episode reward: 40.325, mean reward:  0.040 [-18.966, 22.860], mean action: 1.572 [0.000, 3.000],  loss: 6.517378, mae: 18.785723, mean_q: 22.691444\n",
      " 312245/500000: episode: 549, duration: 3.069s, episode steps: 634, steps per second: 207, episode reward: -51.715, mean reward: -0.082 [-100.000, 17.249], mean action: 1.820 [0.000, 3.000],  loss: 8.187954, mae: 19.174847, mean_q: 23.520578\n",
      " 313245/500000: episode: 550, duration: 7.259s, episode steps: 1000, steps per second: 138, episode reward:  3.869, mean reward:  0.004 [-17.970, 22.237], mean action: 1.513 [0.000, 3.000],  loss: 7.320693, mae: 19.167738, mean_q: 23.158792\n",
      " 314245/500000: episode: 551, duration: 4.955s, episode steps: 1000, steps per second: 202, episode reward: 96.331, mean reward:  0.096 [-19.481, 12.399], mean action: 1.471 [0.000, 3.000],  loss: 6.544363, mae: 20.021841, mean_q: 24.191957\n",
      " 315187/500000: episode: 552, duration: 4.336s, episode steps: 942, steps per second: 217, episode reward: 115.064, mean reward:  0.122 [-10.128, 100.000], mean action: 1.466 [0.000, 3.000],  loss: 7.374856, mae: 20.669880, mean_q: 25.389339\n",
      " 316187/500000: episode: 553, duration: 4.591s, episode steps: 1000, steps per second: 218, episode reward: -21.812, mean reward: -0.022 [-20.145, 14.398], mean action: 1.625 [0.000, 3.000],  loss: 5.600619, mae: 21.012672, mean_q: 25.927380\n",
      " 317187/500000: episode: 554, duration: 4.421s, episode steps: 1000, steps per second: 226, episode reward: -126.403, mean reward: -0.126 [-4.328,  4.297], mean action: 1.751 [0.000, 3.000],  loss: 7.671718, mae: 21.664276, mean_q: 26.632471\n",
      " 318187/500000: episode: 555, duration: 4.834s, episode steps: 1000, steps per second: 207, episode reward: -30.583, mean reward: -0.031 [-3.757,  4.428], mean action: 1.687 [0.000, 3.000],  loss: 5.717408, mae: 22.274145, mean_q: 27.501757\n",
      " 319061/500000: episode: 556, duration: 3.507s, episode steps: 874, steps per second: 249, episode reward: -88.194, mean reward: -0.101 [-100.000, 12.766], mean action: 1.672 [0.000, 3.000],  loss: 6.261635, mae: 22.790648, mean_q: 28.266081\n",
      " 320061/500000: episode: 557, duration: 4.792s, episode steps: 1000, steps per second: 209, episode reward: -41.077, mean reward: -0.041 [-4.233,  4.335], mean action: 1.636 [0.000, 3.000],  loss: 6.386439, mae: 22.542942, mean_q: 27.727934\n",
      " 321061/500000: episode: 558, duration: 4.393s, episode steps: 1000, steps per second: 228, episode reward: -53.723, mean reward: -0.054 [-3.115,  4.473], mean action: 1.638 [0.000, 3.000],  loss: 6.600106, mae: 22.745653, mean_q: 27.888235\n",
      " 321516/500000: episode: 559, duration: 1.810s, episode steps: 455, steps per second: 251, episode reward: -78.633, mean reward: -0.173 [-100.000,  4.248], mean action: 1.648 [0.000, 3.000],  loss: 8.228245, mae: 23.122532, mean_q: 28.217724\n",
      " 321785/500000: episode: 560, duration: 1.001s, episode steps: 269, steps per second: 269, episode reward: -108.427, mean reward: -0.403 [-100.000,  3.345], mean action: 1.770 [0.000, 3.000],  loss: 7.296245, mae: 23.169140, mean_q: 28.598673\n",
      " 322785/500000: episode: 561, duration: 4.333s, episode steps: 1000, steps per second: 231, episode reward: -78.315, mean reward: -0.078 [-4.407,  4.971], mean action: 1.592 [0.000, 3.000],  loss: 6.360230, mae: 23.414539, mean_q: 28.789816\n",
      " 323089/500000: episode: 562, duration: 1.156s, episode steps: 304, steps per second: 263, episode reward: -122.016, mean reward: -0.401 [-100.000,  2.974], mean action: 1.734 [0.000, 3.000],  loss: 5.031368, mae: 23.247726, mean_q: 28.692736\n",
      " 323316/500000: episode: 563, duration: 0.843s, episode steps: 227, steps per second: 269, episode reward: -113.437, mean reward: -0.500 [-100.000,  2.713], mean action: 1.573 [0.000, 3.000],  loss: 5.035898, mae: 23.467497, mean_q: 29.269411\n",
      " 324316/500000: episode: 564, duration: 4.192s, episode steps: 1000, steps per second: 239, episode reward: -59.280, mean reward: -0.059 [-4.878,  3.922], mean action: 1.438 [0.000, 3.000],  loss: 6.957059, mae: 23.869150, mean_q: 29.304419\n",
      " 324941/500000: episode: 565, duration: 2.674s, episode steps: 625, steps per second: 234, episode reward: -167.290, mean reward: -0.268 [-100.000,  3.789], mean action: 1.490 [0.000, 3.000],  loss: 5.089300, mae: 23.662270, mean_q: 28.550472\n",
      " 325941/500000: episode: 566, duration: 4.323s, episode steps: 1000, steps per second: 231, episode reward: -19.309, mean reward: -0.019 [-3.447,  4.394], mean action: 1.680 [0.000, 3.000],  loss: 5.851227, mae: 23.693817, mean_q: 28.931435\n",
      " 326885/500000: episode: 567, duration: 4.361s, episode steps: 944, steps per second: 216, episode reward: -157.018, mean reward: -0.166 [-100.000,  8.351], mean action: 1.631 [0.000, 3.000],  loss: 6.204475, mae: 23.806412, mean_q: 29.026789\n",
      " 327223/500000: episode: 568, duration: 1.281s, episode steps: 338, steps per second: 264, episode reward: -145.526, mean reward: -0.431 [-100.000,  2.848], mean action: 1.367 [0.000, 3.000],  loss: 4.711715, mae: 23.415262, mean_q: 28.390783\n",
      " 327661/500000: episode: 569, duration: 1.743s, episode steps: 438, steps per second: 251, episode reward: -100.813, mean reward: -0.230 [-100.000,  5.070], mean action: 1.525 [0.000, 3.000],  loss: 7.480713, mae: 23.453947, mean_q: 28.553984\n",
      " 328661/500000: episode: 570, duration: 4.463s, episode steps: 1000, steps per second: 224, episode reward: -38.029, mean reward: -0.038 [-3.347,  4.553], mean action: 1.731 [0.000, 3.000],  loss: 5.172404, mae: 23.229128, mean_q: 28.380993\n",
      " 328967/500000: episode: 571, duration: 1.146s, episode steps: 306, steps per second: 267, episode reward: -85.691, mean reward: -0.280 [-100.000,  4.233], mean action: 1.611 [0.000, 3.000],  loss: 3.214782, mae: 23.136450, mean_q: 27.953291\n",
      " 329894/500000: episode: 572, duration: 4.075s, episode steps: 927, steps per second: 227, episode reward: -215.722, mean reward: -0.233 [-100.000,  4.178], mean action: 1.480 [0.000, 3.000],  loss: 6.266547, mae: 22.861633, mean_q: 28.413801\n",
      " 330894/500000: episode: 573, duration: 4.299s, episode steps: 1000, steps per second: 233, episode reward: -40.961, mean reward: -0.041 [-4.786,  5.006], mean action: 1.552 [0.000, 3.000],  loss: 4.554140, mae: 22.109198, mean_q: 27.910450\n",
      " 331794/500000: episode: 574, duration: 3.729s, episode steps: 900, steps per second: 241, episode reward: -154.864, mean reward: -0.172 [-100.000,  4.418], mean action: 1.554 [0.000, 3.000],  loss: 7.008797, mae: 21.481483, mean_q: 27.246897\n",
      " 332794/500000: episode: 575, duration: 4.224s, episode steps: 1000, steps per second: 237, episode reward: -55.771, mean reward: -0.056 [-4.670,  4.259], mean action: 1.637 [0.000, 3.000],  loss: 5.393373, mae: 20.840567, mean_q: 26.680576\n",
      " 333794/500000: episode: 576, duration: 4.147s, episode steps: 1000, steps per second: 241, episode reward: -99.756, mean reward: -0.100 [-4.785,  4.779], mean action: 1.419 [0.000, 3.000],  loss: 4.237891, mae: 20.038172, mean_q: 25.987377\n",
      " 334794/500000: episode: 577, duration: 5.070s, episode steps: 1000, steps per second: 197, episode reward: -33.224, mean reward: -0.033 [-4.277,  4.757], mean action: 1.589 [0.000, 3.000],  loss: 4.126080, mae: 19.543602, mean_q: 25.635820\n",
      " 335794/500000: episode: 578, duration: 4.748s, episode steps: 1000, steps per second: 211, episode reward: -60.946, mean reward: -0.061 [-10.831, 12.475], mean action: 1.441 [0.000, 3.000],  loss: 4.050336, mae: 18.989594, mean_q: 24.947216\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 336641/500000: episode: 579, duration: 3.695s, episode steps: 847, steps per second: 229, episode reward: -169.983, mean reward: -0.201 [-100.000,  9.821], mean action: 1.488 [0.000, 3.000],  loss: 4.036169, mae: 18.424587, mean_q: 24.154737\n",
      " 337641/500000: episode: 580, duration: 4.471s, episode steps: 1000, steps per second: 224, episode reward: -44.753, mean reward: -0.045 [-3.582,  5.179], mean action: 1.521 [0.000, 3.000],  loss: 3.650139, mae: 17.862366, mean_q: 23.288187\n",
      " 338641/500000: episode: 581, duration: 4.476s, episode steps: 1000, steps per second: 223, episode reward: 49.076, mean reward:  0.049 [-21.300, 21.255], mean action: 1.571 [0.000, 3.000],  loss: 3.682633, mae: 17.464451, mean_q: 22.677301\n",
      " 339501/500000: episode: 582, duration: 3.711s, episode steps: 860, steps per second: 232, episode reward: -148.873, mean reward: -0.173 [-100.000,  9.510], mean action: 1.633 [0.000, 3.000],  loss: 3.868889, mae: 17.128979, mean_q: 22.164303\n",
      " 340501/500000: episode: 583, duration: 4.297s, episode steps: 1000, steps per second: 233, episode reward: 47.006, mean reward:  0.047 [-20.248, 22.493], mean action: 1.794 [0.000, 3.000],  loss: 4.725554, mae: 17.208330, mean_q: 22.017027\n",
      " 341501/500000: episode: 584, duration: 4.383s, episode steps: 1000, steps per second: 228, episode reward: -31.212, mean reward: -0.031 [-4.495,  5.838], mean action: 1.686 [0.000, 3.000],  loss: 3.770799, mae: 17.107296, mean_q: 22.081219\n",
      " 342501/500000: episode: 585, duration: 4.714s, episode steps: 1000, steps per second: 212, episode reward: 17.608, mean reward:  0.018 [-3.354, 14.598], mean action: 1.612 [0.000, 3.000],  loss: 4.758236, mae: 17.066435, mean_q: 22.047558\n",
      " 343501/500000: episode: 586, duration: 4.421s, episode steps: 1000, steps per second: 226, episode reward: 21.647, mean reward:  0.022 [-20.240, 21.235], mean action: 1.668 [0.000, 3.000],  loss: 3.032745, mae: 17.364546, mean_q: 22.461691\n",
      " 344501/500000: episode: 587, duration: 4.839s, episode steps: 1000, steps per second: 207, episode reward: 54.090, mean reward:  0.054 [-20.913, 14.944], mean action: 1.683 [0.000, 3.000],  loss: 4.045028, mae: 17.430853, mean_q: 22.544880\n",
      " 345501/500000: episode: 588, duration: 4.685s, episode steps: 1000, steps per second: 213, episode reward: 15.408, mean reward:  0.015 [-10.413, 20.104], mean action: 1.672 [0.000, 3.000],  loss: 3.278492, mae: 17.550169, mean_q: 22.878204\n",
      " 346501/500000: episode: 589, duration: 5.118s, episode steps: 1000, steps per second: 195, episode reward: -2.726, mean reward: -0.003 [-13.896, 15.786], mean action: 1.609 [0.000, 3.000],  loss: 3.593664, mae: 17.620974, mean_q: 23.037867\n",
      " 347501/500000: episode: 590, duration: 4.336s, episode steps: 1000, steps per second: 231, episode reward: -44.038, mean reward: -0.044 [-4.173,  4.598], mean action: 1.650 [0.000, 3.000],  loss: 2.645286, mae: 17.574867, mean_q: 22.839504\n",
      " 348501/500000: episode: 591, duration: 4.742s, episode steps: 1000, steps per second: 211, episode reward: -2.308, mean reward: -0.002 [-3.677,  4.642], mean action: 1.737 [0.000, 3.000],  loss: 3.207979, mae: 17.585518, mean_q: 22.856159\n",
      " 349501/500000: episode: 592, duration: 5.085s, episode steps: 1000, steps per second: 197, episode reward: -47.873, mean reward: -0.048 [-4.154,  4.334], mean action: 1.804 [0.000, 3.000],  loss: 3.181039, mae: 17.871140, mean_q: 23.201006\n",
      " 350501/500000: episode: 593, duration: 5.156s, episode steps: 1000, steps per second: 194, episode reward:  0.392, mean reward:  0.000 [-11.598, 13.037], mean action: 1.772 [0.000, 3.000],  loss: 2.728644, mae: 17.758480, mean_q: 23.069925\n",
      " 351501/500000: episode: 594, duration: 4.388s, episode steps: 1000, steps per second: 228, episode reward: -25.138, mean reward: -0.025 [-5.014,  5.628], mean action: 1.774 [0.000, 3.000],  loss: 2.048892, mae: 17.760199, mean_q: 23.217981\n",
      " 352501/500000: episode: 595, duration: 4.320s, episode steps: 1000, steps per second: 231, episode reward:  7.499, mean reward:  0.007 [-4.107,  4.593], mean action: 1.734 [0.000, 3.000],  loss: 2.653444, mae: 17.619110, mean_q: 22.971277\n",
      " 353501/500000: episode: 596, duration: 4.263s, episode steps: 1000, steps per second: 235, episode reward: -36.033, mean reward: -0.036 [-3.910,  4.262], mean action: 1.752 [0.000, 3.000],  loss: 2.207589, mae: 17.370234, mean_q: 22.707655\n",
      " 354501/500000: episode: 597, duration: 4.952s, episode steps: 1000, steps per second: 202, episode reward: 63.146, mean reward:  0.063 [-19.009, 22.925], mean action: 1.630 [0.000, 3.000],  loss: 2.963743, mae: 17.124273, mean_q: 22.288883\n",
      " 355501/500000: episode: 598, duration: 4.781s, episode steps: 1000, steps per second: 209, episode reward: 49.819, mean reward:  0.050 [-20.749, 12.173], mean action: 1.659 [0.000, 3.000],  loss: 2.482842, mae: 17.023106, mean_q: 22.123636\n",
      " 356501/500000: episode: 599, duration: 4.297s, episode steps: 1000, steps per second: 233, episode reward: -53.684, mean reward: -0.054 [-3.835,  4.503], mean action: 1.673 [0.000, 3.000],  loss: 2.808452, mae: 16.711079, mean_q: 21.689743\n",
      " 357501/500000: episode: 600, duration: 4.770s, episode steps: 1000, steps per second: 210, episode reward: -25.851, mean reward: -0.026 [-4.466,  4.604], mean action: 1.714 [0.000, 3.000],  loss: 2.272247, mae: 16.163582, mean_q: 20.885794\n",
      " 358407/500000: episode: 601, duration: 4.212s, episode steps: 906, steps per second: 215, episode reward: 171.676, mean reward:  0.189 [-19.876, 100.000], mean action: 1.564 [0.000, 3.000],  loss: 2.574302, mae: 15.988788, mean_q: 20.514202\n",
      " 359407/500000: episode: 602, duration: 4.741s, episode steps: 1000, steps per second: 211, episode reward: -3.057, mean reward: -0.003 [-4.061,  5.435], mean action: 1.672 [0.000, 3.000],  loss: 2.587072, mae: 15.888103, mean_q: 20.546022\n",
      " 360407/500000: episode: 603, duration: 4.562s, episode steps: 1000, steps per second: 219, episode reward: -20.195, mean reward: -0.020 [-4.812,  6.005], mean action: 1.700 [0.000, 3.000],  loss: 2.045400, mae: 15.690750, mean_q: 20.266993\n",
      " 361407/500000: episode: 604, duration: 5.081s, episode steps: 1000, steps per second: 197, episode reward: -23.564, mean reward: -0.024 [-3.556,  4.927], mean action: 1.808 [0.000, 3.000],  loss: 2.379372, mae: 15.391437, mean_q: 19.965582\n",
      " 362407/500000: episode: 605, duration: 4.947s, episode steps: 1000, steps per second: 202, episode reward: -46.202, mean reward: -0.046 [-4.082,  4.658], mean action: 1.720 [0.000, 3.000],  loss: 2.720654, mae: 15.156056, mean_q: 19.704615\n",
      " 363407/500000: episode: 606, duration: 4.390s, episode steps: 1000, steps per second: 228, episode reward:  4.148, mean reward:  0.004 [-10.606, 14.760], mean action: 1.777 [0.000, 3.000],  loss: 2.373094, mae: 14.814439, mean_q: 19.091892\n",
      " 364407/500000: episode: 607, duration: 4.936s, episode steps: 1000, steps per second: 203, episode reward: 34.567, mean reward:  0.035 [-11.075, 16.089], mean action: 1.754 [0.000, 3.000],  loss: 2.498252, mae: 14.528857, mean_q: 18.747322\n",
      " 364667/500000: episode: 608, duration: 0.964s, episode steps: 260, steps per second: 270, episode reward: -301.290, mean reward: -1.159 [-100.000, 38.018], mean action: 1.550 [0.000, 3.000],  loss: 1.913510, mae: 14.604523, mean_q: 18.854849\n",
      " 365667/500000: episode: 609, duration: 5.473s, episode steps: 1000, steps per second: 183, episode reward: -15.257, mean reward: -0.015 [-5.129,  5.076], mean action: 1.868 [0.000, 3.000],  loss: 2.092275, mae: 14.359392, mean_q: 18.490328\n",
      " 366667/500000: episode: 610, duration: 4.608s, episode steps: 1000, steps per second: 217, episode reward: 20.919, mean reward:  0.021 [-7.184, 11.627], mean action: 1.805 [0.000, 3.000],  loss: 2.265267, mae: 14.173234, mean_q: 18.256680\n",
      " 367667/500000: episode: 611, duration: 4.444s, episode steps: 1000, steps per second: 225, episode reward: -15.467, mean reward: -0.015 [-4.515,  4.995], mean action: 1.647 [0.000, 3.000],  loss: 2.371965, mae: 13.845653, mean_q: 17.666397\n",
      " 368667/500000: episode: 612, duration: 4.523s, episode steps: 1000, steps per second: 221, episode reward: 17.785, mean reward:  0.018 [-13.007, 11.822], mean action: 1.502 [0.000, 3.000],  loss: 2.944409, mae: 13.589810, mean_q: 17.157980\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 369667/500000: episode: 613, duration: 4.427s, episode steps: 1000, steps per second: 226, episode reward: -23.358, mean reward: -0.023 [-3.996,  5.253], mean action: 1.869 [0.000, 3.000],  loss: 2.215890, mae: 13.499498, mean_q: 17.158890\n",
      " 370667/500000: episode: 614, duration: 4.446s, episode steps: 1000, steps per second: 225, episode reward: 13.251, mean reward:  0.013 [-4.564,  5.202], mean action: 1.638 [0.000, 3.000],  loss: 2.157799, mae: 13.440031, mean_q: 17.000235\n",
      " 371667/500000: episode: 615, duration: 4.524s, episode steps: 1000, steps per second: 221, episode reward: 28.956, mean reward:  0.029 [-4.312,  5.134], mean action: 1.678 [0.000, 3.000],  loss: 1.774809, mae: 13.479854, mean_q: 17.001925\n",
      " 372667/500000: episode: 616, duration: 4.900s, episode steps: 1000, steps per second: 204, episode reward: 27.110, mean reward:  0.027 [-8.479, 11.680], mean action: 1.693 [0.000, 3.000],  loss: 2.098632, mae: 13.262473, mean_q: 16.853521\n",
      " 373667/500000: episode: 617, duration: 4.387s, episode steps: 1000, steps per second: 228, episode reward: -45.125, mean reward: -0.045 [-4.568,  4.277], mean action: 1.731 [0.000, 3.000],  loss: 2.258484, mae: 13.050945, mean_q: 16.572384\n",
      " 374667/500000: episode: 618, duration: 4.829s, episode steps: 1000, steps per second: 207, episode reward: -22.682, mean reward: -0.023 [-4.105,  4.540], mean action: 1.841 [0.000, 3.000],  loss: 1.723618, mae: 12.894774, mean_q: 16.503469\n",
      " 375667/500000: episode: 619, duration: 5.084s, episode steps: 1000, steps per second: 197, episode reward: 32.469, mean reward:  0.032 [-10.585, 13.800], mean action: 1.585 [0.000, 3.000],  loss: 1.732932, mae: 12.626104, mean_q: 16.106861\n",
      " 376667/500000: episode: 620, duration: 5.477s, episode steps: 1000, steps per second: 183, episode reward: -43.646, mean reward: -0.044 [-5.302,  4.988], mean action: 1.680 [0.000, 3.000],  loss: 1.867696, mae: 12.490257, mean_q: 16.096247\n",
      " 377667/500000: episode: 621, duration: 4.516s, episode steps: 1000, steps per second: 221, episode reward:  2.008, mean reward:  0.002 [-4.207, 23.779], mean action: 1.776 [0.000, 3.000],  loss: 1.692309, mae: 12.420355, mean_q: 16.071522\n",
      " 378336/500000: episode: 622, duration: 2.779s, episode steps: 669, steps per second: 241, episode reward: 206.884, mean reward:  0.309 [-20.237, 100.000], mean action: 1.565 [0.000, 3.000],  loss: 2.493093, mae: 12.375727, mean_q: 16.107527\n",
      " 379109/500000: episode: 623, duration: 3.257s, episode steps: 773, steps per second: 237, episode reward: -66.002, mean reward: -0.085 [-100.000, 17.088], mean action: 1.715 [0.000, 3.000],  loss: 2.144080, mae: 12.655502, mean_q: 16.584469\n",
      " 380109/500000: episode: 624, duration: 4.656s, episode steps: 1000, steps per second: 215, episode reward: 83.083, mean reward:  0.083 [-20.982, 22.949], mean action: 1.599 [0.000, 3.000],  loss: 1.575362, mae: 12.976479, mean_q: 17.066692\n",
      " 381109/500000: episode: 625, duration: 5.297s, episode steps: 1000, steps per second: 189, episode reward: 28.917, mean reward:  0.029 [-17.256, 16.743], mean action: 1.739 [0.000, 3.000],  loss: 2.138716, mae: 13.428699, mean_q: 17.830286\n",
      " 381438/500000: episode: 626, duration: 1.252s, episode steps: 329, steps per second: 263, episode reward: -98.881, mean reward: -0.301 [-100.000,  3.630], mean action: 1.708 [0.000, 3.000],  loss: 1.586722, mae: 14.062380, mean_q: 18.777359\n",
      " 382438/500000: episode: 627, duration: 4.001s, episode steps: 1000, steps per second: 250, episode reward: 113.360, mean reward:  0.113 [-18.014, 14.130], mean action: 1.087 [0.000, 3.000],  loss: 1.788375, mae: 14.868368, mean_q: 19.828266\n",
      " 383329/500000: episode: 628, duration: 3.894s, episode steps: 891, steps per second: 229, episode reward: 123.123, mean reward:  0.138 [-10.430, 100.000], mean action: 1.587 [0.000, 3.000],  loss: 1.676454, mae: 15.740116, mean_q: 21.116253\n",
      " 384219/500000: episode: 629, duration: 3.641s, episode steps: 890, steps per second: 244, episode reward: 65.771, mean reward:  0.074 [-19.413, 100.000], mean action: 1.501 [0.000, 3.000],  loss: 2.035472, mae: 16.672541, mean_q: 22.275265\n",
      " 385049/500000: episode: 630, duration: 3.782s, episode steps: 830, steps per second: 219, episode reward: -609.221, mean reward: -0.734 [-100.000, 11.504], mean action: 1.635 [0.000, 3.000],  loss: 3.305845, mae: 17.311289, mean_q: 23.106998\n",
      " 385897/500000: episode: 631, duration: 3.602s, episode steps: 848, steps per second: 235, episode reward: 198.237, mean reward:  0.234 [-20.437, 100.000], mean action: 1.472 [0.000, 3.000],  loss: 1.625461, mae: 17.852234, mean_q: 23.879841\n",
      " 386518/500000: episode: 632, duration: 2.501s, episode steps: 621, steps per second: 248, episode reward: 227.691, mean reward:  0.367 [-11.392, 100.000], mean action: 1.436 [0.000, 3.000],  loss: 4.226532, mae: 17.910994, mean_q: 23.900938\n",
      " 387392/500000: episode: 633, duration: 4.009s, episode steps: 874, steps per second: 218, episode reward: 185.027, mean reward:  0.212 [-19.944, 100.000], mean action: 1.284 [0.000, 3.000],  loss: 2.483196, mae: 18.159639, mean_q: 24.104765\n",
      " 388110/500000: episode: 634, duration: 2.994s, episode steps: 718, steps per second: 240, episode reward: 133.573, mean reward:  0.186 [-9.256, 100.000], mean action: 1.467 [0.000, 3.000],  loss: 2.853206, mae: 18.338257, mean_q: 24.408253\n",
      " 389110/500000: episode: 635, duration: 4.438s, episode steps: 1000, steps per second: 225, episode reward: 52.749, mean reward:  0.053 [-17.678, 13.925], mean action: 1.449 [0.000, 3.000],  loss: 2.061857, mae: 18.581530, mean_q: 24.601540\n",
      " 390018/500000: episode: 636, duration: 4.296s, episode steps: 908, steps per second: 211, episode reward: 117.441, mean reward:  0.129 [-18.207, 100.000], mean action: 1.728 [0.000, 3.000],  loss: 2.853568, mae: 18.671988, mean_q: 24.682158\n",
      " 391018/500000: episode: 637, duration: 4.735s, episode steps: 1000, steps per second: 211, episode reward: 26.186, mean reward:  0.026 [-18.127, 21.386], mean action: 1.999 [0.000, 3.000],  loss: 2.852401, mae: 19.149851, mean_q: 25.218555\n",
      " 391616/500000: episode: 638, duration: 2.355s, episode steps: 598, steps per second: 254, episode reward: -1.251, mean reward: -0.002 [-100.000, 14.029], mean action: 1.493 [0.000, 3.000],  loss: 5.259228, mae: 19.526997, mean_q: 25.443928\n",
      " 392616/500000: episode: 639, duration: 4.202s, episode steps: 1000, steps per second: 238, episode reward: -25.952, mean reward: -0.026 [-18.319, 12.523], mean action: 1.534 [0.000, 3.000],  loss: 2.656839, mae: 19.808681, mean_q: 26.015114\n",
      " 393616/500000: episode: 640, duration: 4.271s, episode steps: 1000, steps per second: 234, episode reward: 61.074, mean reward:  0.061 [-18.916, 23.099], mean action: 1.431 [0.000, 3.000],  loss: 12.308814, mae: 20.442743, mean_q: 26.343351\n",
      " 394616/500000: episode: 641, duration: 5.232s, episode steps: 1000, steps per second: 191, episode reward:  0.153, mean reward:  0.000 [-17.838, 12.572], mean action: 1.274 [0.000, 3.000],  loss: 3.275242, mae: 20.957554, mean_q: 27.210663\n",
      " 395616/500000: episode: 642, duration: 4.352s, episode steps: 1000, steps per second: 230, episode reward: -111.412, mean reward: -0.111 [-15.068, 15.040], mean action: 1.522 [0.000, 3.000],  loss: 3.731828, mae: 21.426695, mean_q: 28.042398\n",
      " 396485/500000: episode: 643, duration: 3.616s, episode steps: 869, steps per second: 240, episode reward: 129.269, mean reward:  0.149 [-3.378, 100.000], mean action: 1.412 [0.000, 3.000],  loss: 11.770338, mae: 21.954645, mean_q: 28.539576\n",
      " 397485/500000: episode: 644, duration: 4.500s, episode steps: 1000, steps per second: 222, episode reward: -56.768, mean reward: -0.057 [-3.959,  4.468], mean action: 1.604 [0.000, 3.000],  loss: 3.336028, mae: 22.383156, mean_q: 29.121168\n",
      " 398485/500000: episode: 645, duration: 4.561s, episode steps: 1000, steps per second: 219, episode reward: -13.106, mean reward: -0.013 [-3.037,  4.147], mean action: 1.451 [0.000, 3.000],  loss: 6.592797, mae: 22.758865, mean_q: 29.335447\n",
      " 399485/500000: episode: 646, duration: 4.923s, episode steps: 1000, steps per second: 203, episode reward: -40.312, mean reward: -0.040 [-18.146, 12.645], mean action: 1.806 [0.000, 3.000],  loss: 2.910937, mae: 22.887379, mean_q: 29.928217\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 400215/500000: episode: 647, duration: 3.195s, episode steps: 730, steps per second: 228, episode reward: 141.264, mean reward:  0.194 [-13.782, 100.000], mean action: 1.782 [0.000, 3.000],  loss: 3.449674, mae: 23.232914, mean_q: 30.345634\n",
      " 401009/500000: episode: 648, duration: 3.665s, episode steps: 794, steps per second: 217, episode reward: 157.286, mean reward:  0.198 [-8.044, 100.000], mean action: 1.469 [0.000, 3.000],  loss: 2.867663, mae: 23.765463, mean_q: 30.940525\n",
      " 401826/500000: episode: 649, duration: 3.434s, episode steps: 817, steps per second: 238, episode reward: -88.453, mean reward: -0.108 [-100.000, 17.369], mean action: 1.649 [0.000, 3.000],  loss: 10.143238, mae: 24.035858, mean_q: 31.267817\n",
      " 402819/500000: episode: 650, duration: 4.840s, episode steps: 993, steps per second: 205, episode reward: 129.609, mean reward:  0.131 [-22.961, 100.000], mean action: 1.224 [0.000, 3.000],  loss: 6.019108, mae: 23.945143, mean_q: 31.496746\n",
      " 403284/500000: episode: 651, duration: 1.860s, episode steps: 465, steps per second: 250, episode reward:  8.237, mean reward:  0.018 [-100.000, 15.954], mean action: 1.613 [0.000, 3.000],  loss: 2.010866, mae: 24.018963, mean_q: 31.773397\n",
      " 403506/500000: episode: 652, duration: 0.826s, episode steps: 222, steps per second: 269, episode reward: -104.107, mean reward: -0.469 [-100.000,  4.973], mean action: 1.824 [0.000, 3.000],  loss: 2.332299, mae: 24.298929, mean_q: 32.053337\n",
      " 404390/500000: episode: 653, duration: 4.195s, episode steps: 884, steps per second: 211, episode reward: 201.729, mean reward:  0.228 [-18.239, 100.000], mean action: 1.321 [0.000, 3.000],  loss: 2.289965, mae: 24.225170, mean_q: 31.724905\n",
      " 405316/500000: episode: 654, duration: 3.938s, episode steps: 926, steps per second: 235, episode reward: 231.486, mean reward:  0.250 [-17.766, 100.000], mean action: 1.100 [0.000, 3.000],  loss: 4.512921, mae: 23.953032, mean_q: 31.666435\n",
      " 406316/500000: episode: 655, duration: 4.984s, episode steps: 1000, steps per second: 201, episode reward: -19.931, mean reward: -0.020 [-15.024, 11.917], mean action: 1.493 [0.000, 3.000],  loss: 3.298319, mae: 23.675673, mean_q: 31.164816\n",
      " 406486/500000: episode: 656, duration: 0.628s, episode steps: 170, steps per second: 271, episode reward: -249.552, mean reward: -1.468 [-100.000,  4.485], mean action: 1.729 [0.000, 3.000],  loss: 5.444645, mae: 23.190519, mean_q: 30.574478\n",
      " 407308/500000: episode: 657, duration: 3.748s, episode steps: 822, steps per second: 219, episode reward: 151.882, mean reward:  0.185 [-9.811, 100.000], mean action: 1.502 [0.000, 3.000],  loss: 2.666634, mae: 23.124397, mean_q: 30.459833\n",
      " 408068/500000: episode: 658, duration: 3.338s, episode steps: 760, steps per second: 228, episode reward: 199.440, mean reward:  0.262 [-4.495, 100.000], mean action: 1.411 [0.000, 3.000],  loss: 4.695346, mae: 22.867296, mean_q: 30.120918\n",
      " 408724/500000: episode: 659, duration: 2.714s, episode steps: 656, steps per second: 242, episode reward: 244.019, mean reward:  0.372 [-17.536, 100.000], mean action: 1.216 [0.000, 3.000],  loss: 3.580831, mae: 22.236546, mean_q: 29.408308\n",
      " 409536/500000: episode: 660, duration: 3.396s, episode steps: 812, steps per second: 239, episode reward: 176.576, mean reward:  0.217 [-4.504, 100.000], mean action: 1.291 [0.000, 3.000],  loss: 5.099576, mae: 21.938282, mean_q: 28.728592\n",
      " 410435/500000: episode: 661, duration: 3.727s, episode steps: 899, steps per second: 241, episode reward: 142.160, mean reward:  0.158 [-5.327, 100.000], mean action: 1.529 [0.000, 3.000],  loss: 3.450989, mae: 21.727493, mean_q: 28.571722\n",
      " 411435/500000: episode: 662, duration: 4.908s, episode steps: 1000, steps per second: 204, episode reward: -4.478, mean reward: -0.004 [-4.544, 19.215], mean action: 1.574 [0.000, 3.000],  loss: 2.787576, mae: 21.366245, mean_q: 28.116989\n",
      " 412381/500000: episode: 663, duration: 4.222s, episode steps: 946, steps per second: 224, episode reward: -285.664, mean reward: -0.302 [-100.000, 12.937], mean action: 1.551 [0.000, 3.000],  loss: 3.206573, mae: 21.212490, mean_q: 28.074114\n",
      " 413293/500000: episode: 664, duration: 3.734s, episode steps: 912, steps per second: 244, episode reward: -617.061, mean reward: -0.677 [-100.000, 18.661], mean action: 1.679 [0.000, 3.000],  loss: 3.411374, mae: 21.094498, mean_q: 27.916342\n",
      " 414210/500000: episode: 665, duration: 4.396s, episode steps: 917, steps per second: 209, episode reward: -154.301, mean reward: -0.168 [-100.000, 11.854], mean action: 1.626 [0.000, 3.000],  loss: 3.467710, mae: 20.985447, mean_q: 27.673738\n",
      " 414950/500000: episode: 666, duration: 3.226s, episode steps: 740, steps per second: 229, episode reward: -76.286, mean reward: -0.103 [-100.000, 10.456], mean action: 1.570 [0.000, 3.000],  loss: 4.257624, mae: 21.057198, mean_q: 27.756346\n",
      " 415950/500000: episode: 667, duration: 4.659s, episode steps: 1000, steps per second: 215, episode reward: 44.760, mean reward:  0.045 [-18.074, 22.169], mean action: 1.419 [0.000, 3.000],  loss: 3.809319, mae: 20.850042, mean_q: 27.380962\n",
      " 416950/500000: episode: 668, duration: 4.959s, episode steps: 1000, steps per second: 202, episode reward: 32.966, mean reward:  0.033 [-19.812, 10.604], mean action: 1.460 [0.000, 3.000],  loss: 4.518493, mae: 20.523829, mean_q: 26.840832\n",
      " 417753/500000: episode: 669, duration: 3.214s, episode steps: 803, steps per second: 250, episode reward: 210.093, mean reward:  0.262 [-11.185, 100.000], mean action: 1.309 [0.000, 3.000],  loss: 4.302186, mae: 20.366806, mean_q: 26.626097\n",
      " 418696/500000: episode: 670, duration: 4.030s, episode steps: 943, steps per second: 234, episode reward: 104.887, mean reward:  0.111 [-9.903, 100.000], mean action: 1.549 [0.000, 3.000],  loss: 5.862082, mae: 20.173288, mean_q: 26.330437\n",
      " 419696/500000: episode: 671, duration: 4.866s, episode steps: 1000, steps per second: 206, episode reward: 36.536, mean reward:  0.037 [-18.282, 22.192], mean action: 1.199 [0.000, 3.000],  loss: 5.214863, mae: 20.028078, mean_q: 26.134565\n",
      " 420696/500000: episode: 672, duration: 5.282s, episode steps: 1000, steps per second: 189, episode reward: -17.718, mean reward: -0.018 [-17.436, 12.368], mean action: 1.433 [0.000, 3.000],  loss: 4.556345, mae: 20.162220, mean_q: 26.310625\n",
      " 421491/500000: episode: 673, duration: 3.463s, episode steps: 795, steps per second: 230, episode reward: 134.991, mean reward:  0.170 [-14.543, 100.000], mean action: 1.516 [0.000, 3.000],  loss: 4.886836, mae: 20.367754, mean_q: 26.763960\n",
      " 422491/500000: episode: 674, duration: 4.189s, episode steps: 1000, steps per second: 239, episode reward: -12.177, mean reward: -0.012 [-17.056, 16.590], mean action: 1.585 [0.000, 3.000],  loss: 4.441366, mae: 20.684942, mean_q: 27.223728\n",
      " 423352/500000: episode: 675, duration: 3.989s, episode steps: 861, steps per second: 216, episode reward: -136.865, mean reward: -0.159 [-100.000, 11.307], mean action: 1.652 [0.000, 3.000],  loss: 4.335934, mae: 21.073380, mean_q: 27.786030\n",
      " 423558/500000: episode: 676, duration: 0.767s, episode steps: 206, steps per second: 268, episode reward: -337.932, mean reward: -1.640 [-100.000,  4.455], mean action: 1.738 [0.000, 3.000],  loss: 3.355445, mae: 21.391941, mean_q: 28.418474\n",
      " 424362/500000: episode: 677, duration: 3.337s, episode steps: 804, steps per second: 241, episode reward: -129.753, mean reward: -0.161 [-100.000, 11.293], mean action: 1.709 [0.000, 3.000],  loss: 10.051589, mae: 21.586424, mean_q: 28.334019\n",
      " 425362/500000: episode: 678, duration: 4.103s, episode steps: 1000, steps per second: 244, episode reward: -62.176, mean reward: -0.062 [-20.107, 16.454], mean action: 1.645 [0.000, 3.000],  loss: 3.158619, mae: 21.702503, mean_q: 28.798702\n",
      " 426230/500000: episode: 679, duration: 3.603s, episode steps: 868, steps per second: 241, episode reward: 99.047, mean reward:  0.114 [-11.703, 100.000], mean action: 1.884 [0.000, 3.000],  loss: 5.691614, mae: 21.831671, mean_q: 28.830362\n",
      " 427155/500000: episode: 680, duration: 4.211s, episode steps: 925, steps per second: 220, episode reward: -170.218, mean reward: -0.184 [-100.000, 12.490], mean action: 1.644 [0.000, 3.000],  loss: 6.814900, mae: 21.908024, mean_q: 28.784004\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 427539/500000: episode: 681, duration: 1.483s, episode steps: 384, steps per second: 259, episode reward: -234.142, mean reward: -0.610 [-100.000, 16.360], mean action: 1.677 [0.000, 3.000],  loss: 6.247028, mae: 21.568014, mean_q: 28.571386\n",
      " 428429/500000: episode: 682, duration: 3.942s, episode steps: 890, steps per second: 226, episode reward: 66.552, mean reward:  0.075 [-18.177, 100.000], mean action: 1.785 [0.000, 3.000],  loss: 5.509879, mae: 21.498756, mean_q: 28.234348\n",
      " 428779/500000: episode: 683, duration: 1.379s, episode steps: 350, steps per second: 254, episode reward: -86.601, mean reward: -0.247 [-100.000,  5.341], mean action: 1.920 [0.000, 3.000],  loss: 5.670286, mae: 21.494730, mean_q: 28.081079\n",
      " 428890/500000: episode: 684, duration: 0.408s, episode steps: 111, steps per second: 272, episode reward: -144.162, mean reward: -1.299 [-100.000,  2.697], mean action: 1.775 [0.000, 3.000],  loss: 3.395953, mae: 21.185696, mean_q: 28.059669\n",
      " 429759/500000: episode: 685, duration: 3.657s, episode steps: 869, steps per second: 238, episode reward: 175.701, mean reward:  0.202 [-16.736, 100.000], mean action: 1.444 [0.000, 3.000],  loss: 5.519320, mae: 21.572548, mean_q: 28.219734\n",
      " 429952/500000: episode: 686, duration: 0.723s, episode steps: 193, steps per second: 267, episode reward: -239.502, mean reward: -1.241 [-100.000,  7.272], mean action: 1.839 [0.000, 3.000],  loss: 12.149283, mae: 21.457340, mean_q: 28.127146\n",
      " 430743/500000: episode: 687, duration: 3.286s, episode steps: 791, steps per second: 241, episode reward: -87.017, mean reward: -0.110 [-100.000, 11.696], mean action: 1.698 [0.000, 3.000],  loss: 6.719970, mae: 21.381678, mean_q: 27.994642\n",
      " 431688/500000: episode: 688, duration: 3.981s, episode steps: 945, steps per second: 237, episode reward: 154.010, mean reward:  0.163 [-3.362, 100.000], mean action: 1.572 [0.000, 3.000],  loss: 5.072456, mae: 21.305510, mean_q: 28.035156\n",
      " 432363/500000: episode: 689, duration: 2.837s, episode steps: 675, steps per second: 238, episode reward: -137.545, mean reward: -0.204 [-100.000, 13.759], mean action: 1.636 [0.000, 3.000],  loss: 6.007157, mae: 20.971424, mean_q: 27.374907\n",
      " 433063/500000: episode: 690, duration: 2.844s, episode steps: 700, steps per second: 246, episode reward: -75.029, mean reward: -0.107 [-100.000, 11.739], mean action: 1.544 [0.000, 3.000],  loss: 6.673381, mae: 20.908165, mean_q: 27.286026\n",
      " 434063/500000: episode: 691, duration: 4.679s, episode steps: 1000, steps per second: 214, episode reward: 21.196, mean reward:  0.021 [-8.424, 20.337], mean action: 1.542 [0.000, 3.000],  loss: 6.899194, mae: 21.064182, mean_q: 27.388510\n",
      " 435063/500000: episode: 692, duration: 4.899s, episode steps: 1000, steps per second: 204, episode reward: -6.771, mean reward: -0.007 [-4.563,  4.359], mean action: 1.532 [0.000, 3.000],  loss: 8.205123, mae: 20.934778, mean_q: 27.195015\n",
      " 436063/500000: episode: 693, duration: 4.821s, episode steps: 1000, steps per second: 207, episode reward: 33.852, mean reward:  0.034 [-17.754, 21.809], mean action: 1.468 [0.000, 3.000],  loss: 5.809467, mae: 20.752983, mean_q: 26.986649\n",
      " 437063/500000: episode: 694, duration: 4.598s, episode steps: 1000, steps per second: 217, episode reward: -34.325, mean reward: -0.034 [-5.101,  4.746], mean action: 1.792 [0.000, 3.000],  loss: 4.661255, mae: 20.606621, mean_q: 26.939480\n",
      " 438063/500000: episode: 695, duration: 4.773s, episode steps: 1000, steps per second: 210, episode reward: -60.685, mean reward: -0.061 [-4.141,  5.201], mean action: 1.626 [0.000, 3.000],  loss: 8.220915, mae: 20.449368, mean_q: 26.438364\n",
      " 439063/500000: episode: 696, duration: 4.327s, episode steps: 1000, steps per second: 231, episode reward: -40.749, mean reward: -0.041 [-3.712,  4.532], mean action: 1.578 [0.000, 3.000],  loss: 5.739908, mae: 20.215683, mean_q: 26.236254\n",
      " 439531/500000: episode: 697, duration: 1.834s, episode steps: 468, steps per second: 255, episode reward: -108.738, mean reward: -0.232 [-100.000, 13.011], mean action: 1.468 [0.000, 3.000],  loss: 6.091881, mae: 20.466692, mean_q: 26.646418\n",
      " 440527/500000: episode: 698, duration: 4.252s, episode steps: 996, steps per second: 234, episode reward: 122.800, mean reward:  0.123 [-10.485, 100.000], mean action: 1.546 [0.000, 3.000],  loss: 6.106122, mae: 20.212072, mean_q: 26.445543\n",
      " 440705/500000: episode: 699, duration: 0.658s, episode steps: 178, steps per second: 271, episode reward: -414.014, mean reward: -2.326 [-100.000,  5.249], mean action: 2.118 [0.000, 3.000],  loss: 5.871567, mae: 20.196451, mean_q: 26.628429\n",
      " 441705/500000: episode: 700, duration: 4.776s, episode steps: 1000, steps per second: 209, episode reward: 32.649, mean reward:  0.033 [-10.611, 14.783], mean action: 1.587 [0.000, 3.000],  loss: 7.392339, mae: 20.131842, mean_q: 25.938808\n",
      " 442705/500000: episode: 701, duration: 5.390s, episode steps: 1000, steps per second: 186, episode reward:  1.705, mean reward:  0.002 [-18.380, 15.938], mean action: 1.680 [0.000, 3.000],  loss: 7.340136, mae: 20.373199, mean_q: 25.724144\n",
      " 443686/500000: episode: 702, duration: 4.836s, episode steps: 981, steps per second: 203, episode reward: 85.739, mean reward:  0.087 [-17.822, 100.000], mean action: 1.513 [0.000, 3.000],  loss: 12.322012, mae: 20.121105, mean_q: 25.693321\n",
      " 444601/500000: episode: 703, duration: 4.259s, episode steps: 915, steps per second: 215, episode reward: 138.501, mean reward:  0.151 [-11.876, 100.000], mean action: 1.530 [0.000, 3.000],  loss: 11.817713, mae: 19.974052, mean_q: 25.572844\n",
      " 444710/500000: episode: 704, duration: 0.399s, episode steps: 109, steps per second: 273, episode reward: -162.807, mean reward: -1.494 [-100.000,  2.144], mean action: 1.945 [0.000, 3.000],  loss: 2.371371, mae: 19.921761, mean_q: 25.753403\n",
      " 444927/500000: episode: 705, duration: 0.788s, episode steps: 217, steps per second: 275, episode reward: -176.089, mean reward: -0.811 [-100.000,  3.833], mean action: 1.608 [0.000, 3.000],  loss: 6.670317, mae: 19.900583, mean_q: 25.504219\n",
      " 445128/500000: episode: 706, duration: 0.792s, episode steps: 201, steps per second: 254, episode reward: -350.356, mean reward: -1.743 [-100.000,  3.931], mean action: 1.771 [0.000, 3.000],  loss: 5.453640, mae: 19.445713, mean_q: 25.517700\n",
      " 445322/500000: episode: 707, duration: 0.708s, episode steps: 194, steps per second: 274, episode reward: -457.835, mean reward: -2.360 [-100.000,  6.192], mean action: 2.103 [0.000, 3.000],  loss: 23.124693, mae: 20.024120, mean_q: 25.818893\n",
      " 445587/500000: episode: 708, duration: 3.062s, episode steps: 265, steps per second:  87, episode reward: -504.763, mean reward: -1.905 [-100.000,  5.956], mean action: 2.015 [0.000, 3.000],  loss: 11.828767, mae: 20.022707, mean_q: 25.425404\n",
      " 445797/500000: episode: 709, duration: 0.829s, episode steps: 210, steps per second: 253, episode reward: -101.635, mean reward: -0.484 [-100.000,  2.953], mean action: 1.767 [0.000, 3.000],  loss: 4.394557, mae: 20.047590, mean_q: 25.558098\n",
      " 446677/500000: episode: 710, duration: 4.095s, episode steps: 880, steps per second: 215, episode reward: 90.384, mean reward:  0.103 [-16.289, 100.000], mean action: 2.024 [0.000, 3.000],  loss: 9.451718, mae: 20.324476, mean_q: 25.363770\n",
      " 447036/500000: episode: 711, duration: 1.391s, episode steps: 359, steps per second: 258, episode reward: -479.703, mean reward: -1.336 [-100.000,  5.050], mean action: 2.042 [0.000, 3.000],  loss: 8.501592, mae: 20.314440, mean_q: 25.535135\n",
      " 447221/500000: episode: 712, duration: 0.683s, episode steps: 185, steps per second: 271, episode reward: -135.505, mean reward: -0.732 [-100.000,  3.154], mean action: 1.514 [0.000, 3.000],  loss: 6.847756, mae: 20.700352, mean_q: 25.625372\n",
      " 447624/500000: episode: 713, duration: 1.551s, episode steps: 403, steps per second: 260, episode reward: -389.099, mean reward: -0.966 [-100.000, 31.733], mean action: 1.985 [0.000, 3.000],  loss: 17.920593, mae: 20.746908, mean_q: 25.443209\n",
      " 447804/500000: episode: 714, duration: 1.046s, episode steps: 180, steps per second: 172, episode reward: -129.853, mean reward: -0.721 [-100.000,  2.261], mean action: 1.861 [0.000, 3.000],  loss: 7.091130, mae: 20.493767, mean_q: 25.441179\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 448333/500000: episode: 715, duration: 9.673s, episode steps: 529, steps per second:  55, episode reward: -284.555, mean reward: -0.538 [-100.000, 23.896], mean action: 1.834 [0.000, 3.000],  loss: 7.230671, mae: 20.679670, mean_q: 25.341021\n",
      " 449333/500000: episode: 716, duration: 5.250s, episode steps: 1000, steps per second: 190, episode reward: 39.862, mean reward:  0.040 [-17.426, 14.192], mean action: 1.786 [0.000, 3.000],  loss: 11.098866, mae: 21.108978, mean_q: 25.346800\n",
      " 450333/500000: episode: 717, duration: 5.417s, episode steps: 1000, steps per second: 185, episode reward: -66.402, mean reward: -0.066 [-14.813, 20.000], mean action: 1.760 [0.000, 3.000],  loss: 9.960929, mae: 21.150255, mean_q: 25.124397\n",
      " 451284/500000: episode: 718, duration: 4.152s, episode steps: 951, steps per second: 229, episode reward: 87.108, mean reward:  0.092 [-18.148, 100.000], mean action: 1.549 [0.000, 3.000],  loss: 11.623812, mae: 21.339605, mean_q: 24.863174\n",
      " 451564/500000: episode: 719, duration: 1.265s, episode steps: 280, steps per second: 221, episode reward: -87.164, mean reward: -0.311 [-100.000,  5.136], mean action: 1.857 [0.000, 3.000],  loss: 10.315025, mae: 21.422373, mean_q: 24.646109\n",
      " 452207/500000: episode: 720, duration: 3.319s, episode steps: 643, steps per second: 194, episode reward: -237.276, mean reward: -0.369 [-100.000, 10.494], mean action: 1.897 [0.000, 3.000],  loss: 15.802024, mae: 21.392849, mean_q: 25.094309\n",
      " 452388/500000: episode: 721, duration: 0.763s, episode steps: 181, steps per second: 237, episode reward: -116.708, mean reward: -0.645 [-100.000,  3.300], mean action: 1.840 [0.000, 3.000],  loss: 6.282443, mae: 21.040110, mean_q: 25.541321\n",
      " 453020/500000: episode: 722, duration: 2.828s, episode steps: 632, steps per second: 223, episode reward: -87.788, mean reward: -0.139 [-100.000, 14.673], mean action: 1.880 [0.000, 3.000],  loss: 13.861407, mae: 21.586617, mean_q: 26.005699\n",
      " 454020/500000: episode: 723, duration: 5.384s, episode steps: 1000, steps per second: 186, episode reward: 16.273, mean reward:  0.016 [-19.325, 23.253], mean action: 1.973 [0.000, 3.000],  loss: 11.291241, mae: 21.584146, mean_q: 26.027281\n",
      " 455020/500000: episode: 724, duration: 5.253s, episode steps: 1000, steps per second: 190, episode reward: -36.818, mean reward: -0.037 [-20.819, 13.567], mean action: 1.599 [0.000, 3.000],  loss: 8.055868, mae: 21.393139, mean_q: 25.887053\n",
      " 456020/500000: episode: 725, duration: 5.668s, episode steps: 1000, steps per second: 176, episode reward: 24.515, mean reward:  0.025 [-20.225, 23.454], mean action: 1.654 [0.000, 3.000],  loss: 6.647945, mae: 21.186184, mean_q: 25.886658\n",
      " 457020/500000: episode: 726, duration: 5.319s, episode steps: 1000, steps per second: 188, episode reward: 19.331, mean reward:  0.019 [-12.054, 12.661], mean action: 1.725 [0.000, 3.000],  loss: 7.229127, mae: 21.167984, mean_q: 25.955391\n",
      " 458020/500000: episode: 727, duration: 6.397s, episode steps: 1000, steps per second: 156, episode reward: 17.841, mean reward:  0.018 [-3.746, 13.082], mean action: 1.528 [0.000, 3.000],  loss: 6.704276, mae: 21.381348, mean_q: 26.324875\n",
      " 458992/500000: episode: 728, duration: 6.342s, episode steps: 972, steps per second: 153, episode reward: 166.353, mean reward:  0.171 [-19.047, 100.000], mean action: 1.374 [0.000, 3.000],  loss: 5.201201, mae: 21.522734, mean_q: 26.797729\n",
      " 459992/500000: episode: 729, duration: 5.457s, episode steps: 1000, steps per second: 183, episode reward: 18.485, mean reward:  0.018 [-10.383, 19.823], mean action: 1.399 [0.000, 3.000],  loss: 5.783436, mae: 21.698540, mean_q: 27.530710\n",
      " 460929/500000: episode: 730, duration: 5.398s, episode steps: 937, steps per second: 174, episode reward: 154.527, mean reward:  0.165 [-17.506, 100.000], mean action: 1.520 [0.000, 3.000],  loss: 6.716013, mae: 21.891567, mean_q: 27.957932\n",
      " 461929/500000: episode: 731, duration: 5.437s, episode steps: 1000, steps per second: 184, episode reward: -21.494, mean reward: -0.021 [-4.453, 16.788], mean action: 1.679 [0.000, 3.000],  loss: 6.553955, mae: 22.281511, mean_q: 28.428486\n",
      " 462826/500000: episode: 732, duration: 4.048s, episode steps: 897, steps per second: 222, episode reward: -57.496, mean reward: -0.064 [-100.000, 17.142], mean action: 1.612 [0.000, 3.000],  loss: 5.625519, mae: 22.395519, mean_q: 28.860922\n",
      " 463826/500000: episode: 733, duration: 4.790s, episode steps: 1000, steps per second: 209, episode reward: -6.405, mean reward: -0.006 [-4.289,  4.913], mean action: 1.522 [0.000, 3.000],  loss: 7.356652, mae: 22.502470, mean_q: 28.769882\n",
      " 464826/500000: episode: 734, duration: 5.559s, episode steps: 1000, steps per second: 180, episode reward: -14.233, mean reward: -0.014 [-5.883,  5.137], mean action: 1.562 [0.000, 3.000],  loss: 5.547645, mae: 22.490246, mean_q: 28.750933\n",
      " 465826/500000: episode: 735, duration: 4.893s, episode steps: 1000, steps per second: 204, episode reward: 32.706, mean reward:  0.033 [-5.584, 15.726], mean action: 1.541 [0.000, 3.000],  loss: 5.652595, mae: 22.517775, mean_q: 28.587387\n",
      " 466826/500000: episode: 736, duration: 4.535s, episode steps: 1000, steps per second: 221, episode reward: 25.749, mean reward:  0.026 [-3.229, 15.242], mean action: 1.513 [0.000, 3.000],  loss: 5.001502, mae: 22.253016, mean_q: 28.100290\n",
      " 467826/500000: episode: 737, duration: 4.406s, episode steps: 1000, steps per second: 227, episode reward: 60.272, mean reward:  0.060 [-3.397, 11.211], mean action: 1.588 [0.000, 3.000],  loss: 4.706626, mae: 22.238447, mean_q: 27.819223\n",
      " 468826/500000: episode: 738, duration: 4.761s, episode steps: 1000, steps per second: 210, episode reward: -45.771, mean reward: -0.046 [-4.872,  4.553], mean action: 1.490 [0.000, 3.000],  loss: 5.633795, mae: 22.150928, mean_q: 27.826845\n",
      " 469826/500000: episode: 739, duration: 5.162s, episode steps: 1000, steps per second: 194, episode reward: -34.823, mean reward: -0.035 [-8.297,  6.678], mean action: 1.565 [0.000, 3.000],  loss: 5.756355, mae: 21.928364, mean_q: 27.234516\n",
      " 470826/500000: episode: 740, duration: 4.544s, episode steps: 1000, steps per second: 220, episode reward:  9.451, mean reward:  0.009 [-19.910, 12.158], mean action: 1.449 [0.000, 3.000],  loss: 5.664451, mae: 21.801165, mean_q: 26.702509\n",
      " 471826/500000: episode: 741, duration: 4.266s, episode steps: 1000, steps per second: 234, episode reward: -18.317, mean reward: -0.018 [-10.680, 13.396], mean action: 1.587 [0.000, 3.000],  loss: 5.462574, mae: 21.459301, mean_q: 26.652988\n",
      " 471936/500000: episode: 742, duration: 0.405s, episode steps: 110, steps per second: 271, episode reward: -135.082, mean reward: -1.228 [-100.000,  1.884], mean action: 1.655 [0.000, 3.000],  loss: 4.268570, mae: 21.549492, mean_q: 26.042145\n",
      " 472936/500000: episode: 743, duration: 4.935s, episode steps: 1000, steps per second: 203, episode reward: -5.484, mean reward: -0.005 [-10.291, 10.653], mean action: 1.694 [0.000, 3.000],  loss: 5.447116, mae: 21.528570, mean_q: 26.446480\n",
      " 473936/500000: episode: 744, duration: 4.455s, episode steps: 1000, steps per second: 224, episode reward: -28.162, mean reward: -0.028 [-4.759,  4.171], mean action: 1.622 [0.000, 3.000],  loss: 5.852459, mae: 21.072985, mean_q: 26.081219\n",
      " 474936/500000: episode: 745, duration: 5.080s, episode steps: 1000, steps per second: 197, episode reward: 74.822, mean reward:  0.075 [-17.500, 12.462], mean action: 1.386 [0.000, 3.000],  loss: 5.292668, mae: 20.831707, mean_q: 25.852850\n",
      " 475936/500000: episode: 746, duration: 4.633s, episode steps: 1000, steps per second: 216, episode reward: -34.240, mean reward: -0.034 [-3.190,  4.710], mean action: 1.683 [0.000, 3.000],  loss: 4.919679, mae: 20.740025, mean_q: 25.639622\n",
      " 476936/500000: episode: 747, duration: 5.381s, episode steps: 1000, steps per second: 186, episode reward: -14.600, mean reward: -0.015 [-3.637,  4.481], mean action: 1.587 [0.000, 3.000],  loss: 3.556142, mae: 20.656231, mean_q: 25.251949\n",
      " 477936/500000: episode: 748, duration: 4.871s, episode steps: 1000, steps per second: 205, episode reward: -11.695, mean reward: -0.012 [-5.079,  4.583], mean action: 1.612 [0.000, 3.000],  loss: 5.297505, mae: 20.455341, mean_q: 25.003242\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 478936/500000: episode: 749, duration: 4.790s, episode steps: 1000, steps per second: 209, episode reward: 39.563, mean reward:  0.040 [-4.587, 10.668], mean action: 1.596 [0.000, 3.000],  loss: 4.491347, mae: 20.153833, mean_q: 24.349251\n",
      " 479936/500000: episode: 750, duration: 4.845s, episode steps: 1000, steps per second: 206, episode reward: -7.694, mean reward: -0.008 [-4.858,  4.431], mean action: 1.551 [0.000, 3.000],  loss: 4.290724, mae: 20.124655, mean_q: 24.325916\n",
      " 480936/500000: episode: 751, duration: 4.562s, episode steps: 1000, steps per second: 219, episode reward: -74.878, mean reward: -0.075 [-15.039, 11.256], mean action: 1.477 [0.000, 3.000],  loss: 3.735338, mae: 19.720993, mean_q: 23.980507\n",
      " 481936/500000: episode: 752, duration: 4.226s, episode steps: 1000, steps per second: 237, episode reward: -3.401, mean reward: -0.003 [-3.664,  5.543], mean action: 1.526 [0.000, 3.000],  loss: 3.384494, mae: 19.396402, mean_q: 23.464296\n",
      " 482936/500000: episode: 753, duration: 4.559s, episode steps: 1000, steps per second: 219, episode reward: -5.990, mean reward: -0.006 [-3.518,  4.498], mean action: 1.573 [0.000, 3.000],  loss: 3.202115, mae: 19.093021, mean_q: 23.077269\n",
      " 483936/500000: episode: 754, duration: 4.766s, episode steps: 1000, steps per second: 210, episode reward:  1.295, mean reward:  0.001 [-10.597, 14.666], mean action: 1.667 [0.000, 3.000],  loss: 3.166941, mae: 19.041721, mean_q: 22.758163\n",
      " 484936/500000: episode: 755, duration: 4.347s, episode steps: 1000, steps per second: 230, episode reward: 37.092, mean reward:  0.037 [-13.666, 20.145], mean action: 1.494 [0.000, 3.000],  loss: 3.051787, mae: 18.968124, mean_q: 22.815468\n",
      " 485900/500000: episode: 756, duration: 4.540s, episode steps: 964, steps per second: 212, episode reward: 149.105, mean reward:  0.155 [-18.045, 100.000], mean action: 1.471 [0.000, 3.000],  loss: 3.266299, mae: 19.023029, mean_q: 22.621414\n",
      " 486900/500000: episode: 757, duration: 4.104s, episode steps: 1000, steps per second: 244, episode reward: -11.470, mean reward: -0.011 [-4.731, 13.981], mean action: 1.530 [0.000, 3.000],  loss: 3.311510, mae: 18.723995, mean_q: 22.496548\n",
      " 487350/500000: episode: 758, duration: 1.903s, episode steps: 450, steps per second: 237, episode reward: -135.232, mean reward: -0.301 [-100.000,  3.866], mean action: 1.813 [0.000, 3.000],  loss: 2.313306, mae: 18.929960, mean_q: 22.591303\n",
      " 488323/500000: episode: 759, duration: 4.324s, episode steps: 973, steps per second: 225, episode reward: 112.180, mean reward:  0.115 [-18.028, 100.000], mean action: 1.634 [0.000, 3.000],  loss: 3.409688, mae: 18.768675, mean_q: 22.218485\n",
      " 489323/500000: episode: 760, duration: 4.269s, episode steps: 1000, steps per second: 234, episode reward: -0.228, mean reward: -0.000 [-12.201, 28.504], mean action: 1.623 [0.000, 3.000],  loss: 3.669320, mae: 18.842882, mean_q: 22.512587\n",
      " 489449/500000: episode: 761, duration: 0.468s, episode steps: 126, steps per second: 269, episode reward: -286.554, mean reward: -2.274 [-100.000,  0.927], mean action: 1.198 [0.000, 3.000],  loss: 3.017367, mae: 18.795399, mean_q: 22.647211\n",
      " 490449/500000: episode: 762, duration: 4.942s, episode steps: 1000, steps per second: 202, episode reward: -21.994, mean reward: -0.022 [-21.388, 22.267], mean action: 1.822 [0.000, 3.000],  loss: 2.790604, mae: 18.826731, mean_q: 22.891481\n",
      " 491154/500000: episode: 763, duration: 3.265s, episode steps: 705, steps per second: 216, episode reward: 189.591, mean reward:  0.269 [-11.726, 100.000], mean action: 1.462 [0.000, 3.000],  loss: 3.426142, mae: 19.032736, mean_q: 22.552935\n",
      " 492154/500000: episode: 764, duration: 5.180s, episode steps: 1000, steps per second: 193, episode reward: 42.224, mean reward:  0.042 [-20.531, 20.813], mean action: 1.568 [0.000, 3.000],  loss: 3.873285, mae: 18.739355, mean_q: 22.356365\n",
      " 492284/500000: episode: 765, duration: 0.481s, episode steps: 130, steps per second: 270, episode reward: -145.589, mean reward: -1.120 [-100.000,  2.928], mean action: 1.854 [0.000, 3.000],  loss: 2.113355, mae: 18.282274, mean_q: 22.168550\n",
      " 493279/500000: episode: 766, duration: 4.548s, episode steps: 995, steps per second: 219, episode reward: 145.220, mean reward:  0.146 [-16.302, 100.000], mean action: 1.364 [0.000, 3.000],  loss: 3.090790, mae: 18.425091, mean_q: 22.142498\n",
      " 494279/500000: episode: 767, duration: 4.961s, episode steps: 1000, steps per second: 202, episode reward: -4.777, mean reward: -0.005 [-18.901, 12.428], mean action: 1.935 [0.000, 3.000],  loss: 2.863160, mae: 18.117214, mean_q: 21.857306\n",
      " 495126/500000: episode: 768, duration: 3.637s, episode steps: 847, steps per second: 233, episode reward: 77.724, mean reward:  0.092 [-13.996, 100.000], mean action: 1.680 [0.000, 3.000],  loss: 3.188079, mae: 18.230036, mean_q: 21.434223\n",
      " 495315/500000: episode: 769, duration: 0.759s, episode steps: 189, steps per second: 249, episode reward: -102.848, mean reward: -0.544 [-100.000,  3.762], mean action: 1.672 [0.000, 3.000],  loss: 2.908971, mae: 17.905323, mean_q: 22.212833\n",
      " 496315/500000: episode: 770, duration: 4.477s, episode steps: 1000, steps per second: 223, episode reward: -6.193, mean reward: -0.006 [-11.130, 14.820], mean action: 1.468 [0.000, 3.000],  loss: 2.523781, mae: 17.538090, mean_q: 21.627962\n",
      " 497315/500000: episode: 771, duration: 4.429s, episode steps: 1000, steps per second: 226, episode reward: 27.212, mean reward:  0.027 [-10.325, 13.481], mean action: 1.664 [0.000, 3.000],  loss: 3.197286, mae: 17.236340, mean_q: 21.244402\n",
      " 498315/500000: episode: 772, duration: 4.755s, episode steps: 1000, steps per second: 210, episode reward: 28.778, mean reward:  0.029 [-14.381, 21.570], mean action: 1.512 [0.000, 3.000],  loss: 1.966966, mae: 16.599934, mean_q: 21.155191\n",
      " 499267/500000: episode: 773, duration: 4.811s, episode steps: 952, steps per second: 198, episode reward: 138.336, mean reward:  0.145 [-20.430, 100.000], mean action: 1.394 [0.000, 3.000],  loss: 1.915443, mae: 16.155348, mean_q: 20.867327\n",
      "done, took 2273.850 seconds\n"
     ]
    }
   ],
   "source": [
    "history = dqn.fit(env, nb_steps=500000, visualize=False, verbose=2)\n",
    "dqn.save_weights('dqn_{}_weights.h5f'.format(ENV_NAME), overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = pd.DataFrame(history.history)\n",
    "ax = df['episode_reward'].plot(color = 'lightgray')\n",
    "df['episode_reward'].rolling(50).mean().plot(color = 'black')\n",
    "ax.set_xlabel(\"Episode\")\n",
    "plt.ylabel(\"Rolling Mean (10) Cumulative Return\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('training_weights')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import io\n",
    "# Redirect stdout to capture test results\n",
    "old_stdout = sys.stdout\n",
    "sys.stdout = mystdout = io.StringIO()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn.test(env, nb_episodes=200, visualize=False)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.stdout = old_stdout\n",
    "\n",
    "results_text = mystdout.getvalue()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_file = open(\"rewards_test.txt\", \"wt\")\n",
    "n = text_file.write(results_text)\n",
    "text_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Testing for 200 episodes ...\\nEpisode 1: reward: -185.719, steps: 121\\nEpisode 2: reward: -58.025, steps: 1000\\nEpisode 3: reward: -30.674, steps: 1000\\nEpisode 4: reward: -32.171, steps: 1000\\nEpisode 5: reward: -57.308, steps: 1000\\nEpisode 6: reward: -193.346, steps: 116\\nEpisode 7: reward: -22.976, steps: 1000\\nEpisode 8: reward: -112.443, steps: 196\\nEpisode 9: reward: -129.385, steps: 414\\nEpisode 10: reward: -42.827, steps: 1000\\nEpisode 11: reward: -17.041, steps: 1000\\nEpisode 12: reward: -125.334, steps: 229\\nEpisode 13: reward: -55.529, steps: 1000\\nEpisode 14: reward: -31.111, steps: 1000\\nEpisode 15: reward: -124.468, steps: 241\\nEpisode 16: reward: -126.410, steps: 357\\nEpisode 17: reward: -48.170, steps: 1000\\nEpisode 18: reward: -43.405, steps: 1000\\nEpisode 19: reward: -144.519, steps: 500\\nEpisode 20: reward: -52.457, steps: 1000\\nEpisode 21: reward: -64.565, steps: 1000\\nEpisode 22: reward: -22.923, steps: 1000\\nEpisode 23: reward: -68.159, steps: 1000\\nEpisode 24: reward: -39.389, steps: 1000\\nEpisode 25: reward: -33.883, steps: 1000\\nEpisode 26: reward: -52.441, steps: 1000\\nEpisode 27: reward: -192.964, steps: 306\\nEpisode 28: reward: -10.460, steps: 1000\\nEpisode 29: reward: -44.881, steps: 1000\\nEpisode 30: reward: -117.995, steps: 195\\nEpisode 31: reward: -56.967, steps: 1000\\nEpisode 32: reward: -29.696, steps: 1000\\nEpisode 33: reward: -36.681, steps: 1000\\nEpisode 34: reward: -125.490, steps: 430\\nEpisode 35: reward: -19.382, steps: 1000\\nEpisode 36: reward: -62.783, steps: 1000\\nEpisode 37: reward: -33.275, steps: 1000\\nEpisode 38: reward: -120.753, steps: 234\\nEpisode 39: reward: -19.352, steps: 1000\\nEpisode 40: reward: -37.709, steps: 1000\\nEpisode 41: reward: -65.755, steps: 1000\\nEpisode 42: reward: -142.629, steps: 333\\nEpisode 43: reward: -125.761, steps: 351\\nEpisode 44: reward: -62.777, steps: 1000\\nEpisode 45: reward: -134.331, steps: 403\\nEpisode 46: reward: -294.980, steps: 277\\nEpisode 47: reward: -154.259, steps: 403\\nEpisode 48: reward: -69.424, steps: 1000\\nEpisode 49: reward: -62.869, steps: 1000\\nEpisode 50: reward: -31.871, steps: 1000\\nEpisode 51: reward: -54.341, steps: 1000\\nEpisode 52: reward: -14.764, steps: 1000\\nEpisode 53: reward: -124.123, steps: 325\\nEpisode 54: reward: -23.395, steps: 1000\\nEpisode 55: reward: -55.556, steps: 1000\\nEpisode 56: reward: -131.860, steps: 241\\nEpisode 57: reward: -29.224, steps: 1000\\nEpisode 58: reward: -122.712, steps: 232\\nEpisode 59: reward: -0.534, steps: 1000\\nEpisode 60: reward: -40.314, steps: 1000\\nEpisode 61: reward: -77.615, steps: 1000\\nEpisode 62: reward: -158.309, steps: 366\\nEpisode 63: reward: -22.015, steps: 1000\\nEpisode 64: reward: -35.633, steps: 1000\\nEpisode 65: reward: -18.085, steps: 1000\\nEpisode 66: reward: -31.452, steps: 1000\\nEpisode 67: reward: -45.287, steps: 1000\\nEpisode 68: reward: -42.349, steps: 1000\\nEpisode 69: reward: -32.742, steps: 1000\\nEpisode 70: reward: -60.645, steps: 1000\\nEpisode 71: reward: -41.165, steps: 1000\\nEpisode 72: reward: -137.933, steps: 531\\nEpisode 73: reward: -33.158, steps: 1000\\nEpisode 74: reward: -114.586, steps: 186\\nEpisode 75: reward: -143.965, steps: 570\\nEpisode 76: reward: -30.994, steps: 1000\\nEpisode 77: reward: -131.612, steps: 436\\nEpisode 78: reward: -10.611, steps: 1000\\nEpisode 79: reward: -139.114, steps: 423\\nEpisode 80: reward: -61.998, steps: 1000\\nEpisode 81: reward: -17.326, steps: 1000\\nEpisode 82: reward: -65.510, steps: 1000\\nEpisode 83: reward: -127.329, steps: 207\\nEpisode 84: reward: -14.459, steps: 1000\\nEpisode 85: reward: -49.029, steps: 1000\\nEpisode 86: reward: -20.471, steps: 1000\\nEpisode 87: reward: -41.993, steps: 1000\\nEpisode 88: reward: -25.193, steps: 1000\\nEpisode 89: reward: -53.753, steps: 1000\\nEpisode 90: reward: -130.749, steps: 423\\nEpisode 91: reward: -9.501, steps: 1000\\nEpisode 92: reward: -27.896, steps: 1000\\nEpisode 93: reward: -152.630, steps: 153\\nEpisode 94: reward: -187.145, steps: 122\\nEpisode 95: reward: -154.129, steps: 132\\nEpisode 96: reward: -37.318, steps: 1000\\nEpisode 97: reward: -114.289, steps: 174\\nEpisode 98: reward: -43.479, steps: 1000\\nEpisode 99: reward: -34.984, steps: 1000\\nEpisode 100: reward: -39.873, steps: 1000\\nEpisode 101: reward: -25.750, steps: 1000\\nEpisode 102: reward: -31.413, steps: 1000\\nEpisode 103: reward: -36.093, steps: 1000\\nEpisode 104: reward: -131.939, steps: 391\\nEpisode 105: reward: -159.225, steps: 340\\nEpisode 106: reward: -26.164, steps: 1000\\nEpisode 107: reward: -18.139, steps: 1000\\nEpisode 108: reward: -23.359, steps: 1000\\nEpisode 109: reward: -21.618, steps: 1000\\nEpisode 110: reward: -128.561, steps: 240\\nEpisode 111: reward: -23.761, steps: 1000\\nEpisode 112: reward: -111.869, steps: 183\\nEpisode 113: reward: -32.767, steps: 1000\\nEpisode 114: reward: -5.993, steps: 1000\\nEpisode 115: reward: -41.567, steps: 1000\\nEpisode 116: reward: -109.517, steps: 385\\nEpisode 117: reward: -44.413, steps: 1000\\nEpisode 118: reward: -17.406, steps: 1000\\nEpisode 119: reward: -41.860, steps: 1000\\nEpisode 120: reward: -151.946, steps: 484\\nEpisode 121: reward: -30.824, steps: 1000\\nEpisode 122: reward: -40.148, steps: 1000\\nEpisode 123: reward: -25.997, steps: 1000\\nEpisode 124: reward: -49.351, steps: 1000\\nEpisode 125: reward: -61.739, steps: 1000\\nEpisode 126: reward: -132.165, steps: 157\\nEpisode 127: reward: -34.725, steps: 1000\\nEpisode 128: reward: -41.098, steps: 1000\\nEpisode 129: reward: -124.401, steps: 169\\nEpisode 130: reward: -30.499, steps: 1000\\nEpisode 131: reward: -37.197, steps: 1000\\nEpisode 132: reward: -54.782, steps: 1000\\nEpisode 133: reward: -55.349, steps: 1000\\nEpisode 134: reward: -34.514, steps: 1000\\nEpisode 135: reward: -159.696, steps: 370\\nEpisode 136: reward: -125.724, steps: 160\\nEpisode 137: reward: -43.158, steps: 1000\\nEpisode 138: reward: -259.176, steps: 271\\nEpisode 139: reward: -12.771, steps: 1000\\nEpisode 140: reward: -65.315, steps: 1000\\nEpisode 141: reward: -133.532, steps: 149\\nEpisode 142: reward: -28.208, steps: 1000\\nEpisode 143: reward: -55.842, steps: 1000\\nEpisode 144: reward: -141.707, steps: 297\\nEpisode 145: reward: -35.490, steps: 1000\\nEpisode 146: reward: -37.787, steps: 1000\\nEpisode 147: reward: -30.412, steps: 1000\\nEpisode 148: reward: -13.471, steps: 1000\\nEpisode 149: reward: -9.513, steps: 1000\\nEpisode 150: reward: -25.863, steps: 1000\\nEpisode 151: reward: -147.339, steps: 437\\nEpisode 152: reward: -47.392, steps: 1000\\nEpisode 153: reward: -52.400, steps: 1000\\nEpisode 154: reward: -29.961, steps: 1000\\nEpisode 155: reward: -126.059, steps: 264\\nEpisode 156: reward: -28.668, steps: 1000\\nEpisode 157: reward: -34.914, steps: 1000\\nEpisode 158: reward: -125.498, steps: 405\\nEpisode 159: reward: -164.816, steps: 582\\nEpisode 160: reward: -35.301, steps: 1000\\nEpisode 161: reward: -34.149, steps: 1000\\nEpisode 162: reward: -52.518, steps: 1000\\nEpisode 163: reward: -65.719, steps: 1000\\nEpisode 164: reward: -25.925, steps: 1000\\nEpisode 165: reward: -53.581, steps: 1000\\nEpisode 166: reward: -46.428, steps: 1000\\nEpisode 167: reward: -61.211, steps: 1000\\nEpisode 168: reward: -42.879, steps: 1000\\nEpisode 169: reward: -240.826, steps: 102\\nEpisode 170: reward: -44.731, steps: 1000\\nEpisode 171: reward: -128.365, steps: 240\\nEpisode 172: reward: -128.605, steps: 450\\nEpisode 173: reward: 19.075, steps: 1000\\nEpisode 174: reward: -50.306, steps: 1000\\nEpisode 175: reward: -40.143, steps: 1000\\nEpisode 176: reward: -23.146, steps: 1000\\nEpisode 177: reward: -30.280, steps: 1000\\nEpisode 178: reward: -17.574, steps: 1000\\nEpisode 179: reward: -65.999, steps: 1000\\nEpisode 180: reward: -64.706, steps: 1000\\nEpisode 181: reward: -42.372, steps: 1000\\nEpisode 182: reward: -53.658, steps: 1000\\nEpisode 183: reward: -226.659, steps: 98\\nEpisode 184: reward: -40.847, steps: 1000\\nEpisode 185: reward: -52.199, steps: 1000\\nEpisode 186: reward: -32.723, steps: 1000\\nEpisode 187: reward: -70.007, steps: 1000\\nEpisode 188: reward: -15.485, steps: 1000\\nEpisode 189: reward: -41.312, steps: 1000\\nEpisode 190: reward: -34.260, steps: 1000\\nEpisode 191: reward: -194.282, steps: 108\\nEpisode 192: reward: -31.010, steps: 1000\\nEpisode 193: reward: -22.321, steps: 1000\\nEpisode 194: reward: -48.328, steps: 1000\\nEpisode 195: reward: -157.052, steps: 385\\nEpisode 196: reward: -47.065, steps: 1000\\nEpisode 197: reward: -54.448, steps: 1000\\nEpisode 198: reward: -70.707, steps: 1000\\nEpisode 199: reward: -245.557, steps: 100\\nEpisode 200: reward: -27.955, steps: 1000\\n'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
